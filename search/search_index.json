{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"None","text":"<p>Documentation: https://ambiqai.github.io/heartkit</p> <p>Source Code: https://github.com/AmbiqAI/heartkit</p> <p>HeartKit is an optimized open-source TinyML model purpose-built to enable running a variety of real-time heart-monitoring applications on battery-powered, edge devices. By leveraging a modern multi-head network architecture coupled with Ambiq's ultra low-power SoC, the model is designed to be efficient, explainable, and extensible.</p> <p>The architecture consists of an ECG segmentation model followed by three upstream heads: HRV head, arrhythmia head, and beat head. The ECG segmentation model serves as the backbone and is used to annotate every sample as either P-wave, QRS, T-wave, or none. The arrhythmia head is used to detect the presence of Atrial Fibrillation (AFIB) or Atrial Flutter (AFL). The HRV head is used to calculate heart rate, rhythm (e.g., bradycardia), and heart rate variability from the R peaks. Lastly, the beat head is used to identify individual irregular beats (PAC, PVC).</p> <p>Key Features:</p> <ul> <li>Efficient: Novel architecture coupled w/ Ambiq's ultra low-power SoCs enable extreme energy efficiency.</li> <li>Explainable: Inference results are paired with metrics to provide explainable insights.</li> <li>Extensible: Add or remove heads for desired end application.</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.11+</li> <li>Poetry 1.2.1+</li> </ul> <p>The following are also required to compile/flash the binary for the EVB demo:</p> <ul> <li>Arm GNU Toolchain 11.3</li> <li>Segger J-Link v7.56+</li> </ul> <p>Note</p> <p>A VSCode Dev Container is also available and defined in ./.devcontainer.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>$ poetry install\n\n---&gt; 100%\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>HeartKit can be used as either a CLI-based app or as a python package to perform advanced experimentation. In both forms, HeartKit exposes a number of modes and tasks discussed below. Refer to the Overview Guide to learn more about available options and configurations.</p>"},{"location":"#modes","title":"Modes","text":"<ul> <li><code>download</code>: Download datasets</li> <li><code>train</code>: Train a model for specified task and dataset(s)</li> <li><code>evaluate</code>: Evaluate a model for specified task and dataset(s)</li> <li><code>export</code>: Export a trained model to TensorFlow Lite and TFLM</li> <li><code>demo</code>: Run full demo on PC or EVB</li> </ul>"},{"location":"#tasks","title":"Tasks","text":"<ul> <li><code>Segmentation</code>: Perform ECG based segmentation (P-Wave, QRS, T-Wave)</li> <li><code>HRV</code>: Heart rate, rhythm, HRV metrics (RR interval)</li> <li><code>Arrhythmia</code>: Heart arrhythmia detection (AFIB, AFL)</li> <li><code>Beat</code>: Classify individual beats (NORM, PAC, PVC, NOISE)</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>HeartKit leverages a multi-head network- a backbone segmentation model followed by 3 uptream heads:</p> <ul> <li>Segmentation backbone utilizes a custom 1-D UNET architecture to perform ECG segmentation.</li> <li>HRV head utilizes segmentation results to derive a number of useful metrics including heart rate, rhythm and RR interval.</li> <li>Arrhythmia head utilizes a 1-D MBConv CNN to detect arrhythmias include AFIB and AFL.</li> <li>Beat-level head utilizes a 1-D MBConv CNN to detect irregular individual beats (PAC, PVC).</li> </ul> <p> </p> <p>Refer to Architecture Overview for additional details on the model design.</p>"},{"location":"#datasets","title":"Datasets","text":"<p>HeartKit leverages several open-source datasets for training each of the HeartKit models. Additionally, HeartKit contains a customizable synthetic 12-lead ECG generator. Check out the Datasets Guide to learn more about the datasets used along with their corresponding licenses and limitations.</p>"},{"location":"#results","title":"Results","text":"<p>The following table provides the latest performance and accuracy results of all models when running on Apollo4 Plus EVB. Additional result details can be found in Results Section.</p> Task Params FLOPS Metric Cycles/Inf Time/Inf Segmentation 33K 6.5M 87.0% IOU 531ms 102M Arrhythmia 50K 3.6M 99.0% F1 465ms 89M Beat 73K 2.2M 91.5% F1 241ms 46M"},{"location":"#references","title":"References","text":"<ul> <li>ECG Heartbeat classification using deep transfer learning with Convolutional Neural Network and STFT technique</li> <li>Classification of ECG based on Hybrid Features using CNNs for Wearable Applications</li> <li>ECG Heartbeat classification using deep transfer learning with Convolutional Neural Network and STFT technique</li> <li>U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection</li> <li>UNET 3+: A FULL-SCALE CONNECTED UNET FOR MEDICAL IMAGE SEGMENTATION</li> <li>ResUNet-a: a deep learning framework for semantic segmentation of remotely sensed data</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>HeartKit leverages a multi-head network- a backbone segmentation model followed by 3 upstream heads:</p> <p></p>"},{"location":"architecture/#ecg-segmentation","title":"ECG Segmentation","text":"<p>The ECG segmentation model serves as the backbone and is used to annotate every sample as either P-wave, QRS, T-wave, or none. The resulting ECG data and segmentation mask is then fed into upstream \u201cheads\u201d. This model utilizes a custom 1-D UNET architecture w/ additional skip connections between encoder and decoder blocks. The encoder blocks are convolutional based and include both expansion and inverted residuals layers. The only preprocessing performed is band-pass filtering and standardization on the window of ECG data.</p>"},{"location":"architecture/#hrv-head","title":"HRV Head","text":"<p>The HRV head uses only DSP and statistics (i.e. no neural network is used). Using a combination of segmentation results and QRS filter, the HRV head detects R peak candidates. RR intervals are extracted and filtered, and then used to derive a variety of HRV metrics including heart rate, rhythm, SDNN, SDRR, SDANN, etc. All of the identified R peaks are further fed to the beat classifier head. Note that if segmentation model is not enabled, HRV head falls back to identifying R peaks purely on gradient of QRS signal.</p>"},{"location":"architecture/#arrhythmia-head","title":"Arrhythmia Head","text":"<p>The arrhythmia head is used to detect the presence of Atrial Fibrillation (AFIB) or Atrial Flutter (AFL). Note that if heart arrhythmia is detected, the remaining heads are skipped. The arrhythmia model utilizes a 1-D CNN built using MBConv style blocks that incorporate expansion, inverted residuals, and squeeze and excitation layers. Furthermore, longer filter and stide lengths are utilized in the initial layers to capture more temporal dependencies.</p>"},{"location":"architecture/#beat-head","title":"Beat Head","text":"<p>The beat head is used to extract individual beats and classify them as either normal, premature/ectopic atrial contraction (PAC), premature/ectopic ventricular contraction (PVC), or noise. In addition to the target beat, the surrounding beats are also fed into the network as context. The \u201cneighboring\u201d beats are determined based on the average RR interval and not the actual R peak. The beat head also utilizes a 1-D CNN built using MBConv style blocks.</p>"},{"location":"datasets/","title":"Datasets","text":"<p>Several datasets are readily available online that are suitable for training various heart-related models. The following datasets are ones either used or plan to use. For arrhythmia and beat classification, Icentia11k dataset is used as it contains the largest number of patients in a highly ambulatory setting- users wearing a 1-lead chest band for up to two weeks. For segmentation, syntheticl, LUDB, and QTDB datasets are being utilized. Please make sure to review each dataset's license for terms and limitations.</p>"},{"location":"datasets/#icentia11k-dataset","title":"Icentia11k Dataset","text":"<p>This dataset consists of ECG recordings from 11,000 patients and 2 billion labelled beats. The data was collected by the CardioSTAT, a single-lead heart monitor device from Icentia. The raw signals were recorded with a 16-bit resolution and sampled at 250 Hz with the CardioSTAT in a modified lead 1 position. We provide derived version of the dataset where each patient is stored in separate HDF5 files on S3. This makes it faster to download as well as makes it possible to leverage TensorFlow <code>prefetch</code> and <code>interleave</code> to parallelize data loading.</p> <p>Heart Tasks: Arrhythmia, beat</p> <p>Warning</p> <p>The dataset is intended for evaluation purposes only and cannot be used for commercial use without permission. Please visit Physionet for more details.</p>"},{"location":"datasets/#ludb-dataset","title":"LUDB Dataset","text":"<p>The Lobachevsky University Electrocardiography database (LUDB) consists of 200 10-second 12-lead records. The boundaries and peaks of P, T waves and QRS complexes were manually annotated by cardiologists. Each record is annotated with the corresponding diagnosis. Please visit Physionet for more details.</p> <p>Heart Tasks: Segmentation, HRV</p>"},{"location":"datasets/#qt-dataset","title":"QT Dataset","text":"<p>Over 100 fifteen-minute two-lead ECG recordings with onset, peak, and end markers for P, QRS, T, and (where present) U waves of from 30 to 50 selected beats in each recording. Please visit Physionet QTDB for more details.</p> <p>Heart Tasks: Segmentation, HRV</p>"},{"location":"datasets/#mit-bih-arrhythmia-dataset","title":"MIT BIH Arrhythmia Dataset","text":"<p>This dataset consists of ECG recordings from 47 different subjects recorded at a sampling rate of 360 Hz. 23 records (numbered from 100 to 124 inclusive with some numbers missing) chosen at random from this set, and 25 records (numbered from 200 to 234 inclusive, again with some numbers missing) selected from the same set to include a variety of rare but clinically important phenomena that would not be well-represented by a small random sample of Holter recordings. Each of the 48 records is slightly over 30 minutes long. Please visit Physionet MITDB for more details.</p> <p>Heart Tasks: Arrhythmia</p>"},{"location":"datasets/#mit-bih-normal-sinus-rhythm-dataset","title":"MIT BIH Normal Sinus Rhythm Dataset","text":"<p>This dataset includes 18 long-term ECG recordings of subjects referred to the Arrhythmia Laboratory at Boston's Beth Israel Hospital (now the Beth Israel Deaconess Medical Center). Subjects included in this dataset were found to have had no significant arrhythmias; they include 5 men, aged 26 to 45, and 13 women, aged 20 to 50. Please visit Physionet NSRDB for more details.</p> <p>Heart Tasks: HRV</p>"},{"location":"datasets/#ptb-diagnostics-dataset","title":"PTB Diagnostics Dataset","text":"<p>This dataset consists of ECG records from 290 subjects: 148 diagnosed as MI, 52 healthy controls, and the rest are diagnosed with 7 different disease. Each record contains ECG signals from 12 leads sampled at the frequency of 1000 Hz. Please visit Physionet PTBDB for more details.</p>"},{"location":"overview/","title":"Overview","text":"<p>HeartKit can be used as either a CLI-based app or as a python package to perform advanced experimentation. In both forms, HeartKit exposes a number of modes and tasks discussed below:</p>"},{"location":"overview/#modes","title":"Modes","text":"<ul> <li><code>download</code>: Download datasets</li> <li><code>train</code>: Train a model for specified task and dataset(s)</li> <li><code>evaluate</code>: Evaluate a model for specified task and dataset(s)</li> <li><code>export</code>: Export a trained model to TensorFlow Lite and TFLM</li> <li><code>demo</code>: Run full demo on PC or EVB</li> </ul>"},{"location":"overview/#tasks","title":"Tasks","text":"<ul> <li><code>Segmentation</code>: Perform ECG based segmentation (P-Wave, QRS, T-Wave)</li> <li><code>HRV</code>: Heart rate, rhythm, HRV metrics (RR interval)</li> <li><code>Arrhythmia</code>: Heart arrhythmia detection (AFIB, AFL)</li> <li><code>Beat</code>: Classify individual beats (PAC, PVC)</li> </ul>"},{"location":"overview/#using-cli","title":"Using CLI","text":"<p>The HeartKit command line interface (CLI) makes it easy to run a variefy of single-line commands without the need for writing any code. You can rull all tasks and modes from the terminal with the <code>heartkit</code> command.</p> <pre><code>$ heartkit --help\n\nHeartKit CLI Options:\n    --task [segmentation, arrhythmia, beat, hrv]\n    --mode [download, train, evaluate, export, demo]\n    --config [\"./path/to/config.json\", or '{\"raw: \"json\"}']\n</code></pre> <p>Note</p> <p>Before running commands, be sure to activate python environment: <code>poetry shell</code>. On Windows using Powershell, use <code>.venv\\Scripts\\activate.ps1</code>.</p>"},{"location":"overview/#1-download-datasets","title":"1. Download Datasets","text":"<p>The <code>download</code> command is used to download all datasets specified in the configuration file. Please refer to Datasets for details on the available datasets.</p> <p>The following example will download and prepare all currently used datasets.</p> <p>Example</p> CLIPython <pre><code>heartkit --mode download --config ./configs/download-datasets.json\n</code></pre> <pre><code>import heartkit as hk\nhk.datasets.download_datasets(hk.defines.HeartDownloadParams(\nds_path=\"./datasets\",\ndatasets=[\"icentia11k\", \"ludb\", \"qtdb\", \"synthetic\"],\nprogress=True\n))\n</code></pre> <p>Note</p> <p>The Icentia11k dataset requires roughly 200 GB of disk space and can take around 2 hours to download.</p>"},{"location":"overview/#2-train-model","title":"2. Train Model","text":"<p>The <code>train</code> command is used to train a HeartKit model. The following command will train the arrhythmia model using the reference configuration. Please refer to <code>heartkit/defines.py</code> to see supported options.</p> <p>Example</p> CLIPython <pre><code>heartkit --task arrhythmia --mode train --config ./configs/train-arrhythmia-model.json\n</code></pre> <pre><code>import heartkit as hk\nhk.arrhythmia.train_model(hk.defines.HeartTrainParams(\njob_dir=\"./results/arrhythmia\",\nds_path=\"./datasets\",\nsampling_rate=200,\nframe_size=800,\nsamples_per_patient=[100, 800, 800],\nval_samples_per_patient=[100, 800, 800],\ntrain_patients=10000,\nval_patients=0.10,\nval_size=200000,\nbatch_size=256,\nbuffer_size=100000,\nepochs=100,\nsteps_per_epoch=20,\nval_metric=\"loss\",\ndatasets=[\"icentia11k\"]\n))\n</code></pre>"},{"location":"overview/#3-evaluate-model","title":"3. Evaluate Model","text":"<p>The <code>evaluate</code> command will evaluate the performance of the model on the reserved test set. A confidence threshold can also be set such that a label is only assigned when the model's probability is greater than the threshold; otherwise, a label of inconclusive will be assigned.</p> <p>Example</p> CLIPython <pre><code>heartkit --task arrhythmia --mode evaluate --config ./configs/evaluate-arrhythmia-model.json\n</code></pre> <pre><code>import heartkit as hk\nhk.arrhythmia.evaluate_model(hk.defines.HeartTestParams(\njob_dir=\"./results/arrhythmia\",\nds_path=\"./datasets\",\nsampling_rate=200,\nframe_size=800,\nsamples_per_patient=[100, 800, 800],\ntest_patients=1000,\ntest_size=100000,\nmodel_file=\"./results/arrhythmia/model.tf\",\nthreshold=0.95\n))\n</code></pre>"},{"location":"overview/#4-export-model","title":"4. Export Model","text":"<p>The <code>export</code> command will convert the trained TensorFlow model into both TensorFlow Lite (TFL) and TensorFlow Lite for microcontroller (TFLM) variants. The command will also verify the models' outputs match. Post-training quantization can also be enabled by setting the <code>quantization</code> flag in the configuration.</p> <p>Example</p> CLIPython <pre><code>heartkit --task arrhythmia --mode export --config ./configs/export-arrhythmia-model.json\n</code></pre> <pre><code>import heartkit as hk\nhk.arrhythmia.export_model(hk.defines.HeartExportParams(\njob_dir=\"./results/arrhythmia\",\nds_path=\"./datasets\",\nsampling_rate=200,\nframe_size=800,\nsamples_per_patient=[100, 500, 100],\nmodel_file=\"./results/arrhythmia/model.tf\",\nquantization=true,\nthreshold=0.95,\ntflm_var_name=\"g_arrhythmia_model\",\ntflm_file=\"./evb/src/arrhythmia_model_buffer.h\"\n))\n</code></pre> <p>Once converted, the TFLM header file will be copied to location specified by <code>tflm_file</code>. If parameters were changed (e.g. window size, quantization), <code>./evb/src/constants.h</code> will need to be updated accordingly.</p>"},{"location":"overview/#5-demo","title":"5. Demo","text":"<p>The <code>demo</code> command is used to run a full-fledged HeartKit demonstration. The demo is decoupled into three tasks: (1) a REST server to provide a unified API, (2) a front-end UI, and (3) a backend to fetch samples and perform inference. The host PC performs tasks (1) and (2). For (3), the trained models can run on either the <code>PC</code> or an Apollo 4 evaluation board (<code>EVB</code>) by setting the <code>backend</code> field in the configuration. When the <code>PC</code> backend is selected, the host PC will perform task (3) entirely to fetch samples and perform inference. When the <code>EVB</code> backend is selected, the <code>EVB</code> will perform inference using either sensor data or prior data. The PC connects to the <code>EVB</code> via RPC over serial transport to provide sample data and capture inference results.</p> <p>Please refer to Arrhythmia Demo Tutorial and HeartKit Demo Tutorial for further instructions.</p>"},{"location":"results/","title":"Results","text":""},{"location":"results/#overview","title":"Overview","text":"<p>The following table provides performance and accuracy results of all models when running on Apollo4 Plus EVB.</p> Task Params FLOPS Metric Cycles/Inf Time/Inf Segmentation 33K 6.5M 87.0% IOU 531ms 102M Arrhythmia 50K 3.6M 99.0% F1 465ms 89M Beat 73K 2.2M 91.5% F1 241ms 46M"},{"location":"results/#segmentation-results","title":"Segmentation Results","text":"Segmentation Confusion Matrix"},{"location":"results/#heart-arrhythmia-results","title":"Heart Arrhythmia Results","text":"<p>The results of the arrhythmia model when testing on 1,000 patients (not used during training) is summarized below. The baseline model is simply selecting the argmax of model outputs (<code>normal</code>, <code>AFIB/AFL</code>). The 75% confidence version adds inconclusive label that is assigned when softmax output is less than 75% for any model output.</p> Metric Baseline 75% Confidence Accuracy 96.5% 99.1% F1 Score 96.4% 99.0% Drop 0.0% 12.0% <p>The confusion matrix for the 75% confidence model is depicted below.</p> <p> </p> Arrhythmia Confusion Matrix"},{"location":"results/#beat-classification-results","title":"Beat Classification Results","text":"<p>The results of three beat models when testing on 1,000 patients (not used during training) are summarized below. The 800x1 model serves as the baseline and classifies individual beats (1 channel) with a fixed time window of 800 ms (160 samples). The 2400x1 model increases the time window to 2,400 ms (480 samples) in order to include surrounding data as context. Increasing the time window increases the accuracy by over <code>10%</code> but also causes computation to increase by <code>3.5x</code>. The third and best model uses a time window of 800 ms to capture individual beats but includes two additional channels. Using the local average RR interval, the previous and subsequent <code>beats</code> are included as side channels. Unlike normal beats, premature and ectopic beats won't be aligned to neighboring beats and serves as useful context. This provides similar temporal resolution as 800x1 but reduces computation by <code>3.3x</code> while further improving accuracy by <code>1.7%</code>.</p> Model 800x1 2400x1 800x3 Parameters 73K 73K 73K FLOPS 2.1M 7.6M 2.2M Accuracy 78.2% 88.6% 90.3% F1 Score 77.5% 87.2% 90.1% <p>The confusion matrix for the 800x3 model is depicted below using a confidence threshold of 50%.</p> <p> </p> Beat Confusion Matrix"},{"location":"results/#hrv-results","title":"HRV Results","text":"<p>The HRV metrics are computed using off-the-shelf definitions based purely on the output of the segmentation and beat models. The current metrics include heart rate, rhythm, and RR variation. We intend to include additional metrics later such as QTc along with frequency metrics.</p>"},{"location":"api/datasets/","title":"Datasets","text":"<p>See Datasets for information about available datasets.</p>"},{"location":"api/datasets/#heartkit.datasets.defines","title":"<code>heartkit.datasets.defines</code>","text":""},{"location":"api/datasets/#heartkit.datasets.download","title":"<code>heartkit.datasets.download</code>","text":""},{"location":"api/datasets/#heartkit.datasets.download.download_datasets","title":"<code>download_datasets(params)</code>","text":"<p>Download all specified datasets.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartDownloadParams</code>)         \u2013 <p>Download parameters</p> </li> </ul> Source code in <code>heartkit/datasets/download.py</code> <pre><code>def download_datasets(params: HeartDownloadParams):\n\"\"\"Download all specified datasets.\n    Args:\n        params (HeartDownloadParams): Download parameters\n    \"\"\"\nos.makedirs(params.ds_path, exist_ok=True)\nif \"icentia11k\" in params.datasets:\nIcentiaDataset(str(params.ds_path)).download(\nnum_workers=params.data_parallelism,\nforce=params.force,\n)\nif \"ludb\" in params.datasets:\nLudbDataset(str(params.ds_path)).download(\nnum_workers=params.data_parallelism,\nforce=params.force,\n)\nif \"qtdb\" in params.datasets:\nQtdbDataset(str(params.ds_path)).download(\nnum_workers=params.data_parallelism,\nforce=params.force,\n)\nif \"ptbxl\" in params.datasets:\nPtbxlDataset(str(params.ds_path)).download(\nnum_workers=params.data_parallelism,\nforce=params.force,\n)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.augmentation","title":"<code>heartkit.datasets.augmentation</code>","text":""},{"location":"api/datasets/#heartkit.datasets.augmentation.augment_pipeline","title":"<code>augment_pipeline(x, augmentations, sample_rate)</code>","text":"<p>Apply augmentation pipeline</p> <p>Parameters:</p> <ul> <li> x             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> augmentations             (<code>list[AugmentationParams]</code>)         \u2013 <p>Augmentations to apply</p> </li> <li> sample_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Augmented signal</p> </li> </ul> Source code in <code>heartkit/datasets/augmentation.py</code> <pre><code>def augment_pipeline(x: npt.NDArray, augmentations: list[AugmentationParams], sample_rate: float) -&gt; npt.NDArray:\n\"\"\"Apply augmentation pipeline\n    Args:\n        x (npt.NDArray): Signal\n        augmentations (list[AugmentationParams]): Augmentations to apply\n        sample_rate: Sampling rate in Hz.\n    Returns:\n        npt.NDArray: Augmented signal\n    \"\"\"\nfor augmentation in augmentations:\nargs = augmentation.args\nif augmentation.name == \"baseline_wander\":\namplitude = args.get(\"amplitude\", [0.05, 0.06])\nfrequency = args.get(\"frequency\", [0, 1])\nx = signal.add_baseline_wander(\nx,\namplitude=np.random.uniform(amplitude[0], amplitude[1]),\nfrequency=np.random.uniform(frequency[0], frequency[1]),\nsample_rate=sample_rate,\n)\nelif augmentation.name == \"motion_noise\":\namplitude = args.get(\"amplitude\", [0.5, 1.0])\nfrequency = args.get(\"frequency\", [0.4, 0.6])\nx = signal.add_motion_noise(\nx,\namplitude=np.random.uniform(amplitude[0], amplitude[1]),\nfrequency=np.random.uniform(frequency[0], frequency[1]),\nsample_rate=sample_rate,\n)\nelif augmentation.name == \"burst_noise\":\namplitude = args.get(\"amplitude\", [0.05, 0.5])\nfrequency = args.get(\"frequency\", [sample_rate / 4, sample_rate / 2])\nburst_number = args.get(\"burst_number\", [0, 2])\nx = signal.add_burst_noise(\nx,\namplitude=np.random.uniform(amplitude[0], amplitude[1]),\nfrequency=np.random.uniform(frequency[0], frequency[1]),\nburst_number=np.random.randint(burst_number[0], burst_number[1]),\nsample_rate=sample_rate,\n)\nelif augmentation.name == \"powerline_noise\":\namplitude = args.get(\"amplitude\", [0.005, 0.01])\nfrequency = args.get(\"frequency\", [50, 60])\nx = signal.add_powerline_noise(\nx,\namplitude=np.random.uniform(amplitude[0], amplitude[1]),\nfrequency=np.random.uniform(frequency[0], frequency[1]),\nsample_rate=sample_rate,\n)\nelif augmentation.name == \"noise_sources\":\nnum_sources = args.get(\"num_sources\", [1, 2])\namplitude = args.get(\"amplitude\", [0, 0.1])\nfrequency = args.get(\"frequency\", [0, sample_rate / 2])\nnum_sources: int = np.random.randint(num_sources[0], num_sources[1])\nx = signal.add_noise_sources(\nx,\namplitudes=[np.random.uniform(amplitude[0], amplitude[1]) for _ in range(num_sources)],\nfrequencies=[np.random.uniform(frequency[0], frequency[1]) for _ in range(num_sources)],\nsample_rate=sample_rate,\n)\nreturn x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.augmentation.baseline_wander","title":"<code>baseline_wander(y, scale=0.001)</code>","text":"<p>Apply baseline wander</p> <p>Parameters:</p> <ul> <li> y             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> scale             (<code>float</code>)         \u2013 <p>Noise scale. Defaults to 1e-3.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: New signal</p> </li> </ul> Source code in <code>heartkit/datasets/augmentation.py</code> <pre><code>def baseline_wander(y: npt.NDArray, scale: float = 1e-3) -&gt; npt.NDArray:\n\"\"\"Apply baseline wander\n    Args:\n        y (npt.NDArray): Signal\n        scale (float, optional): Noise scale. Defaults to 1e-3.\n    Returns:\n        npt.NDArray: New signal\n    \"\"\"\nskew = np.linspace(0, random.uniform(0, 2) * np.pi, y.size)\nskew = np.sin(skew) * random.uniform(scale / 10, scale)\ny = y + skew\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.augmentation.emg_noise","title":"<code>emg_noise(y, scale=1e-05, sampling_frequency=1000)</code>","text":"<p>Add EMG noise to signal</p> <p>Parameters:</p> <ul> <li> y             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> scale             (<code>float</code>)         \u2013 <p>Noise scale. Defaults to 1e-5.</p> </li> <li> sampling_frequency             (<code>int</code>)         \u2013 <p>Sampling rate in Hz. Defaults to 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: New signal</p> </li> </ul> Source code in <code>heartkit/datasets/augmentation.py</code> <pre><code>def emg_noise(y: npt.NDArray, scale: float = 1e-5, sampling_frequency: int = 1000) -&gt; npt.NDArray:\n\"\"\"Add EMG noise to signal\n    Args:\n        y (npt.NDArray): Signal\n        scale (float, optional): Noise scale. Defaults to 1e-5.\n        sampling_frequency (int, optional): Sampling rate in Hz. Defaults to 1000.\n    Returns:\n        npt.NDArray: New signal\n    \"\"\"\nnoise = np.repeat(\nnp.sin(np.linspace(-0.5 * np.pi, 1.5 * np.pi, sampling_frequency) * 10000),\nnp.ceil(y.size / sampling_frequency),\n)\nreturn y + scale * noise[: y.size]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.augmentation.lead_noise","title":"<code>lead_noise(y, scale=1)</code>","text":"<p>Add Lead noise</p> <p>Parameters:</p> <ul> <li> y             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> scale             (<code>float</code>)         \u2013 <p>Noise scale. Defaults to 1.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: New signal</p> </li> </ul> Source code in <code>heartkit/datasets/augmentation.py</code> <pre><code>def lead_noise(y: npt.NDArray, scale: float = 1) -&gt; npt.NDArray:\n\"\"\"Add Lead noise\n    Args:\n        y (npt.NDArray): Signal\n        scale (float, optional): Noise scale. Defaults to 1.\n    Returns:\n        npt.NDArray: New signal\n    \"\"\"\nreturn y + np.random.normal(-scale, scale, size=y.shape)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.augmentation.random_scaling","title":"<code>random_scaling(y, lower=0.5, upper=2.0)</code>","text":"<p>Randomly scale signal.</p> <p>Parameters:</p> <ul> <li> y             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> lower             (<code>float</code>)         \u2013 <p>Lower bound. Defaults to 0.5.</p> </li> <li> upper             (<code>float</code>)         \u2013 <p>Upper bound. Defaults to 2.0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: New signal</p> </li> </ul> Source code in <code>heartkit/datasets/augmentation.py</code> <pre><code>def random_scaling(y: npt.NDArray, lower: float = 0.5, upper: float = 2.0) -&gt; npt.NDArray:\n\"\"\"Randomly scale signal.\n    Args:\n        y (npt.NDArray): Signal\n        lower (float, optional): Lower bound. Defaults to 0.5.\n        upper (float, optional): Upper bound. Defaults to 2.0.\n    Returns:\n        npt.NDArray: New signal\n    \"\"\"\nreturn y * random.uniform(lower, upper)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess","title":"<code>heartkit.datasets.preprocess</code>","text":""},{"location":"api/datasets/#heartkit.datasets.preprocess.filter_signal","title":"<code>filter_signal(data, lowcut, highcut, sample_rate, order=3, axis=0)</code>","text":"<p>Apply band-pass filter to signal using butterworth design and forward-backward cascaded filter</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> lowcut             (<code>float</code>)         \u2013 <p>Lower cutoff in Hz</p> </li> <li> highcut             (<code>float</code>)         \u2013 <p>Upper cutoff in Hz</p> </li> <li> sample_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz</p> </li> <li> order             (<code>int</code>)         \u2013 <p>Filter order. Defaults to 3.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Filtered signal</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def filter_signal(\ndata: npt.NDArray,\nlowcut: float,\nhighcut: float,\nsample_rate: float,\norder: int = 3,\naxis: int = 0,\n) -&gt; npt.NDArray:\n\"\"\"Apply band-pass filter to signal using butterworth design and forward-backward cascaded filter\n    Args:\n        data (npt.NDArray): Signal\n        lowcut (float): Lower cutoff in Hz\n        highcut (float): Upper cutoff in Hz\n        sample_rate (float): Sampling rate in Hz\n        order (int, optional): Filter order. Defaults to 3.\n    Returns:\n        npt.NDArray: Filtered signal\n    \"\"\"\nsos = get_butter_bp_sos(lowcut=lowcut, highcut=highcut, sample_rate=sample_rate, order=order)\nreturn scipy.signal.sosfiltfilt(sos, data, axis=axis)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.generate_arm_biquad_sos","title":"<code>generate_arm_biquad_sos(lowcut, highcut, sample_rate, order=3, var_name='biquadFilter')</code>","text":"<p>Generate ARM second order section coefficients.</p> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def generate_arm_biquad_sos(\nlowcut: float,\nhighcut: float,\nsample_rate: float,\norder: int = 3,\nvar_name: str = \"biquadFilter\",\n):\n\"\"\"Generate ARM second order section coefficients.\"\"\"\nsos = get_butter_bp_sos(lowcut=lowcut, highcut=highcut, sample_rate=sample_rate, order=order)\n# Each section needs to be mapped as follows:\n#   [b0, b1, b2, a0, a1, a2] -&gt; [b0, b1, b2, -a1, -a2]\nsec_len_name = f\"{var_name.upper()}_NUM_SECS\"\narm_sos = sos[:, [0, 1, 2, 4, 5]] * [1, 1, 1, -1, -1]\ncoef_str = \", \".join(f\"{os.linesep:&lt;4}{c}\" if i % 5 == 0 else f\"{c}\" for i, c in enumerate(arm_sos.flatten()))\narm_str = (\nf\"#define {sec_len_name} ({order:0d}){os.linesep}\"\nf\"static float32_t {var_name}State[2 * {sec_len_name}];{os.linesep}\"\nf\"static float32_t {var_name}[5 * {sec_len_name}] = {{ {coef_str}\\n}};{os.linesep}\"\n)\nreturn arm_str\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.get_butter_bp_sos","title":"<code>get_butter_bp_sos(lowcut, highcut, sample_rate, order=3)</code>  <code>cached</code>","text":"<p>Compute band-pass filter coefficients as SOS. This function caches.</p> <p>Parameters:</p> <ul> <li> lowcut             (<code>float</code>)         \u2013 <p>Lower cutoff in Hz</p> </li> <li> highcut             (<code>float</code>)         \u2013 <p>Upper cutoff in Hz</p> </li> <li> sample_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz</p> </li> <li> order             (<code>int</code>)         \u2013 <p>Filter order. Defaults to 3.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: SOS</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>@functools.cache\ndef get_butter_bp_sos(\nlowcut: float,\nhighcut: float,\nsample_rate: float,\norder: int = 3,\n) -&gt; npt.NDArray:\n\"\"\"Compute band-pass filter coefficients as SOS. This function caches.\n    Args:\n        lowcut (float): Lower cutoff in Hz\n        highcut (float): Upper cutoff in Hz\n        sample_rate (float): Sampling rate in Hz\n        order (int, optional): Filter order. Defaults to 3.\n    Returns:\n        npt.NDArray: SOS\n    \"\"\"\nnyq = 0.5 * sample_rate\nlow = lowcut / nyq\nhigh = highcut / nyq\nsos = scipy.signal.butter(order, [low, high], btype=\"band\", output=\"sos\")\nreturn sos\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.normalize_signal","title":"<code>normalize_signal(data, eps=0.001, axis=0)</code>","text":"<p>Normalize signal about its mean and std.</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> eps             (<code>float</code>)         \u2013 <p>Epsilon added to st. dev. Defaults to 1e-3.</p> </li> <li> axis             (<code>int</code>)         \u2013 <p>Axis to normalize along. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Normalized signal</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def normalize_signal(data: npt.NDArray, eps: float = 1e-3, axis: int = 0) -&gt; npt.NDArray:\n\"\"\"Normalize signal about its mean and std.\n    Args:\n        data (npt.NDArray): Signal\n        eps (float, optional): Epsilon added to st. dev. Defaults to 1e-3.\n        axis (int, optional): Axis to normalize along. Defaults to 0.\n    Returns:\n        npt.NDArray: Normalized signal\n    \"\"\"\nmu = np.nanmean(data, axis=axis)\nstd = np.nanstd(data, axis=axis) + eps\nreturn (data - mu) / std\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.pad_signal","title":"<code>pad_signal(x, max_len=None, padding='pre')</code>","text":"<p>Pads signal shorter than <code>max_len</code> and trims those longer than <code>max_len</code>.</p> <p>Parameters:</p> <ul> <li> x             (<code>npt.NDArray</code>)         \u2013 <p>Array of sequences.</p> </li> <li> max_len             (<code>int | None</code>)         \u2013 <p>Maximum length of sequence. Defaults to longest.</p> </li> <li> padding             (<code>Literal['pre', 'post']</code>)         \u2013 <p>Before or after sequence. Defaults to pre.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray Array of padded sequences.</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def pad_signal(\nx: npt.NDArray,\nmax_len: int | None = None,\npadding: Literal[\"pre\", \"post\"] = \"pre\",\n) -&gt; npt.NDArray:\n\"\"\"Pads signal shorter than `max_len` and trims those longer than `max_len`.\n    Args:\n        x (npt.NDArray): Array of sequences.\n        max_len (int | None, optional): Maximum length of sequence. Defaults to longest.\n        padding (Literal[\"pre\", \"post\"]): Before or after sequence. Defaults to pre.\n    Returns:\n        npt.NDArray Array of padded sequences.\n    \"\"\"\nif max_len is None:\nmax_len = max(map(len, x))\nx_shape = x[0].shape\nx_dtype = x[0].dtype\nx_padded = np.zeros((len(x), max_len) + x_shape[1:], dtype=x_dtype)\nfor i, x_i in enumerate(x):\ntrim_len = min(max_len, len(x_i))\nif padding == \"pre\":\nx_padded[i, -trim_len:] = x_i[-trim_len:]\nelif padding == \"post\":\nx_padded[i, :trim_len] = x_i[:trim_len]\nelse:\nraise ValueError(f\"Unknown padding: {padding}\")\nreturn x_padded\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.preprocess_signal","title":"<code>preprocess_signal(data, sample_rate, target_rate=None)</code>","text":"<p>Pre-process signal</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> sample_rate             (<code>float</code>)         \u2013 <p>Sampling rate (Hz)</p> </li> <li> target_rate             (<code>float</code>)         \u2013 <p>Target sampling rate (Hz)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Pre-processed signal</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def preprocess_signal(data: npt.NDArray, sample_rate: float, target_rate: float | None = None) -&gt; npt.NDArray:\n\"\"\"Pre-process signal\n    Args:\n        data (npt.NDArray): Signal\n        sample_rate (float): Sampling rate (Hz)\n        target_rate (float): Target sampling rate (Hz)\n    Returns:\n        npt.NDArray: Pre-processed signal\n    \"\"\"\naxis = 0\nnorm_en = True\nnorm_eps = 1e-6\nfilter_en = True\nfilt_lo = 0.5\nfilt_hi = 30\nresample_en = target_rate is not None and sample_rate != target_rate\nx = np.copy(data)\nif filter_en:\nx = filter_signal(x, lowcut=filt_lo, highcut=filt_hi, sample_rate=sample_rate, axis=axis)\nif resample_en:\nx = resample_signal(x, sample_rate=sample_rate, target_rate=target_rate, axis=axis)\nif norm_en:\nx = normalize_signal(x, eps=norm_eps)\nreturn x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.resample_signal","title":"<code>resample_signal(data, sample_rate, target_rate, axis=0)</code>","text":"<p>Resample signal using scipy FFT-based resample routine.</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>Signal</p> </li> <li> sample_rate             (<code>float</code>)         \u2013 <p>Signal sampling rate</p> </li> <li> target_rate             (<code>float</code>)         \u2013 <p>Target sampling rate</p> </li> <li> axis             (<code>int</code>)         \u2013 <p>Axis to resample along. Defaults to 0.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Resampled signal</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def resample_signal(data: npt.NDArray, sample_rate: float, target_rate: float, axis: int = 0) -&gt; npt.NDArray:\n\"\"\"Resample signal using scipy FFT-based resample routine.\n    Args:\n        data (npt.NDArray): Signal\n        sample_rate (float): Signal sampling rate\n        target_rate (float): Target sampling rate\n        axis (int, optional): Axis to resample along. Defaults to 0.\n    Returns:\n        npt.NDArray: Resampled signal\n    \"\"\"\nratio = target_rate / sample_rate\ndesired_length = int(np.round(data.shape[axis] * ratio))\nreturn scipy.signal.resample(data, desired_length, axis=axis)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.rolling_standardize","title":"<code>rolling_standardize(x, win_len)</code>","text":"<p>Performs rolling standardization</p> <p>Parameters:</p> <ul> <li> x             (<code>npt.NDArray</code>)         \u2013 <p>Data</p> </li> <li> win_len             (<code>int</code>)         \u2013 <p>Window length</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Standardized data</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def rolling_standardize(x: npt.NDArray, win_len: int) -&gt; npt.NDArray:\n\"\"\"Performs rolling standardization\n    Args:\n        x (npt.NDArray): Data\n        win_len (int): Window length\n    Returns:\n        npt.NDArray: Standardized data\n    \"\"\"\nx_roll = np.lib.stride_tricks.sliding_window_view(x, win_len)\nx_roll_std = np.std(x_roll, axis=-1)\nx_roll_mu = np.mean(x_roll, axis=-1)\nx_std = np.concatenate((np.repeat(x_roll_std[0], x.shape[0] - x_roll_std.shape[0]), x_roll_std))\nx_mu = np.concatenate((np.repeat(x_roll_mu[0], x.shape[0] - x_roll_mu.shape[0]), x_roll_mu))\nx_norm = (x - x_mu) / x_std\nreturn x_norm\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.preprocess.running_mean_std","title":"<code>running_mean_std(iterator, dtype=None)</code>","text":"<p>Calculate mean and standard deviation while iterating over the data iterator.     iterator (Iterable): Data iterator.     dtype (npt.DTypeLike | None): Type of accumulators.</p> <p>Returns:</p> <ul> <li> <code>tuple[float, float]</code>         \u2013 <p>tuple[float, float]; mean, Std.</p> </li> </ul> Source code in <code>heartkit/datasets/preprocess.py</code> <pre><code>def running_mean_std(iterator, dtype: npt.DTypeLike | None = None) -&gt; tuple[float, float]:\n\"\"\"Calculate mean and standard deviation while iterating over the data iterator.\n        iterator (Iterable): Data iterator.\n        dtype (npt.DTypeLike | None): Type of accumulators.\n    Returns:\n        tuple[float, float]; mean, Std.\n    \"\"\"\nsum_x = np.zeros((), dtype=dtype)\nsum_x2 = np.zeros((), dtype=dtype)\nn = 0\nfor x in iterator:\nsum_x += np.sum(x, dtype=dtype)\nsum_x2 += np.sum(x**2, dtype=dtype)\nn += x.size\nmean = sum_x / n\nstd = np.math.sqrt((sum_x2 / n) - (mean**2))\nreturn mean, std\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k","title":"<code>heartkit.datasets.icentia11k</code>","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaBeat","title":"<code>IcentiaBeat</code>","text":"<p>         Bases: <code>IntEnum</code></p> <p>Incentia beat labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>class IcentiaBeat(IntEnum):\n\"\"\"Incentia beat labels\"\"\"\nundefined = 0\nnormal = 1\npac = 2\n# aberrated = 3\npvc = 4\n@classmethod\ndef hi_priority(cls) -&gt; list[int]:\n\"\"\"High priority labels\"\"\"\nreturn [cls.pac, cls.pvc]\n@classmethod\ndef lo_priority(cls) -&gt; list[int]:\n\"\"\"Low priority labels\"\"\"\nreturn [cls.undefined, cls.normal]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaBeat.hi_priority","title":"<code>hi_priority()</code>  <code>classmethod</code>","text":"<p>High priority labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>@classmethod\ndef hi_priority(cls) -&gt; list[int]:\n\"\"\"High priority labels\"\"\"\nreturn [cls.pac, cls.pvc]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaBeat.lo_priority","title":"<code>lo_priority()</code>  <code>classmethod</code>","text":"<p>Low priority labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>@classmethod\ndef lo_priority(cls) -&gt; list[int]:\n\"\"\"Low priority labels\"\"\"\nreturn [cls.undefined, cls.normal]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset","title":"<code>IcentiaDataset</code>","text":"<p>         Bases: <code>HeartKitDataset</code></p> <p>Icentia dataset</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>class IcentiaDataset(HeartKitDataset):\n\"\"\"Icentia dataset\"\"\"\ndef __init__(\nself,\nds_path: str,\ntask: HeartTask = HeartTask.arrhythmia,\nframe_size: int = 1250,\ntarget_rate: int = 250,\n) -&gt; None:\nsuper().__init__(os.path.join(ds_path, \"icentia11k\"), task, frame_size, target_rate)\n@property\ndef sampling_rate(self) -&gt; int:\n\"\"\"Sampling rate in Hz\"\"\"\nreturn 250\n@property\ndef mean(self) -&gt; float:\n\"\"\"Dataset mean\"\"\"\nreturn 0.0018\n@property\ndef std(self) -&gt; float:\n\"\"\"Dataset st dev\"\"\"\nreturn 1.3711\n@property\ndef patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn np.arange(11_000)\ndef get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get training patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[:10_000]\ndef get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get patient IDs reserved for testing only\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[10_000:]\ndef _pt_key(self, patient_id: int):\nreturn f\"p{patient_id:05d}\"\n@functools.cached_property\ndef arr_rhythm_patients(self) -&gt; npt.NDArray:\n\"\"\"Find all patients with arrhythmia events. This takes roughly 10 secs.\n        Returns:\n            npt.NDArray: Patient ids\n        \"\"\"\npatient_ids = self.patient_ids.tolist()\nwith Pool() as pool:\narr_pts_bool = list(pool.imap(self._pt_has_rhythm_arrhythmia, patient_ids))\npatient_ids = np.where(arr_pts_bool)[0]\nreturn patient_ids\ndef task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n        Args:\n            patient_generator (PatientGenerator): Patient data generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample data generator\n        \"\"\"\nif self.task == HeartTask.arrhythmia:\nreturn self.rhythm_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nif self.task == HeartTask.beat:\nreturn self.beat_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nif self.task == HeartTask.hrv:\nreturn self.heart_rate_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\ndef _split_train_test_patients(self, patient_ids: npt.NDArray, test_size: float) -&gt; list[list[int]]:\n\"\"\"Perform train/test split on patients for given task.\n        NOTE: We only perform inter-patient splits and not intra-patient.\n        Args:\n            patient_ids (npt.NDArray): Patient Ids\n            test_size (float): Test size\n        Returns:\n            list[list[int]]: Train and test sets of patient ids\n        \"\"\"\n# Use stratified split for arrhythmia task\nif self.task == HeartTask.arrhythmia:\narr_pt_ids = np.intersect1d(self.arr_rhythm_patients, patient_ids)\nnorm_pt_ids = np.setdiff1d(patient_ids, arr_pt_ids)\n(\nnorm_train_pt_ids,\nnorm_val_pt_ids,\n) = sklearn.model_selection.train_test_split(norm_pt_ids, test_size=test_size)\n(\narr_train_pt_ids,\nafib_val_pt_ids,\n) = sklearn.model_selection.train_test_split(arr_pt_ids, test_size=test_size)\ntrain_pt_ids = np.concatenate((norm_train_pt_ids, arr_train_pt_ids))\nval_pt_ids = np.concatenate((norm_val_pt_ids, afib_val_pt_ids))\nnp.random.shuffle(train_pt_ids)\nnp.random.shuffle(val_pt_ids)\nreturn train_pt_ids, val_pt_ids\n# END IF\nreturn sklearn.model_selection.train_test_split(patient_ids, test_size=test_size)\ndef rhythm_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and rhythm label using patient generator.\n        Args:\n            patient_generator (PatientGenerator): Patient Generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\ntgt_labels = (\nIcentiaRhythm.normal,\nIcentiaRhythm.afib,\nIcentiaRhythm.aflut,\n)\nif isinstance(samples_per_patient, Iterable):\nsamples_per_tgt = samples_per_patient\nelse:\nnum_per_tgt = int(max(1, samples_per_patient / len(tgt_labels)))\nsamples_per_tgt = num_per_tgt * [len(tgt_labels)]\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\n# Group patient rhythms by type (segment, start, stop, delta)\nfor _, segments in patient_generator:\n# This maps segment index to segment key\nseg_map: list[str] = list(segments.keys())\npt_tgt_seg_map = [[] for _ in tgt_labels]\nfor seg_idx, seg_key in enumerate(seg_map):\nrlabels = segments[seg_key][\"rlabels\"][:]\nif not rlabels.shape[0]:\ncontinue\nrlabels = rlabels[np.where(rlabels[:, 1] != IcentiaRhythm.noise.value)[0]]\nxs, xe, xl = rlabels[0::2, 0], rlabels[1::2, 0], rlabels[0::2, 1]\nfor tgt_idx, tgt_class in enumerate(tgt_labels):\nidxs = np.where((xe - xs &gt;= input_size) &amp; (xl == tgt_class))\nseg_vals = np.vstack((seg_idx * np.ones_like(idxs), xs[idxs], xe[idxs])).T\npt_tgt_seg_map[tgt_idx] += seg_vals.tolist()\n# END FOR\n# END FOR\npt_tgt_seg_map = [np.array(b) for b in pt_tgt_seg_map]\n# Grab target segments\nseg_samples: list[tuple[int, int, int, int]] = []\nfor tgt_idx, tgt_class in enumerate(tgt_labels):\ntgt_segments = pt_tgt_seg_map[tgt_idx]\nif not tgt_segments.shape[0]:\ncontinue\ntgt_seg_indices: list[int] = random.choices(\nnp.arange(tgt_segments.shape[0]),\nweights=tgt_segments[:, 2] - tgt_segments[:, 1],\nk=samples_per_tgt[tgt_idx],\n)\nfor tgt_seg_idx in tgt_seg_indices:\nseg_idx, rhy_start, rhy_end = tgt_segments[tgt_seg_idx]\nframe_start = np.random.randint(rhy_start, rhy_end - input_size + 1)\nframe_end = frame_start + input_size\nseg_samples.append((seg_idx, frame_start, frame_end, HeartRhythmMap[tgt_class]))\n# END FOR\n# END FOR\n# Shuffle segments\nrandom.shuffle(seg_samples)\nfor seg_idx, frame_start, frame_end, label in seg_samples:\nx: npt.NDArray = segments[seg_map[seg_idx]][\"data\"][frame_start:frame_end].astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nyield x, label\n# END FOR\n# END FOR\ndef beat_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and beat label using patient generator.\n        There are over 2.5 billion normal and undefined while less than 40 million arrhythmia beats.\n        The following routine sorts each patient's beats by type and then approx. uniformly samples them by amount requested.\n        We start with arrhythmia types followed by undefined and normal. For each beat we resplit remaining samples requested.\n        Args:\n            patient_generator (PatientGenerator): Patient generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\nnlabel_threshold = 0.25\nblabel_padding = 20\nrr_win_len = int(10 * self.sampling_rate)\nrr_min_len = int(0.3 * self.sampling_rate)\nrr_max_len = int(2.0 * self.sampling_rate)\ntgt_beat_labels = [\nIcentiaBeat.normal,\nIcentiaBeat.pac,\nIcentiaBeat.pvc,\n# IcentiaBeat.undefined,\n]\nif isinstance(samples_per_patient, Iterable):\nsamples_per_tgt = samples_per_patient\nelse:\nnum_per_tgt = int(max(1, samples_per_patient / len(tgt_beat_labels)))\nsamples_per_tgt = num_per_tgt * [len(tgt_beat_labels)]\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\n# For each patient\nfor _, segments in patient_generator:\n# This maps segment index to segment key\nseg_map: list[str] = list(segments.keys())\n# Capture beat locations for each segment\npt_beat_map = [[] for _ in tgt_beat_labels]\nfor seg_idx, seg_key in enumerate(seg_map):\nblabels = segments[seg_key][\"blabels\"][:]\nnum_blabels = blabels.shape[0]\nif num_blabels &lt;= 0:\ncontinue\n# END IF\nnum_nlabels = np.sum(blabels[:, 1] == IcentiaBeat.normal)\nif num_nlabels / num_blabels &lt; nlabel_threshold:\ncontinue\n# Capture all beat locations\nfor tgt_beat_idx, beat in enumerate(tgt_beat_labels):\nbeat_idxs = np.where(blabels[blabel_padding:-blabel_padding, 1] == beat.value)[0] + blabel_padding\nif beat == IcentiaBeat.normal:\nfilt_func = lambda i: blabels[i - 1, 1] == blabels[i + 1, 1] == IcentiaBeat.normal\nelif beat in (IcentiaBeat.pac, IcentiaBeat.pvc):\nfilt_func = lambda i: IcentiaBeat.undefined not in (\nblabels[i - 1, 1],\nblabels[i + 1, 1],\n)\nelif beat == IcentiaBeat.undefined:\nfilt_func = lambda i: blabels[i - 1, 1] == blabels[i + 1, 1] == IcentiaBeat.undefined\nelse:\nfilt_func = lambda _: True\nbeat_idxs = filter(filt_func, beat_idxs)\npt_beat_map[tgt_beat_idx] += [(seg_idx, blabels[i, 0]) for i in beat_idxs]\n# END FOR\n# END FOR\npt_beat_map = [np.array(b) for b in pt_beat_map]\n# Randomly select N samples of each target beat\npt_segs_beat_idxs: list[tuple[int, int, int]] = []\nfor tgt_beat_idx, beat in enumerate(tgt_beat_labels):\ntgt_beats = pt_beat_map[tgt_beat_idx]\ntgt_count = min(samples_per_tgt[tgt_beat_idx], len(tgt_beats))\ntgt_idxs = np.random.choice(np.arange(len(tgt_beats)), size=tgt_count, replace=False)\npt_segs_beat_idxs += [(tgt_beats[i][0], tgt_beats[i][1], HeartBeatMap[beat]) for i in tgt_idxs]\n# END FOR\n# Shuffle all\nrandom.shuffle(pt_segs_beat_idxs)\n# Yield selected samples for patient\nfor seg_idx, beat_idx, beat in pt_segs_beat_idxs:\nframe_start = max(0, beat_idx - int(random.uniform(0.4722, 0.5278) * input_size))\nframe_end = frame_start + input_size\ndata = segments[seg_map[seg_idx]][\"data\"]\nblabels = segments[seg_map[seg_idx]][\"blabels\"]\nrr_xs = np.searchsorted(blabels[:, 0], max(0, frame_start - rr_win_len))\nrr_xe = np.searchsorted(blabels[:, 0], frame_end + rr_win_len)\nif rr_xe &lt;= rr_xs:\ncontinue\nblabel_diffs = np.diff(blabels[rr_xs : rr_xe + 1, 0])\nblabel_diffs = blabel_diffs[(blabel_diffs &gt; rr_min_len) &amp; (blabel_diffs &lt; rr_max_len)]\nif blabel_diffs.size &lt;= 0:\ncontinue\navg_rr = int(np.mean(blabel_diffs))\n# avg_rr = input_size\nif frame_start - avg_rr &lt; 0 or frame_end + avg_rr &gt;= data.shape[0]:\ncontinue\nx = np.hstack(\n(\ndata[frame_start - avg_rr : frame_end - avg_rr],\ndata[frame_start:frame_end],\ndata[frame_start + avg_rr : frame_end + avg_rr],\n)\n)\nx = np.nan_to_num(x).astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\ny = beat\nyield x, y\n# END FOR\n# END FOR\ndef heart_rate_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and heart rate label using patient generator.\n        Args:\n            patient_generator (PatientGenerator): Patient generator\n            samples_per_patient (int, optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\nlabel_frame_size = self.frame_size\nmax_frame_size = max(self.frame_size, label_frame_size)\nfor _, segments in patient_generator:\nfor _ in range(samples_per_patient):\nsegment = segments[np.random.choice(list(segments.keys()))]\nsegment_size: int = segment[\"data\"].shape[0]\nframe_center = np.random.randint(segment_size - max_frame_size) + max_frame_size // 2\nsignal_frame_start = frame_center - self.frame_size // 2\nsignal_frame_end = frame_center + self.frame_size // 2\nx = segment[\"data\"][signal_frame_start:signal_frame_end]\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nlabel_frame_start = frame_center - label_frame_size // 2\nlabel_frame_end = frame_center + label_frame_size // 2\nbeat_indices = segment[\"blabels\"][:, 0]\nframe_beat_indices = self.get_complete_beats(beat_indices, start=label_frame_start, end=label_frame_end)\ny = self._get_heart_rate_label(frame_beat_indices, self.sampling_rate)\nyield x, y\n# END FOR\n# END FOR\ndef signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n        Generate frames using patient generator.\n        from the segments in patient data by placing a frame in a random location within one of the segments.\n        Args:\n            patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                    Patient data may contain only signals, since labels are not used.\n            samples_per_patient (int): Samples per patient.\n        Returns:\n            SampleGenerator: Generator of input data of shape (frame_size, 1)\n        \"\"\"\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\nfor _, segments in patient_generator:\nfor _ in range(samples_per_patient):\nsegment = segments[np.random.choice(list(segments.keys()))]\nsegment_size = segment[\"data\"].shape[0]\nframe_start = np.random.randint(segment_size - input_size)\nframe_end = frame_start + input_size\nx = segment[\"data\"][frame_start:frame_end]\nx = np.nan_to_num(x).astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nyield x\n# END FOR\n# END FOR\ndef uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n        Args:\n            patient_ids (pt.ArrayLike): Array of patient ids\n            repeat (bool, optional): Whether to repeat generator. Defaults to True.\n            shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n        Returns:\n            PatientGenerator: Patient generator\n        Yields:\n            Iterator[PatientGenerator]\n        \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = self._pt_key(patient_id)\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\npatient_data = h5[pt_key]\nyield patient_id, patient_data\n# END FOR\nif not repeat:\nbreak\n# END WHILE\ndef random_patient_generator(\nself,\npatient_ids: list[int],\npatient_weights: list[int] | None = None,\n) -&gt; PatientGenerator:\n\"\"\"Samples patient data from the provided patient distribution.\n        Args:\n            patient_ids (list[int]): Patient ids\n            patient_weights (list[int] | None, optional): Probabilities associated with each patient. Defaults to None.\n        Returns:\n            PatientGenerator: Patient generator\n        Yields:\n            Iterator[PatientGenerator]\n        \"\"\"\nwhile True:\nfor patient_id in np.random.choice(patient_ids, size=1024, p=patient_weights):\npt_key = self._pt_key(patient_id)\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\npatient_data = h5[pt_key]\nyield patient_id, patient_data\n# END FOR\n# END WHILE\ndef get_complete_beats(\nself,\nindices: npt.NDArray,\nlabels: npt.NDArray | None = None,\nstart: int = 0,\nend: int | None = None,\n) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"\n        Find all complete beats within a frame i.e. start and end of the beat lie within the frame.\n        The indices are assumed to specify the end of a heartbeat.\n        Args:\n            indices (npt.NDArray): List of sorted beat indices.\n            labels (npt.NDArray | None): List of beat labels. Defaults to None.\n            start (int): Index of the first sample in the frame. Defaults to 0.\n            end (int | None): Index of the last sample in the frame. Defaults to None.\n        Returns:\n            tuple[npt.NDArray, npt.NDArray]: (beat indices, beat labels)\n        \"\"\"\nif end is None:\nend = indices[-1]\nif start &gt;= end:\nraise ValueError(\"`end` must be greater than `start`\")\nstart_index = np.searchsorted(indices, start, side=\"left\") + 1\nend_index = np.searchsorted(indices, end, side=\"right\")\nindices_slice = indices[start_index:end_index]\nif labels is None:\nreturn indices_slice\nlabel_slice = labels[start_index:end_index]\nreturn (indices_slice, label_slice)\ndef _get_rhythm_label(self, durations: npt.NDArray, labels: npt.NDArray):\n\"\"\"Determine rhythm label based on the longest rhythm among arrhythmias.\n        Args:\n            durations (npt.NDArray): Array of rhythm durations\n            labels (npt.NDArray): Array of rhythm labels\n        Returns:\n            Rhythm label as an integer\n        \"\"\"\n# sum up the durations of each rhythm\nsummed_durations = np.zeros(len(IcentiaRhythm))\nfor rhythm in IcentiaRhythm:\nsummed_durations[rhythm.value] = durations[labels == rhythm.value].sum()\nlongest_hp_rhythm = np.argmax(summed_durations[IcentiaRhythm.hi_priority()])\nif summed_durations[IcentiaRhythm.hi_priority()][longest_hp_rhythm] &gt; 0:\ny = HeartRhythmMap[IcentiaRhythm.hi_priority()[longest_hp_rhythm]]\nelse:\nlongest_lp_rhythm = np.argmax(summed_durations[IcentiaRhythm.lo_priority()])\n# handle the case of no detected rhythm\nif summed_durations[IcentiaRhythm.lo_priority()][longest_lp_rhythm] &gt; 0:\ny = HeartRhythmMap[IcentiaRhythm.lo_priority()[longest_lp_rhythm]]\nelse:\ny = HeartRhythmMap[IcentiaRhythm.noise]\nreturn y\ndef _get_beat_label(self, labels: npt.NDArray):\n\"\"\"Determine beat label based on the occurrence of pac / abberated / pvc,\n            otherwise pick the most common beat type among the normal / undefined.\n        Args:\n            labels (npt.NDArray): Array of beat labels.\n        Returns:\n            int: Beat label as an integer.\n        \"\"\"\n# calculate the count of each beat type in the frame\nbeat_counts = np.bincount(labels, minlength=len(IcentiaBeat))\nmax_hp_idx = np.argmax(beat_counts[IcentiaBeat.hi_priority()])\nif beat_counts[IcentiaBeat.hi_priority()][max_hp_idx] &gt; 0:\ny = HeartBeatMap[IcentiaBeat.hi_priority()[max_hp_idx]]\nelse:\nmax_lp_idx = np.argmax(beat_counts[IcentiaBeat.lo_priority()])\n# handle the case of no detected beats\nif beat_counts[IcentiaBeat.lo_priority()][max_lp_idx] &gt; 0:\ny = HeartBeatMap[IcentiaBeat.lo_priority()[max_lp_idx]]\nelse:\ny = HeartBeatMap[IcentiaBeat.undefined]\nreturn y\ndef _get_heart_rate_label(self, qrs_indices, fs=None) -&gt; int:\n\"\"\"Determine the heart rate label based on an array of QRS indices (separating individual heartbeats).\n            The QRS indices are assumed to be measured in seconds if sampling frequency `fs` is not specified.\n            The heartbeat label is based on the following BPM (beats per minute) values: (0) tachycardia &lt;60 BPM,\n            (1) bradycardia &gt;100 BPM, (2) healthy 60-100 BPM, (3) noisy if QRS detection failed.\n        Args:\n            qrs_indices (list[int]): Array of QRS indices.\n            fs (float, optional): Sampling frequency of the signal. Defaults to None.\n        Returns:\n            int: Heart rate label\n        \"\"\"\nif not qrs_indices:\nreturn HeartRate.noise.value\nrr_intervals = np.diff(qrs_indices)\nif fs is not None:\nrr_intervals = rr_intervals / fs\nbpm = 60 / rr_intervals.mean()\nif bpm &lt; 60:\nreturn HeartRate.bradycardia.value\nif bpm &lt;= 100:\nreturn HeartRate.normal.value\nreturn HeartRate.tachycardia.value\ndef _pt_has_rhythm_arrhythmia(self, patient_id: int):\npt_key = self._pt_key(patient_id)\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\nfor _, segment in h5[pt_key].items():\nrlabels = segment[\"rlabels\"][:]\nif not rlabels.shape[0]:\ncontinue\nrlabels = rlabels[:, 1]\nif len(np.where((rlabels == IcentiaRhythm.afib) | (rlabels == IcentiaRhythm.aflut))[0]):\nreturn True\nreturn False\ndef get_rhythm_statistics(\nself,\npatient_ids: npt.NDArray | None = None,\nsave_path: str | None = None,\n) -&gt; pd.DataFrame:\n\"\"\"Utility function to extract rhythm statistics across entire dataset. Useful for EDA.\n        Args:\n            patient_ids (npt.NDArray | None, optional): Patients IDs to include. Defaults to all.\n            save_path (str | None, optional): Parquet file path to save results. Defaults to None.\n        Returns:\n            pd.DataFrame: DataFrame of statistics\n        \"\"\"\nif patient_ids is None:\npatient_ids = self.patient_ids\npt_gen = self.uniform_patient_generator(patient_ids=patient_ids, repeat=False)\nstats = []\nfor pt, segments in pt_gen:\n# Group patient rhythms by type (segment, start, stop)\nsegment_label_map: dict[str, list[tuple[str, int, int]]] = {}\nfor seg_key, segment in segments.items():\nrlabels = segment[\"rlabels\"][:]\nif rlabels.shape[0] == 0:\ncontinue  # Segment has no rhythm labels\nrlabels = rlabels[np.where(rlabels[:, 1] != IcentiaRhythm.noise.value)[0]]\nfor i, l in enumerate(rlabels[::2, 1]):\nif l in (\nIcentiaRhythm.noise,\nIcentiaRhythm.normal,\nIcentiaRhythm.afib,\nIcentiaRhythm.aflut,\n):\nrhy_start, rhy_stop = (\nrlabels[i * 2 + 0, 0],\nrlabels[i * 2 + 1, 0],\n)\nstats.append(\ndict(\npt=pt,\nrc=seg_key,\nrhythm=l,\nstart=rhy_start,\nstop=rhy_stop,\ndur=rhy_stop - rhy_start,\n)\n)\nsegment_label_map[l] = segment_label_map.get(l, []) + [\n(seg_key, rlabels[i * 2 + 0, 0], rlabels[i * 2 + 1, 0])\n]\n# END IF\n# END FOR\n# END FOR\n# END FOR\ndf = pd.DataFrame(stats)\nif save_path:\ndf.to_parquet(save_path)\nreturn df\ndef download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download dataset\n        Args:\n            num_workers (int | None, optional): # parallel workers. Defaults to None.\n            force (bool, optional): Force redownload. Defaults to False.\n        \"\"\"\ndef download_s3_file(\ns3_file: str,\nsave_path: str,\nbucket: str,\nclient: boto3.client,\nforce: bool = False,\n):\nif not force and os.path.exists(save_path):\nreturn\nclient.download_file(\nBucket=bucket,\nKey=s3_file,\nFilename=save_path,\n)\ns3_bucket = \"ambiqai-ecg-icentia11k-dataset\"\ns3_prefix = \"patients\"\nos.makedirs(self.ds_path, exist_ok=True)\npatient_ids = self.patient_ids\n# Creating only one session and one client\nsession = boto3.Session()\nclient = session.client(\"s3\", config=Config(signature_version=UNSIGNED))\nfunc = functools.partial(download_s3_file, bucket=s3_bucket, client=client, force=force)\nwith tqdm(desc=\"Downloading icentia11k dataset from S3\", total=len(patient_ids)) as pbar:\npt_keys = [self._pt_key(patient_id) for patient_id in patient_ids]\nwith ThreadPoolExecutor(max_workers=2 * num_workers) as executor:\nfutures = (\nexecutor.submit(\nfunc,\nf\"{s3_prefix}/{pt_key}.h5\",\nos.path.join(self.ds_path, f\"{pt_key}.h5\"),\n)\nfor pt_key in pt_keys\n)\nfor future in as_completed(futures):\nerr = future.exception()\nif err:\nprint(\"Failed on file\", err)\npbar.update(1)\n# END FOR\n# END WITH\n# END WITH\ndef download_raw_dataset(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Downloads full Icentia dataset zipfile and converts into individial patient HDF5 files.\n        NOTE: This is a very long process (e.g. 24 hrs). Please use `icentia11k.download_dataset` instead.\n        Args:\n            force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n            num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n        \"\"\"\nlogger.info(\"Downloading icentia11k dataset\")\nds_url = (\n\"https://physionet.org/static/published-projects/icentia11k-continuous-ecg/\"\n\"icentia11k-single-lead-continuous-raw-electrocardiogram-dataset-1.0.zip\"\n)\nds_zip_path = os.path.join(self.ds_path, \"icentia11k.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating icentia11k patient data\")\nself._convert_dataset_zip_to_hdf5(\nzip_path=ds_zip_path,\nforce=force,\nnum_workers=num_workers,\n)\nlogger.info(\"Finished icentia11k patient data\")\ndef _convert_dataset_pt_zip_to_hdf5(self, patient: int, zip_path: str, force: bool = False):\n\"\"\"Extract patient data from Icentia zipfile. Pulls out ECG data along with all labels.\n        Args:\n            patient (int): Patient id\n            zip_path (str): Zipfile path\n            force (bool, optional): Whether to override destination if it exists. Defaults to False.\n        \"\"\"\nimport re  # pylint: disable=import-outside-toplevel\nimport wfdb  # pylint: disable=import-outside-toplevel\n# These map Wfdb labels to icentia labels\nWfdbRhythmMap = {\"\": 0, \"(N\": 1, \"(AFIB\": 2, \"(AFL\": 3, \")\": 4}\nWfdbBeatMap = {\"Q\": 0, \"N\": 1, \"S\": 2, \"a\": 3, \"V\": 4}\nlogger.info(f\"Processing patient {patient}\")\npt_id = self._pt_key(patient)\npt_path = os.path.join(self.ds_path, f\"{pt_id}.h5\")\nif not force and os.path.exists(pt_path):\nprint(\"skipping patient\")\nreturn\nzp = zipfile.ZipFile(zip_path, mode=\"r\")  # pylint: disable=consider-using-with\nh5 = h5py.File(pt_path, mode=\"w\")\n# Find all patient .dat file indices\nzp_rec_names = filter(\nlambda f: re.match(f\"{pt_id}_[A-z0-9]+.dat\", os.path.basename(f)),\n(f.filename for f in zp.filelist),\n)\nfor zp_rec_name in zp_rec_names:\ntry:\nzp_hdr_name = zp_rec_name.replace(\".dat\", \".hea\")\nzp_atr_name = zp_rec_name.replace(\".dat\", \".atr\")\nwith tempfile.TemporaryDirectory() as tmpdir:\nrec_fpath = os.path.join(tmpdir, os.path.basename(zp_rec_name))\natr_fpath = rec_fpath.replace(\".dat\", \".atr\")\nhdr_fpath = rec_fpath.replace(\".dat\", \".hea\")\nwith open(hdr_fpath, \"wb\") as fp:\nfp.write(zp.read(zp_hdr_name))\nwith open(rec_fpath, \"wb\") as fp:\nfp.write(zp.read(zp_rec_name))\nwith open(atr_fpath, \"wb\") as fp:\nfp.write(zp.read(zp_atr_name))\nrec = wfdb.rdrecord(os.path.splitext(rec_fpath)[0], physical=True)\natr = wfdb.rdann(os.path.splitext(atr_fpath)[0], extension=\"atr\")\npt_seg_path = f\"/{os.path.splitext(os.path.basename(zp_rec_name))[0].replace('_', '/')}\"\ndata = rec.p_signal.astype(np.float16)\nblabels = np.array(\n[[atr.sample[i], WfdbBeatMap.get(s)] for i, s in enumerate(atr.symbol) if s in WfdbBeatMap],\ndtype=np.int32,\n)\nrlabels = np.array(\n[\n[atr.sample[i], WfdbRhythmMap.get(atr.aux_note[i], 0)]\nfor i, s in enumerate(atr.symbol)\nif s == \"+\"\n],\ndtype=np.int32,\n)\nh5.create_dataset(\nname=f\"{pt_seg_path}/data\",\ndata=data,\ncompression=\"gzip\",\ncompression_opts=3,\n)\nh5.create_dataset(name=f\"{pt_seg_path}/blabels\", data=blabels)\nh5.create_dataset(name=f\"{pt_seg_path}/rlabels\", data=rlabels)\nexcept Exception as err:  # pylint: disable=broad-except\nprint(f\"Failed processing {zp_rec_name}\", err)\ncontinue\nh5.close()\ndef _convert_dataset_zip_to_hdf5(\nself,\nzip_path: str,\npatient_ids: npt.NDArray | None = None,\nforce: bool = False,\nnum_workers: int | None = None,\n):\n\"\"\"Convert zipped Icentia dataset into individial patient HDF5 files.\n        Args:\n            zip_path (str): Zipfile path\n            patient_ids (npt.NDArray | None, optional): List of patient IDs to extract. Defaults to all.\n            force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n            num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n        \"\"\"\nif not patient_ids:\npatient_ids = self.patient_ids\nf = functools.partial(self._convert_dataset_pt_zip_to_hdf5, zip_path=zip_path, force=force)\nwith Pool(processes=num_workers) as pool:\n_ = list(tqdm(pool.imap(f, patient_ids), total=len(patient_ids)))\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.arr_rhythm_patients","title":"<code>arr_rhythm_patients: npt.NDArray</code>  <code>cached</code> <code>property</code>","text":"<p>Find all patients with arrhythmia events. This takes roughly 10 secs.</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: Patient ids</p> </li> </ul>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Dataset mean</p>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.patient_ids","title":"<code>patient_ids: npt.NDArray</code>  <code>property</code>","text":"<p>Get dataset patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.sampling_rate","title":"<code>sampling_rate: int</code>  <code>property</code>","text":"<p>Sampling rate in Hz</p>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.std","title":"<code>std: float</code>  <code>property</code>","text":"<p>Dataset st dev</p>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.beat_data_generator","title":"<code>beat_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and beat label using patient generator. There are over 2.5 billion normal and undefined while less than 40 million arrhythmia beats. The following routine sorts each patient's beats by type and then approx. uniformly samples them by amount requested. We start with arrhythmia types followed by undefined and normal. For each beat we resplit remaining samples requested.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def beat_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and beat label using patient generator.\n    There are over 2.5 billion normal and undefined while less than 40 million arrhythmia beats.\n    The following routine sorts each patient's beats by type and then approx. uniformly samples them by amount requested.\n    We start with arrhythmia types followed by undefined and normal. For each beat we resplit remaining samples requested.\n    Args:\n        patient_generator (PatientGenerator): Patient generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\nnlabel_threshold = 0.25\nblabel_padding = 20\nrr_win_len = int(10 * self.sampling_rate)\nrr_min_len = int(0.3 * self.sampling_rate)\nrr_max_len = int(2.0 * self.sampling_rate)\ntgt_beat_labels = [\nIcentiaBeat.normal,\nIcentiaBeat.pac,\nIcentiaBeat.pvc,\n# IcentiaBeat.undefined,\n]\nif isinstance(samples_per_patient, Iterable):\nsamples_per_tgt = samples_per_patient\nelse:\nnum_per_tgt = int(max(1, samples_per_patient / len(tgt_beat_labels)))\nsamples_per_tgt = num_per_tgt * [len(tgt_beat_labels)]\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\n# For each patient\nfor _, segments in patient_generator:\n# This maps segment index to segment key\nseg_map: list[str] = list(segments.keys())\n# Capture beat locations for each segment\npt_beat_map = [[] for _ in tgt_beat_labels]\nfor seg_idx, seg_key in enumerate(seg_map):\nblabels = segments[seg_key][\"blabels\"][:]\nnum_blabels = blabels.shape[0]\nif num_blabels &lt;= 0:\ncontinue\n# END IF\nnum_nlabels = np.sum(blabels[:, 1] == IcentiaBeat.normal)\nif num_nlabels / num_blabels &lt; nlabel_threshold:\ncontinue\n# Capture all beat locations\nfor tgt_beat_idx, beat in enumerate(tgt_beat_labels):\nbeat_idxs = np.where(blabels[blabel_padding:-blabel_padding, 1] == beat.value)[0] + blabel_padding\nif beat == IcentiaBeat.normal:\nfilt_func = lambda i: blabels[i - 1, 1] == blabels[i + 1, 1] == IcentiaBeat.normal\nelif beat in (IcentiaBeat.pac, IcentiaBeat.pvc):\nfilt_func = lambda i: IcentiaBeat.undefined not in (\nblabels[i - 1, 1],\nblabels[i + 1, 1],\n)\nelif beat == IcentiaBeat.undefined:\nfilt_func = lambda i: blabels[i - 1, 1] == blabels[i + 1, 1] == IcentiaBeat.undefined\nelse:\nfilt_func = lambda _: True\nbeat_idxs = filter(filt_func, beat_idxs)\npt_beat_map[tgt_beat_idx] += [(seg_idx, blabels[i, 0]) for i in beat_idxs]\n# END FOR\n# END FOR\npt_beat_map = [np.array(b) for b in pt_beat_map]\n# Randomly select N samples of each target beat\npt_segs_beat_idxs: list[tuple[int, int, int]] = []\nfor tgt_beat_idx, beat in enumerate(tgt_beat_labels):\ntgt_beats = pt_beat_map[tgt_beat_idx]\ntgt_count = min(samples_per_tgt[tgt_beat_idx], len(tgt_beats))\ntgt_idxs = np.random.choice(np.arange(len(tgt_beats)), size=tgt_count, replace=False)\npt_segs_beat_idxs += [(tgt_beats[i][0], tgt_beats[i][1], HeartBeatMap[beat]) for i in tgt_idxs]\n# END FOR\n# Shuffle all\nrandom.shuffle(pt_segs_beat_idxs)\n# Yield selected samples for patient\nfor seg_idx, beat_idx, beat in pt_segs_beat_idxs:\nframe_start = max(0, beat_idx - int(random.uniform(0.4722, 0.5278) * input_size))\nframe_end = frame_start + input_size\ndata = segments[seg_map[seg_idx]][\"data\"]\nblabels = segments[seg_map[seg_idx]][\"blabels\"]\nrr_xs = np.searchsorted(blabels[:, 0], max(0, frame_start - rr_win_len))\nrr_xe = np.searchsorted(blabels[:, 0], frame_end + rr_win_len)\nif rr_xe &lt;= rr_xs:\ncontinue\nblabel_diffs = np.diff(blabels[rr_xs : rr_xe + 1, 0])\nblabel_diffs = blabel_diffs[(blabel_diffs &gt; rr_min_len) &amp; (blabel_diffs &lt; rr_max_len)]\nif blabel_diffs.size &lt;= 0:\ncontinue\navg_rr = int(np.mean(blabel_diffs))\n# avg_rr = input_size\nif frame_start - avg_rr &lt; 0 or frame_end + avg_rr &gt;= data.shape[0]:\ncontinue\nx = np.hstack(\n(\ndata[frame_start - avg_rr : frame_end - avg_rr],\ndata[frame_start:frame_end],\ndata[frame_start + avg_rr : frame_end + avg_rr],\n)\n)\nx = np.nan_to_num(x).astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\ny = beat\nyield x, y\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.beat_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.download","title":"<code>download(num_workers=None, force=False)</code>","text":"<p>Download dataset</p> <p>Parameters:</p> <ul> <li> num_workers             (<code>int | None</code>)         \u2013 </li> <li> force             (<code>bool</code>)         \u2013 <p>Force redownload. Defaults to False.</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download dataset\n    Args:\n        num_workers (int | None, optional): # parallel workers. Defaults to None.\n        force (bool, optional): Force redownload. Defaults to False.\n    \"\"\"\ndef download_s3_file(\ns3_file: str,\nsave_path: str,\nbucket: str,\nclient: boto3.client,\nforce: bool = False,\n):\nif not force and os.path.exists(save_path):\nreturn\nclient.download_file(\nBucket=bucket,\nKey=s3_file,\nFilename=save_path,\n)\ns3_bucket = \"ambiqai-ecg-icentia11k-dataset\"\ns3_prefix = \"patients\"\nos.makedirs(self.ds_path, exist_ok=True)\npatient_ids = self.patient_ids\n# Creating only one session and one client\nsession = boto3.Session()\nclient = session.client(\"s3\", config=Config(signature_version=UNSIGNED))\nfunc = functools.partial(download_s3_file, bucket=s3_bucket, client=client, force=force)\nwith tqdm(desc=\"Downloading icentia11k dataset from S3\", total=len(patient_ids)) as pbar:\npt_keys = [self._pt_key(patient_id) for patient_id in patient_ids]\nwith ThreadPoolExecutor(max_workers=2 * num_workers) as executor:\nfutures = (\nexecutor.submit(\nfunc,\nf\"{s3_prefix}/{pt_key}.h5\",\nos.path.join(self.ds_path, f\"{pt_key}.h5\"),\n)\nfor pt_key in pt_keys\n)\nfor future in as_completed(futures):\nerr = future.exception()\nif err:\nprint(\"Failed on file\", err)\npbar.update(1)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.download--parallel-workers-defaults-to-none","title":"parallel workers. Defaults to None.","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.download_raw_dataset","title":"<code>download_raw_dataset(num_workers=None, force=False)</code>","text":"<p>Downloads full Icentia dataset zipfile and converts into individial patient HDF5 files. NOTE: This is a very long process (e.g. 24 hrs). Please use <code>icentia11k.download_dataset</code> instead.</p> <p>Parameters:</p> <ul> <li> force             (<code>bool</code>)         \u2013 <p>Whether to force re-download if destination exists. Defaults to False.</p> </li> <li> num_workers             (<code>int</code>)         \u2013 </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def download_raw_dataset(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Downloads full Icentia dataset zipfile and converts into individial patient HDF5 files.\n    NOTE: This is a very long process (e.g. 24 hrs). Please use `icentia11k.download_dataset` instead.\n    Args:\n        force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n        num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n    \"\"\"\nlogger.info(\"Downloading icentia11k dataset\")\nds_url = (\n\"https://physionet.org/static/published-projects/icentia11k-continuous-ecg/\"\n\"icentia11k-single-lead-continuous-raw-electrocardiogram-dataset-1.0.zip\"\n)\nds_zip_path = os.path.join(self.ds_path, \"icentia11k.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating icentia11k patient data\")\nself._convert_dataset_zip_to_hdf5(\nzip_path=ds_zip_path,\nforce=force,\nnum_workers=num_workers,\n)\nlogger.info(\"Finished icentia11k patient data\")\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.download_raw_dataset--parallel-workers-defaults-to-oscpu_count","title":"parallel workers. Defaults to os.cpu_count().","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.get_complete_beats","title":"<code>get_complete_beats(indices, labels=None, start=0, end=None)</code>","text":"<p>Find all complete beats within a frame i.e. start and end of the beat lie within the frame. The indices are assumed to specify the end of a heartbeat.</p> <p>Parameters:</p> <ul> <li> indices             (<code>npt.NDArray</code>)         \u2013 <p>List of sorted beat indices.</p> </li> <li> labels             (<code>npt.NDArray | None</code>)         \u2013 <p>List of beat labels. Defaults to None.</p> </li> <li> start             (<code>int</code>)         \u2013 <p>Index of the first sample in the frame. Defaults to 0.</p> </li> <li> end             (<code>int | None</code>)         \u2013 <p>Index of the last sample in the frame. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: (beat indices, beat labels)</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def get_complete_beats(\nself,\nindices: npt.NDArray,\nlabels: npt.NDArray | None = None,\nstart: int = 0,\nend: int | None = None,\n) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"\n    Find all complete beats within a frame i.e. start and end of the beat lie within the frame.\n    The indices are assumed to specify the end of a heartbeat.\n    Args:\n        indices (npt.NDArray): List of sorted beat indices.\n        labels (npt.NDArray | None): List of beat labels. Defaults to None.\n        start (int): Index of the first sample in the frame. Defaults to 0.\n        end (int | None): Index of the last sample in the frame. Defaults to None.\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: (beat indices, beat labels)\n    \"\"\"\nif end is None:\nend = indices[-1]\nif start &gt;= end:\nraise ValueError(\"`end` must be greater than `start`\")\nstart_index = np.searchsorted(indices, start, side=\"left\") + 1\nend_index = np.searchsorted(indices, end, side=\"right\")\nindices_slice = indices[start_index:end_index]\nif labels is None:\nreturn indices_slice\nlabel_slice = labels[start_index:end_index]\nreturn (indices_slice, label_slice)\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.get_rhythm_statistics","title":"<code>get_rhythm_statistics(patient_ids=None, save_path=None)</code>","text":"<p>Utility function to extract rhythm statistics across entire dataset. Useful for EDA.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>npt.NDArray | None</code>)         \u2013 <p>Patients IDs to include. Defaults to all.</p> </li> <li> save_path             (<code>str | None</code>)         \u2013 <p>Parquet file path to save results. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>pd.DataFrame</code>         \u2013 <p>pd.DataFrame: DataFrame of statistics</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def get_rhythm_statistics(\nself,\npatient_ids: npt.NDArray | None = None,\nsave_path: str | None = None,\n) -&gt; pd.DataFrame:\n\"\"\"Utility function to extract rhythm statistics across entire dataset. Useful for EDA.\n    Args:\n        patient_ids (npt.NDArray | None, optional): Patients IDs to include. Defaults to all.\n        save_path (str | None, optional): Parquet file path to save results. Defaults to None.\n    Returns:\n        pd.DataFrame: DataFrame of statistics\n    \"\"\"\nif patient_ids is None:\npatient_ids = self.patient_ids\npt_gen = self.uniform_patient_generator(patient_ids=patient_ids, repeat=False)\nstats = []\nfor pt, segments in pt_gen:\n# Group patient rhythms by type (segment, start, stop)\nsegment_label_map: dict[str, list[tuple[str, int, int]]] = {}\nfor seg_key, segment in segments.items():\nrlabels = segment[\"rlabels\"][:]\nif rlabels.shape[0] == 0:\ncontinue  # Segment has no rhythm labels\nrlabels = rlabels[np.where(rlabels[:, 1] != IcentiaRhythm.noise.value)[0]]\nfor i, l in enumerate(rlabels[::2, 1]):\nif l in (\nIcentiaRhythm.noise,\nIcentiaRhythm.normal,\nIcentiaRhythm.afib,\nIcentiaRhythm.aflut,\n):\nrhy_start, rhy_stop = (\nrlabels[i * 2 + 0, 0],\nrlabels[i * 2 + 1, 0],\n)\nstats.append(\ndict(\npt=pt,\nrc=seg_key,\nrhythm=l,\nstart=rhy_start,\nstop=rhy_stop,\ndur=rhy_stop - rhy_start,\n)\n)\nsegment_label_map[l] = segment_label_map.get(l, []) + [\n(seg_key, rlabels[i * 2 + 0, 0], rlabels[i * 2 + 1, 0])\n]\n# END IF\n# END FOR\n# END FOR\n# END FOR\ndf = pd.DataFrame(stats)\nif save_path:\ndf.to_parquet(save_path)\nreturn df\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.get_test_patient_ids","title":"<code>get_test_patient_ids()</code>","text":"<p>Get patient IDs reserved for testing only</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get patient IDs reserved for testing only\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[10_000:]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.get_train_patient_ids","title":"<code>get_train_patient_ids()</code>","text":"<p>Get training patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get training patient IDs\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[:10_000]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.heart_rate_data_generator","title":"<code>heart_rate_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and heart rate label using patient generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient generator</p> </li> <li> samples_per_patient             (<code>int</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def heart_rate_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and heart rate label using patient generator.\n    Args:\n        patient_generator (PatientGenerator): Patient generator\n        samples_per_patient (int, optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\nlabel_frame_size = self.frame_size\nmax_frame_size = max(self.frame_size, label_frame_size)\nfor _, segments in patient_generator:\nfor _ in range(samples_per_patient):\nsegment = segments[np.random.choice(list(segments.keys()))]\nsegment_size: int = segment[\"data\"].shape[0]\nframe_center = np.random.randint(segment_size - max_frame_size) + max_frame_size // 2\nsignal_frame_start = frame_center - self.frame_size // 2\nsignal_frame_end = frame_center + self.frame_size // 2\nx = segment[\"data\"][signal_frame_start:signal_frame_end]\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nlabel_frame_start = frame_center - label_frame_size // 2\nlabel_frame_end = frame_center + label_frame_size // 2\nbeat_indices = segment[\"blabels\"][:, 0]\nframe_beat_indices = self.get_complete_beats(beat_indices, start=label_frame_start, end=label_frame_end)\ny = self._get_heart_rate_label(frame_beat_indices, self.sampling_rate)\nyield x, y\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.heart_rate_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.random_patient_generator","title":"<code>random_patient_generator(patient_ids, patient_weights=None)</code>","text":"<p>Samples patient data from the provided patient distribution.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>list[int]</code>)         \u2013 <p>Patient ids</p> </li> <li> patient_weights             (<code>list[int] | None</code>)         \u2013 <p>Probabilities associated with each patient. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> PatientGenerator(            <code>PatientGenerator</code> )        \u2013 <p>Patient generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>PatientGenerator</code>         \u2013 <p>Iterator[PatientGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def random_patient_generator(\nself,\npatient_ids: list[int],\npatient_weights: list[int] | None = None,\n) -&gt; PatientGenerator:\n\"\"\"Samples patient data from the provided patient distribution.\n    Args:\n        patient_ids (list[int]): Patient ids\n        patient_weights (list[int] | None, optional): Probabilities associated with each patient. Defaults to None.\n    Returns:\n        PatientGenerator: Patient generator\n    Yields:\n        Iterator[PatientGenerator]\n    \"\"\"\nwhile True:\nfor patient_id in np.random.choice(patient_ids, size=1024, p=patient_weights):\npt_key = self._pt_key(patient_id)\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\npatient_data = h5[pt_key]\nyield patient_id, patient_data\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.rhythm_data_generator","title":"<code>rhythm_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and rhythm label using patient generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient Generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def rhythm_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and rhythm label using patient generator.\n    Args:\n        patient_generator (PatientGenerator): Patient Generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\ntgt_labels = (\nIcentiaRhythm.normal,\nIcentiaRhythm.afib,\nIcentiaRhythm.aflut,\n)\nif isinstance(samples_per_patient, Iterable):\nsamples_per_tgt = samples_per_patient\nelse:\nnum_per_tgt = int(max(1, samples_per_patient / len(tgt_labels)))\nsamples_per_tgt = num_per_tgt * [len(tgt_labels)]\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\n# Group patient rhythms by type (segment, start, stop, delta)\nfor _, segments in patient_generator:\n# This maps segment index to segment key\nseg_map: list[str] = list(segments.keys())\npt_tgt_seg_map = [[] for _ in tgt_labels]\nfor seg_idx, seg_key in enumerate(seg_map):\nrlabels = segments[seg_key][\"rlabels\"][:]\nif not rlabels.shape[0]:\ncontinue\nrlabels = rlabels[np.where(rlabels[:, 1] != IcentiaRhythm.noise.value)[0]]\nxs, xe, xl = rlabels[0::2, 0], rlabels[1::2, 0], rlabels[0::2, 1]\nfor tgt_idx, tgt_class in enumerate(tgt_labels):\nidxs = np.where((xe - xs &gt;= input_size) &amp; (xl == tgt_class))\nseg_vals = np.vstack((seg_idx * np.ones_like(idxs), xs[idxs], xe[idxs])).T\npt_tgt_seg_map[tgt_idx] += seg_vals.tolist()\n# END FOR\n# END FOR\npt_tgt_seg_map = [np.array(b) for b in pt_tgt_seg_map]\n# Grab target segments\nseg_samples: list[tuple[int, int, int, int]] = []\nfor tgt_idx, tgt_class in enumerate(tgt_labels):\ntgt_segments = pt_tgt_seg_map[tgt_idx]\nif not tgt_segments.shape[0]:\ncontinue\ntgt_seg_indices: list[int] = random.choices(\nnp.arange(tgt_segments.shape[0]),\nweights=tgt_segments[:, 2] - tgt_segments[:, 1],\nk=samples_per_tgt[tgt_idx],\n)\nfor tgt_seg_idx in tgt_seg_indices:\nseg_idx, rhy_start, rhy_end = tgt_segments[tgt_seg_idx]\nframe_start = np.random.randint(rhy_start, rhy_end - input_size + 1)\nframe_end = frame_start + input_size\nseg_samples.append((seg_idx, frame_start, frame_end, HeartRhythmMap[tgt_class]))\n# END FOR\n# END FOR\n# Shuffle segments\nrandom.shuffle(seg_samples)\nfor seg_idx, frame_start, frame_end, label in seg_samples:\nx: npt.NDArray = segments[seg_map[seg_idx]][\"data\"][frame_start:frame_end].astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nyield x, label\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.rhythm_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.signal_generator","title":"<code>signal_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames using patient generator. from the segments in patient data by placing a frame in a random location within one of the segments.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Generator that yields a tuple of patient id and patient data.     Patient data may contain only signals, since labels are not used.</p> </li> <li> samples_per_patient             (<code>int</code>)         \u2013 <p>Samples per patient.</p> </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Generator of input data of shape (frame_size, 1)</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n    Generate frames using patient generator.\n    from the segments in patient data by placing a frame in a random location within one of the segments.\n    Args:\n        patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                Patient data may contain only signals, since labels are not used.\n        samples_per_patient (int): Samples per patient.\n    Returns:\n        SampleGenerator: Generator of input data of shape (frame_size, 1)\n    \"\"\"\ninput_size = int(np.round((self.sampling_rate / self.target_rate) * self.frame_size))\nfor _, segments in patient_generator:\nfor _ in range(samples_per_patient):\nsegment = segments[np.random.choice(list(segments.keys()))]\nsegment_size = segment[\"data\"].shape[0]\nframe_start = np.random.randint(segment_size - input_size)\nframe_end = frame_start + input_size\nx = segment[\"data\"][frame_start:frame_end]\nx = np.nan_to_num(x).astype(np.float32)\nif self.sampling_rate != self.target_rate:\nx = resample_signal(x, self.sampling_rate, self.target_rate)\nyield x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.task_data_generator","title":"<code>task_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Task-level data generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient data generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample data generator</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n    Args:\n        patient_generator (PatientGenerator): Patient data generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample data generator\n    \"\"\"\nif self.task == HeartTask.arrhythmia:\nreturn self.rhythm_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nif self.task == HeartTask.beat:\nreturn self.beat_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nif self.task == HeartTask.hrv:\nreturn self.heart_rate_data_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.task_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaDataset.uniform_patient_generator","title":"<code>uniform_patient_generator(patient_ids, repeat=True, shuffle=True)</code>","text":"<p>Yield data for each patient in the array.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>pt.ArrayLike</code>)         \u2013 <p>Array of patient ids</p> </li> <li> repeat             (<code>bool</code>)         \u2013 <p>Whether to repeat generator. Defaults to True.</p> </li> <li> shuffle             (<code>bool</code>)         \u2013 <p>Whether to shuffle patient ids.. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> PatientGenerator(            <code>PatientGenerator</code> )        \u2013 <p>Patient generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>PatientGenerator</code>         \u2013 <p>Iterator[PatientGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>def uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n    Args:\n        patient_ids (pt.ArrayLike): Array of patient ids\n        repeat (bool, optional): Whether to repeat generator. Defaults to True.\n        shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n    Returns:\n        PatientGenerator: Patient generator\n    Yields:\n        Iterator[PatientGenerator]\n    \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = self._pt_key(patient_id)\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\npatient_data = h5[pt_key]\nyield patient_id, patient_data\n# END FOR\nif not repeat:\nbreak\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaHeartRate","title":"<code>IcentiaHeartRate</code>","text":"<p>         Bases: <code>IntEnum</code></p> <p>Icentia heart rate labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>class IcentiaHeartRate(IntEnum):\n\"\"\"Icentia heart rate labels\"\"\"\ntachycardia = 0\nbradycardia = 1\nnormal = 2\nnoise = 3\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaRhythm","title":"<code>IcentiaRhythm</code>","text":"<p>         Bases: <code>IntEnum</code></p> <p>Icentia rhythm labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>class IcentiaRhythm(IntEnum):\n\"\"\"Icentia rhythm labels\"\"\"\nnoise = 0\nnormal = 1\nafib = 2\naflut = 3\nend = 4\nunknown = 5\n@classmethod\ndef hi_priority(cls) -&gt; list[int]:\n\"\"\"High priority labels\"\"\"\nreturn [cls.afib, cls.aflut]\n@classmethod\ndef lo_priority(cls) -&gt; list[int]:\n\"\"\"Low priority labels\"\"\"\nreturn [cls.noise, cls.normal, cls.end, cls.unknown]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaRhythm.hi_priority","title":"<code>hi_priority()</code>  <code>classmethod</code>","text":"<p>High priority labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>@classmethod\ndef hi_priority(cls) -&gt; list[int]:\n\"\"\"High priority labels\"\"\"\nreturn [cls.afib, cls.aflut]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.icentia11k.IcentiaRhythm.lo_priority","title":"<code>lo_priority()</code>  <code>classmethod</code>","text":"<p>Low priority labels</p> Source code in <code>heartkit/datasets/icentia11k.py</code> <pre><code>@classmethod\ndef lo_priority(cls) -&gt; list[int]:\n\"\"\"Low priority labels\"\"\"\nreturn [cls.noise, cls.normal, cls.end, cls.unknown]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb","title":"<code>heartkit.datasets.ludb</code>","text":""},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset","title":"<code>LudbDataset</code>","text":"<p>         Bases: <code>HeartKitDataset</code></p> <p>LUDB dataset</p> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>class LudbDataset(HeartKitDataset):\n\"\"\"LUDB dataset\"\"\"\ndef __init__(\nself,\nds_path: str,\ntask: HeartTask = HeartTask.arrhythmia,\nframe_size: int = 1250,\ntarget_rate: int = 250,\n) -&gt; None:\nsuper().__init__(os.path.join(ds_path, \"ludb\"), task, frame_size, target_rate)\n@property\ndef sampling_rate(self) -&gt; int:\n\"\"\"Sampling rate in Hz\"\"\"\nreturn 500\n@property\ndef mean(self) -&gt; float:\n\"\"\"Dataset mean\"\"\"\nreturn 0\n@property\ndef std(self) -&gt; float:\n\"\"\"Dataset st dev\"\"\"\nreturn 1\n@property\ndef patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn np.arange(1, 201)\ndef get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[:180]\ndef get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[180:]\ndef task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n        Args:\n            patient_generator (PatientGenerator): Patient data generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample data generator\n        \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\ndef segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n        Args:\n            patient_generator (PatientGenerator): Patient Generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\nfor _, pt in patient_generator:\n# NOTE: [:] will load all data into RAM- ideal for small dataset\ndata = pt[\"data\"][:]\nsegs = pt[\"segmentations\"][:]\nfids = pt[\"fiducials\"][:]\nif self.sampling_rate != self.target_rate:\nratio = self.target_rate / self.sampling_rate\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\nsegs[:, (SEG_BEG_IDX, SEG_END_IDX)] = segs[:, (SEG_BEG_IDX, SEG_END_IDX)] * ratio\nfids[:, FID_LOC_IDX] = fids[:, FID_LOC_IDX] * ratio\n# END IF\n# Create segmentation mask\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX], seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nstart_offset = max(0, segs[0][SEG_BEG_IDX] - 100)\nstop_offset = max(0, data.shape[0] - segs[-1][SEG_END_IDX] + 100)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead\nlead_idx = np.random.randint(data.shape[1])\n# Randomly select frame within the segment\nframe_start = np.random.randint(start_offset, data.shape[0] - self.frame_size - stop_offset)\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32)\ny = labels[frame_start:frame_end, lead_idx].astype(np.int32)\nyield x, y\n# END FOR\n# start_offset = max(segs[0][SEG_BEG_IDX], int(0.55 * self.frame_size))\n# stop_offset = int(data.shape[0] - 0.55 * self.frame_size)\n# # Identify R peak locations and randomly shuffle\n# rfids = fids[\n#     (fids[:, FID_LBL_IDX] == 2)\n#     &amp; (start_offset &lt; fids[:, FID_LOC_IDX])\n#     &amp; (fids[:, FID_LOC_IDX] &lt; stop_offset)\n# ]\n# if rfids.shape[0] &lt;= 2:\n#     continue\n# np.random.shuffle(rfids)\n# for i in range(min(samples_per_patient, rfids.shape[0])):\n#     lead_idx = rfids[i, FID_LEAD_IDX]\n#     frame_start = max(rfids[i, FID_LOC_IDX] - int(random.uniform(0.45, 0.55) * self.frame_size), 0)\n#     frame_end = frame_start + self.frame_size\n#     if frame_end - frame_start &lt; self.frame_size:\n#         continue\n#     x = data[frame_start:frame_end, lead_idx].astype(np.float32)\n#     y = labels[frame_start:frame_end, lead_idx].astype(np.int32)\n#     yield x, y\n# # END FOR\n# END FOR\ndef signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n        Generate frames using patient generator.\n        from the segments in patient data by placing a frame in a random location within one of the segments.\n        Args:\n            patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                    Patient data may contain only signals, since labels are not used.\n            samples_per_patient (int): Samples per patient.\n        Returns:\n            SampleGenerator: Generator of input data of shape (frame_size, 1)\n        \"\"\"\nfor _, pt in patient_generator:\ndata = pt[\"data\"][:]\nif self.sampling_rate != self.target_rate:\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\n# END IF\nfor _ in range(samples_per_patient):\nlead_idx = np.random.randint(data.shape[1])\nif data.shape[0] &gt; self.frame_size:\nframe_start = np.random.randint(data.shape[0] - self.frame_size)\nelse:\nframe_start = 0\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\nyield x\n# END FOR\n# END FOR\ndef get_patient_data_segments(self, patient: int) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Get patient's entire data and segments\n        Args:\n            patient (int): Patient ID (1-based)\n        Returns:\n            tuple[npt.NDArray, npt.NDArray]: (data, segment labels)\n        \"\"\"\npt_key = f\"p{patient:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as pt:\ndata: npt.NDArray = pt[\"data\"][:]\nsegs: npt.NDArray = pt[\"segmentations\"][:]\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):  # pylint: disable=no-member\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX] + 0, seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nreturn data, labels\ndef uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n        Args:\n            patient_ids (pt.ArrayLike): Array of patient ids\n            repeat (bool, optional): Whether to repeat generator. Defaults to True.\n            shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n        Returns:\n            PatientGenerator: Patient generator\n        Yields:\n            Iterator[PatientGenerator]\n        \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = f\"p{patient_id:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\nyield patient_id, h5\n# END FOR\nif not repeat:\nbreak\n# END WHILE\ndef convert_pt_wfdb_to_hdf5(\nself, patient: int, src_path: str, dst_path: str, force: bool = False\n) -&gt; tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n\"\"\"Convert LUDB patient data from WFDB to more consumable HDF5 format.\n        Args:\n            patient (int): Patient id (1-based)\n            src_path (str): Source path to WFDB folder\n            dst_path (str): Destination path to store HDF5 file\n        Returns:\n            tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials\n        \"\"\"\nimport wfdb  # pylint: disable=import-outside-toplevel\npt_id = f\"p{patient:05d}\"\npt_src_path = os.path.join(src_path, f\"{patient}\")\nrec = wfdb.rdrecord(pt_src_path)\ndata = np.zeros_like(rec.p_signal)\nsegs = []\nfids = []\nfor i, lead in enumerate(rec.sig_name):\nlead_id = LudbLeadsMap.get(lead)\nann = wfdb.rdann(pt_src_path, extension=lead)\nseg_start = seg_stop = sym_id = None\ndata[:, lead_id] = rec.p_signal[:, i]\nfor j, symbol in enumerate(ann.symbol):\n# Start of segment\nif symbol == \"(\":\nseg_start = ann.sample[j]\nseg_stop = None\n# Fiducial / segment type\nelif symbol in LudbSymbolMap:\nsym_id = LudbSymbolMap.get(symbol)\nif seg_start is None:\nseg_start = ann.sample[j]\nfids.append([lead_id, sym_id, ann.sample[j]])\n# End of segment (start and symbol are never 0 but can be None)\nelif symbol == \")\" and seg_start and sym_id:\nseg_stop = ann.sample[j]\nsegs.append([lead_id, sym_id, seg_start, seg_stop])\nelse:\nseg_start = seg_stop = None\nsym_id = None\n# END FOR\n# END FOR\nsegs = np.array(segs)\nfids = np.array(fids)\nif dst_path:\nos.makedirs(dst_path, exist_ok=True)\npt_dst_path = os.path.join(dst_path, f\"{pt_id}.h5\")\nwith h5py.File(pt_dst_path, \"w\") as h5:\nh5.create_dataset(\"data\", data=data, compression=\"gzip\")\nh5.create_dataset(\"segmentations\", data=segs, compression=\"gzip\")\nh5.create_dataset(\"fiducials\", data=fids, compression=\"gzip\")\n# END WITH\n# END IF\nreturn data, segs, fids\ndef convert_dataset_zip_to_hdf5(\nself,\nzip_path: str,\npatient_ids: npt.NDArray | None = None,\nforce: bool = False,\nnum_workers: int | None = None,\n):\n\"\"\"Convert dataset into individial patient HDF5 files.\n        Args:\n            zip_path (str): Zip path\n            patient_ids (npt.NDArray | None, optional): List of patient IDs to extract. Defaults to all.\n            force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n            num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n        \"\"\"\nif not patient_ids:\npatient_ids = self.patient_ids\nsubdir = \"lobachevsky-university-electrocardiography-database-1.0.1\"\nwith Pool(processes=num_workers) as pool, tempfile.TemporaryDirectory() as tmpdir, zipfile.ZipFile(\nzip_path, mode=\"r\"\n) as zp:\nludb_dir = os.path.join(tmpdir, \"ludb\")\nzp.extractall(ludb_dir)\nf = functools.partial(\nself.convert_pt_wfdb_to_hdf5,\nsrc_path=os.path.join(ludb_dir, subdir, \"data\"),\ndst_path=self.ds_path,\nforce=force,\n)\n_ = list(tqdm(pool.imap(f, patient_ids), total=len(patient_ids)))\n# END WITH\ndef download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download LUDB dataset\n        Args:\n            num_workers (int | None, optional): # parallel workers. Defaults to None.\n            force (bool, optional): Force redownload. Defaults to False.\n        \"\"\"\nlogger.info(\"Downloading LUDB dataset\")\nds_url = (\n\"https://physionet.org/static/published-projects/ludb/\"\n\"lobachevsky-university-electrocardiography-database-1.0.1.zip\"\n)\nds_zip_path = os.path.join(self.ds_path, \"ludb.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating LUDB patient data\")\nself.convert_dataset_zip_to_hdf5(zip_path=ds_zip_path, force=force, num_workers=num_workers)\nlogger.info(\"Finished LUDB patient data\")\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Dataset mean</p>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.patient_ids","title":"<code>patient_ids: npt.NDArray</code>  <code>property</code>","text":"<p>Get dataset patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.sampling_rate","title":"<code>sampling_rate: int</code>  <code>property</code>","text":"<p>Sampling rate in Hz</p>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.std","title":"<code>std: float</code>  <code>property</code>","text":"<p>Dataset st dev</p>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.convert_dataset_zip_to_hdf5","title":"<code>convert_dataset_zip_to_hdf5(zip_path, patient_ids=None, force=False, num_workers=None)</code>","text":"<p>Convert dataset into individial patient HDF5 files.</p> <p>Parameters:</p> <ul> <li> zip_path             (<code>str</code>)         \u2013 <p>Zip path</p> </li> <li> patient_ids             (<code>npt.NDArray | None</code>)         \u2013 <p>List of patient IDs to extract. Defaults to all.</p> </li> <li> force             (<code>bool</code>)         \u2013 <p>Whether to force re-download if destination exists. Defaults to False.</p> </li> <li> num_workers             (<code>int</code>)         \u2013 </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def convert_dataset_zip_to_hdf5(\nself,\nzip_path: str,\npatient_ids: npt.NDArray | None = None,\nforce: bool = False,\nnum_workers: int | None = None,\n):\n\"\"\"Convert dataset into individial patient HDF5 files.\n    Args:\n        zip_path (str): Zip path\n        patient_ids (npt.NDArray | None, optional): List of patient IDs to extract. Defaults to all.\n        force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n        num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n    \"\"\"\nif not patient_ids:\npatient_ids = self.patient_ids\nsubdir = \"lobachevsky-university-electrocardiography-database-1.0.1\"\nwith Pool(processes=num_workers) as pool, tempfile.TemporaryDirectory() as tmpdir, zipfile.ZipFile(\nzip_path, mode=\"r\"\n) as zp:\nludb_dir = os.path.join(tmpdir, \"ludb\")\nzp.extractall(ludb_dir)\nf = functools.partial(\nself.convert_pt_wfdb_to_hdf5,\nsrc_path=os.path.join(ludb_dir, subdir, \"data\"),\ndst_path=self.ds_path,\nforce=force,\n)\n_ = list(tqdm(pool.imap(f, patient_ids), total=len(patient_ids)))\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.convert_dataset_zip_to_hdf5--parallel-workers-defaults-to-oscpu_count","title":"parallel workers. Defaults to os.cpu_count().","text":""},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.convert_pt_wfdb_to_hdf5","title":"<code>convert_pt_wfdb_to_hdf5(patient, src_path, dst_path, force=False)</code>","text":"<p>Convert LUDB patient data from WFDB to more consumable HDF5 format.</p> <p>Parameters:</p> <ul> <li> patient             (<code>int</code>)         \u2013 <p>Patient id (1-based)</p> </li> <li> src_path             (<code>str</code>)         \u2013 <p>Source path to WFDB folder</p> </li> <li> dst_path             (<code>str</code>)         \u2013 <p>Destination path to store HDF5 file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def convert_pt_wfdb_to_hdf5(\nself, patient: int, src_path: str, dst_path: str, force: bool = False\n) -&gt; tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n\"\"\"Convert LUDB patient data from WFDB to more consumable HDF5 format.\n    Args:\n        patient (int): Patient id (1-based)\n        src_path (str): Source path to WFDB folder\n        dst_path (str): Destination path to store HDF5 file\n    Returns:\n        tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials\n    \"\"\"\nimport wfdb  # pylint: disable=import-outside-toplevel\npt_id = f\"p{patient:05d}\"\npt_src_path = os.path.join(src_path, f\"{patient}\")\nrec = wfdb.rdrecord(pt_src_path)\ndata = np.zeros_like(rec.p_signal)\nsegs = []\nfids = []\nfor i, lead in enumerate(rec.sig_name):\nlead_id = LudbLeadsMap.get(lead)\nann = wfdb.rdann(pt_src_path, extension=lead)\nseg_start = seg_stop = sym_id = None\ndata[:, lead_id] = rec.p_signal[:, i]\nfor j, symbol in enumerate(ann.symbol):\n# Start of segment\nif symbol == \"(\":\nseg_start = ann.sample[j]\nseg_stop = None\n# Fiducial / segment type\nelif symbol in LudbSymbolMap:\nsym_id = LudbSymbolMap.get(symbol)\nif seg_start is None:\nseg_start = ann.sample[j]\nfids.append([lead_id, sym_id, ann.sample[j]])\n# End of segment (start and symbol are never 0 but can be None)\nelif symbol == \")\" and seg_start and sym_id:\nseg_stop = ann.sample[j]\nsegs.append([lead_id, sym_id, seg_start, seg_stop])\nelse:\nseg_start = seg_stop = None\nsym_id = None\n# END FOR\n# END FOR\nsegs = np.array(segs)\nfids = np.array(fids)\nif dst_path:\nos.makedirs(dst_path, exist_ok=True)\npt_dst_path = os.path.join(dst_path, f\"{pt_id}.h5\")\nwith h5py.File(pt_dst_path, \"w\") as h5:\nh5.create_dataset(\"data\", data=data, compression=\"gzip\")\nh5.create_dataset(\"segmentations\", data=segs, compression=\"gzip\")\nh5.create_dataset(\"fiducials\", data=fids, compression=\"gzip\")\n# END WITH\n# END IF\nreturn data, segs, fids\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.download","title":"<code>download(num_workers=None, force=False)</code>","text":"<p>Download LUDB dataset</p> <p>Parameters:</p> <ul> <li> num_workers             (<code>int | None</code>)         \u2013 </li> <li> force             (<code>bool</code>)         \u2013 <p>Force redownload. Defaults to False.</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download LUDB dataset\n    Args:\n        num_workers (int | None, optional): # parallel workers. Defaults to None.\n        force (bool, optional): Force redownload. Defaults to False.\n    \"\"\"\nlogger.info(\"Downloading LUDB dataset\")\nds_url = (\n\"https://physionet.org/static/published-projects/ludb/\"\n\"lobachevsky-university-electrocardiography-database-1.0.1.zip\"\n)\nds_zip_path = os.path.join(self.ds_path, \"ludb.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating LUDB patient data\")\nself.convert_dataset_zip_to_hdf5(zip_path=ds_zip_path, force=force, num_workers=num_workers)\nlogger.info(\"Finished LUDB patient data\")\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.download--parallel-workers-defaults-to-none","title":"parallel workers. Defaults to None.","text":""},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.get_patient_data_segments","title":"<code>get_patient_data_segments(patient)</code>","text":"<p>Get patient's entire data and segments</p> <p>Parameters:</p> <ul> <li> patient             (<code>int</code>)         \u2013 <p>Patient ID (1-based)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: (data, segment labels)</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def get_patient_data_segments(self, patient: int) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Get patient's entire data and segments\n    Args:\n        patient (int): Patient ID (1-based)\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: (data, segment labels)\n    \"\"\"\npt_key = f\"p{patient:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as pt:\ndata: npt.NDArray = pt[\"data\"][:]\nsegs: npt.NDArray = pt[\"segmentations\"][:]\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):  # pylint: disable=no-member\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX] + 0, seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nreturn data, labels\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.get_test_patient_ids","title":"<code>get_test_patient_ids()</code>","text":"<p>Get dataset patient IDs reserved for testing only</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[180:]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.get_train_patient_ids","title":"<code>get_train_patient_ids()</code>","text":"<p>Get dataset training patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[:180]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.segmentation_generator","title":"<code>segmentation_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and segment labels.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient Generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n    Args:\n        patient_generator (PatientGenerator): Patient Generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\nfor _, pt in patient_generator:\n# NOTE: [:] will load all data into RAM- ideal for small dataset\ndata = pt[\"data\"][:]\nsegs = pt[\"segmentations\"][:]\nfids = pt[\"fiducials\"][:]\nif self.sampling_rate != self.target_rate:\nratio = self.target_rate / self.sampling_rate\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\nsegs[:, (SEG_BEG_IDX, SEG_END_IDX)] = segs[:, (SEG_BEG_IDX, SEG_END_IDX)] * ratio\nfids[:, FID_LOC_IDX] = fids[:, FID_LOC_IDX] * ratio\n# END IF\n# Create segmentation mask\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX], seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nstart_offset = max(0, segs[0][SEG_BEG_IDX] - 100)\nstop_offset = max(0, data.shape[0] - segs[-1][SEG_END_IDX] + 100)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead\nlead_idx = np.random.randint(data.shape[1])\n# Randomly select frame within the segment\nframe_start = np.random.randint(start_offset, data.shape[0] - self.frame_size - stop_offset)\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32)\ny = labels[frame_start:frame_end, lead_idx].astype(np.int32)\nyield x, y\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.segmentation_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.signal_generator","title":"<code>signal_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames using patient generator. from the segments in patient data by placing a frame in a random location within one of the segments.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Generator that yields a tuple of patient id and patient data.     Patient data may contain only signals, since labels are not used.</p> </li> <li> samples_per_patient             (<code>int</code>)         \u2013 <p>Samples per patient.</p> </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Generator of input data of shape (frame_size, 1)</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n    Generate frames using patient generator.\n    from the segments in patient data by placing a frame in a random location within one of the segments.\n    Args:\n        patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                Patient data may contain only signals, since labels are not used.\n        samples_per_patient (int): Samples per patient.\n    Returns:\n        SampleGenerator: Generator of input data of shape (frame_size, 1)\n    \"\"\"\nfor _, pt in patient_generator:\ndata = pt[\"data\"][:]\nif self.sampling_rate != self.target_rate:\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\n# END IF\nfor _ in range(samples_per_patient):\nlead_idx = np.random.randint(data.shape[1])\nif data.shape[0] &gt; self.frame_size:\nframe_start = np.random.randint(data.shape[0] - self.frame_size)\nelse:\nframe_start = 0\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\nyield x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.task_data_generator","title":"<code>task_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Task-level data generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient data generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample data generator</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n    Args:\n        patient_generator (PatientGenerator): Patient data generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample data generator\n    \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.task_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.ludb.LudbDataset.uniform_patient_generator","title":"<code>uniform_patient_generator(patient_ids, repeat=True, shuffle=True)</code>","text":"<p>Yield data for each patient in the array.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>pt.ArrayLike</code>)         \u2013 <p>Array of patient ids</p> </li> <li> repeat             (<code>bool</code>)         \u2013 <p>Whether to repeat generator. Defaults to True.</p> </li> <li> shuffle             (<code>bool</code>)         \u2013 <p>Whether to shuffle patient ids.. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> PatientGenerator(            <code>PatientGenerator</code> )        \u2013 <p>Patient generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>PatientGenerator</code>         \u2013 <p>Iterator[PatientGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/ludb.py</code> <pre><code>def uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n    Args:\n        patient_ids (pt.ArrayLike): Array of patient ids\n        repeat (bool, optional): Whether to repeat generator. Defaults to True.\n        shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n    Returns:\n        PatientGenerator: Patient generator\n    Yields:\n        Iterator[PatientGenerator]\n    \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = f\"p{patient_id:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\nyield patient_id, h5\n# END FOR\nif not repeat:\nbreak\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb","title":"<code>heartkit.datasets.qtdb</code>","text":""},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset","title":"<code>QtdbDataset</code>","text":"<p>         Bases: <code>HeartKitDataset</code></p> <p>QT dataset</p> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>class QtdbDataset(HeartKitDataset):\n\"\"\"QT dataset\"\"\"\ndef __init__(\nself,\nds_path: str,\ntask: HeartTask = HeartTask.arrhythmia,\nframe_size: int = 1250,\ntarget_rate: int = 250,\n) -&gt; None:\nsuper().__init__(os.path.join(ds_path, \"qtdb\"), task, frame_size, target_rate)\n@property\ndef sampling_rate(self) -&gt; int:\n\"\"\"Sampling rate in Hz\"\"\"\nreturn 250\n@property\ndef mean(self) -&gt; float:\n\"\"\"Dataset mean\"\"\"\nreturn 0\n@property\ndef std(self) -&gt; float:\n\"\"\"Dataset st dev\"\"\"\nreturn 1\n@property\ndef patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn np.array(\n[\n30,\n31,\n32,\n33,\n34,\n35,\n36,\n37,\n38,\n39,\n40,\n41,\n42,\n43,\n44,\n45,\n46,\n47,\n48,\n49,\n50,\n51,\n52,\n100,\n102,\n103,\n104,\n106,\n107,\n110,\n111,\n112,\n114,\n116,\n117,\n121,\n122,\n123,\n124,\n126,\n129,\n133,\n136,\n166,\n170,\n203,\n210,\n211,\n213,\n221,\n223,\n230,\n231,\n232,\n233,\n301,\n302,\n303,\n306,\n307,\n308,\n310,\n405,\n406,\n409,\n411,\n509,\n603,\n604,\n606,\n607,\n609,\n612,\n704,\n803,\n808,\n811,\n820,\n821,\n840,\n847,\n853,\n871,\n872,\n873,\n883,\n891,\n14046,\n14157,\n14172,\n15814,\n16265,\n16272,\n16273,\n16420,\n16483,\n16539,\n16773,\n16786,\n16795,\n17152,\n17453,\n]\n)  # 104, 114, 116 have multiple recordings\ndef get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[:82]\ndef get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn self.patient_ids[82:]\ndef task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n        Args:\n            patient_generator (PatientGenerator): Patient data generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample data generator\n        \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\ndef segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n        Args:\n            patient_generator (PatientGenerator): Patient Generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\nfor _, pt in patient_generator:\n# NOTE: [:] will load all data into RAM- ideal for small dataset\ndata = pt[\"data\"][:]\nsegs = pt[\"segmentations\"][:]\nfids = pt[\"fiducials\"][:]\nif self.sampling_rate != self.target_rate:\nratio = self.target_rate / self.sampling_rate\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\nsegs[:, (SEG_BEG_IDX, SEG_END_IDX)] = segs[:, (SEG_BEG_IDX, SEG_END_IDX)] * ratio\nfids[:, FID_LOC_IDX] = fids[:, FID_LOC_IDX] * ratio\n# END IF\n# Create segmentation mask\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX], seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nstart_offset = max(0, segs[0][SEG_BEG_IDX] - 100)\nstop_offset = max(0, data.shape[0] - segs[-1][SEG_END_IDX] + 100)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead\nlead_idx = np.random.randint(data.shape[1])\n# Randomly select frame within the segment\nframe_start = np.random.randint(start_offset, data.shape[0] - self.frame_size - stop_offset)\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\ny = labels[frame_start:frame_end, lead_idx].astype(np.int32)\nyield x, y\n# END FOR\n# start_offset = max(segs[0][SEG_BEG_IDX], int(0.55 * self.frame_size))\n# stop_offset = int(data.shape[0] - 0.55 * self.frame_size)\n# # Identify R peak locations and randomly shuffle\n# rfids = fids[\n#     (fids[:, FID_LBL_IDX] == 2)\n#     &amp; (start_offset &lt; fids[:, FID_LOC_IDX])\n#     &amp; (fids[:, FID_LOC_IDX] &lt; stop_offset)\n# ]\n# if rfids.shape[0] &lt;= 2:\n#     continue\n# np.random.shuffle(rfids)\n# num_samples = 0\n# for i in range(rfids.shape[0]):\n#     lead_idx = rfids[i, FID_LEAD_IDX]\n#     frame_start = max(rfids[i, FID_LOC_IDX] - int(random.uniform(0.45, 0.55) * self.frame_size), 0)\n#     frame_end = frame_start + self.frame_size\n#     if frame_end - frame_start &lt; self.frame_size:\n#         continue\n#     x = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\n#     y = labels[frame_start:frame_end, lead_idx].astype(np.int32)\n#     # Should contain all fiducials\n#     if np.intersect1d(y, [1, 2, 3]).size &lt; 3:\n#         continue\n#     yield x, y\n#     num_samples += 1\n#     if num_samples &gt; samples_per_patient:\n#         break\n# # END FOR\n# END FOR\ndef signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n        Generate frames using patient generator.\n        from the segments in patient data by placing a frame in a random location within one of the segments.\n        Args:\n            patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                    Patient data may contain only signals, since labels are not used.\n            samples_per_patient (int): Samples per patient.\n        Returns:\n            SampleGenerator: Generator of input data of shape (frame_size, 1)\n        \"\"\"\nfor _, pt in patient_generator:\ndata = pt[\"data\"][:]\nif self.sampling_rate != self.target_rate:\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\n# END IF\nfor _ in range(samples_per_patient):\nlead_idx = np.random.randint(data.shape[1])\nif data.shape[0] &gt; self.frame_size:\nframe_start = np.random.randint(data.shape[0] - self.frame_size)\nelse:\nframe_start = 0\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\nyield x\n# END FOR\n# END FOR\ndef get_patient_data_segments(self, patient: int) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Get patient's entire data and segments\n        Args:\n            patient (int): Patient ID (1-based)\n        Returns:\n            tuple[npt.NDArray, npt.NDArray]: (data, segment labels)\n        \"\"\"\npt_key = f\"p{patient:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as pt:\ndata: npt.NDArray = pt[\"data\"][:]\nsegs: npt.NDArray = pt[\"segmentations\"][:]\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):  # pylint: disable=no-member\nseg = segs[seg_idx]\nlabels[seg[2] : seg[3] + 0, seg[0]] = seg[1]\nreturn data, labels\ndef uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n        Args:\n            patient_ids (pt.ArrayLike): Array of patient ids\n            repeat (bool, optional): Whether to repeat generator. Defaults to True.\n            shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n        Returns:\n            PatientGenerator: Patient generator\n        Yields:\n            Iterator[PatientGenerator]\n        \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = f\"{patient_id}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\nyield patient_id, h5\n# END FOR\nif not repeat:\nbreak\n# END WHILE\ndef convert_pt_wfdb_to_hdf5(\nself, patient: int, src_path: str, dst_path: str, force: bool = False\n) -&gt; tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n\"\"\"Convert QTDB patient data from WFDB to more consumable HDF5 format.\n        Args:\n            patient (int): Patient id (1-based)\n            src_path (str): Source path to WFDB folder\n            dst_path (str): Destination path to store HDF5 file\n        Returns:\n            tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials\n        \"\"\"\nimport wfdb  # pylint: disable=import-outside-toplevel\npt_id = f\"sel{patient}\" if os.path.isfile(os.path.join(src_path, f\"sel{patient}.dat\")) else f\"sele{patient:04d}\"\npt_src_path = os.path.join(src_path, pt_id)\nrec = wfdb.rdrecord(pt_src_path)\ndata = np.zeros_like(rec.p_signal)\nsegs = []\nfids = []\nfor i, _ in enumerate(rec.sig_name):\nlead_id = i\nann = wfdb.rdann(pt_src_path, extension=f\"pu{lead_id}\")\nseg_start = seg_stop = sym_id = None\ndata[:, lead_id] = rec.p_signal[:, i]\nfor j, symbol in enumerate(ann.symbol):\n# Start of segment\nif symbol == \"(\":\nseg_start = ann.sample[j]\nseg_stop = None\n# Fiducial / segment type\nelif symbol in QtdbSymbolMap:\nsym_id = QtdbSymbolMap.get(symbol)\nif seg_start is None:\nseg_start = ann.sample[j]\nfids.append([lead_id, sym_id, ann.sample[j]])\nelif symbol == \")\" and seg_start and sym_id:\nseg_stop = ann.sample[j]\nsegs.append([lead_id, sym_id, seg_start, seg_stop])\nelse:\nseg_start = seg_stop = None\nsym_id = None\n# END FOR\n# END FOR\nfids = np.array(fids)\nsegs = np.array(segs)\nif dst_path:\nos.makedirs(dst_path, exist_ok=True)\npt_dst_path = os.path.join(dst_path, f\"{patient}.h5\")\nwith h5py.File(pt_dst_path, \"w\") as h5:\nh5.create_dataset(\"data\", data=data, compression=\"gzip\")\nh5.create_dataset(\"segmentations\", data=segs, compression=\"gzip\")\nh5.create_dataset(\"fiducials\", data=fids, compression=\"gzip\")\n# END WITH\n# END IF\nreturn data, segs, fids\ndef convert_dataset_zip_to_hdf5(\nself,\nzip_path: str,\npatient_ids: npt.NDArray | None = None,\nforce: bool = False,\nnum_workers: int | None = None,\n):\n\"\"\"Convert dataset into individial patient HDF5 files.\n        Args:\n            zip_path (str): Zip path\n            patient_ids (npt.NDArray | None, optional): List of patient IDs to extract. Defaults to all.\n            force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n            num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n        \"\"\"\nif not patient_ids:\npatient_ids = self.patient_ids\nsubdir = \"qt-database-1.0.0\"\nwith Pool(processes=num_workers) as pool, tempfile.TemporaryDirectory() as tmpdir, zipfile.ZipFile(\nzip_path, mode=\"r\"\n) as zp:\nqtdb_dir = os.path.join(tmpdir, \"qtdb\")\nzp.extractall(qtdb_dir)\nf = functools.partial(\nself.convert_pt_wfdb_to_hdf5,\nsrc_path=os.path.join(qtdb_dir, subdir),\ndst_path=self.ds_path,\nforce=force,\n)\n_ = list(tqdm(pool.imap(f, patient_ids), total=len(patient_ids)))\n# END WITH\ndef download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download QT dataset\n        Args:\n            num_workers (int | None, optional): # parallel workers. Defaults to None.\n            force (bool, optional): Force redownload. Defaults to False.\n        \"\"\"\nlogger.info(\"Downloading QTDB dataset\")\nds_url = \"https://physionet.org/static/published-projects/qtdb/qt-database-1.0.0.zip\"\nds_zip_path = os.path.join(self.ds_path, \"qtdb.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating QT patient data\")\nself.convert_dataset_zip_to_hdf5(zip_path=ds_zip_path, force=force, num_workers=num_workers)\nlogger.info(\"Finished QTDB patient data\")\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Dataset mean</p>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.patient_ids","title":"<code>patient_ids: npt.NDArray</code>  <code>property</code>","text":"<p>Get dataset patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.sampling_rate","title":"<code>sampling_rate: int</code>  <code>property</code>","text":"<p>Sampling rate in Hz</p>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.std","title":"<code>std: float</code>  <code>property</code>","text":"<p>Dataset st dev</p>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.convert_dataset_zip_to_hdf5","title":"<code>convert_dataset_zip_to_hdf5(zip_path, patient_ids=None, force=False, num_workers=None)</code>","text":"<p>Convert dataset into individial patient HDF5 files.</p> <p>Parameters:</p> <ul> <li> zip_path             (<code>str</code>)         \u2013 <p>Zip path</p> </li> <li> patient_ids             (<code>npt.NDArray | None</code>)         \u2013 <p>List of patient IDs to extract. Defaults to all.</p> </li> <li> force             (<code>bool</code>)         \u2013 <p>Whether to force re-download if destination exists. Defaults to False.</p> </li> <li> num_workers             (<code>int</code>)         \u2013 </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def convert_dataset_zip_to_hdf5(\nself,\nzip_path: str,\npatient_ids: npt.NDArray | None = None,\nforce: bool = False,\nnum_workers: int | None = None,\n):\n\"\"\"Convert dataset into individial patient HDF5 files.\n    Args:\n        zip_path (str): Zip path\n        patient_ids (npt.NDArray | None, optional): List of patient IDs to extract. Defaults to all.\n        force (bool, optional): Whether to force re-download if destination exists. Defaults to False.\n        num_workers (int, optional): # parallel workers. Defaults to os.cpu_count().\n    \"\"\"\nif not patient_ids:\npatient_ids = self.patient_ids\nsubdir = \"qt-database-1.0.0\"\nwith Pool(processes=num_workers) as pool, tempfile.TemporaryDirectory() as tmpdir, zipfile.ZipFile(\nzip_path, mode=\"r\"\n) as zp:\nqtdb_dir = os.path.join(tmpdir, \"qtdb\")\nzp.extractall(qtdb_dir)\nf = functools.partial(\nself.convert_pt_wfdb_to_hdf5,\nsrc_path=os.path.join(qtdb_dir, subdir),\ndst_path=self.ds_path,\nforce=force,\n)\n_ = list(tqdm(pool.imap(f, patient_ids), total=len(patient_ids)))\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.convert_dataset_zip_to_hdf5--parallel-workers-defaults-to-oscpu_count","title":"parallel workers. Defaults to os.cpu_count().","text":""},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.convert_pt_wfdb_to_hdf5","title":"<code>convert_pt_wfdb_to_hdf5(patient, src_path, dst_path, force=False)</code>","text":"<p>Convert QTDB patient data from WFDB to more consumable HDF5 format.</p> <p>Parameters:</p> <ul> <li> patient             (<code>int</code>)         \u2013 <p>Patient id (1-based)</p> </li> <li> src_path             (<code>str</code>)         \u2013 <p>Source path to WFDB folder</p> </li> <li> dst_path             (<code>str</code>)         \u2013 <p>Destination path to store HDF5 file</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def convert_pt_wfdb_to_hdf5(\nself, patient: int, src_path: str, dst_path: str, force: bool = False\n) -&gt; tuple[npt.NDArray, npt.NDArray, npt.NDArray]:\n\"\"\"Convert QTDB patient data from WFDB to more consumable HDF5 format.\n    Args:\n        patient (int): Patient id (1-based)\n        src_path (str): Source path to WFDB folder\n        dst_path (str): Destination path to store HDF5 file\n    Returns:\n        tuple[npt.NDArray, npt.NDArray, npt.NDArray]: data, segments, and fiducials\n    \"\"\"\nimport wfdb  # pylint: disable=import-outside-toplevel\npt_id = f\"sel{patient}\" if os.path.isfile(os.path.join(src_path, f\"sel{patient}.dat\")) else f\"sele{patient:04d}\"\npt_src_path = os.path.join(src_path, pt_id)\nrec = wfdb.rdrecord(pt_src_path)\ndata = np.zeros_like(rec.p_signal)\nsegs = []\nfids = []\nfor i, _ in enumerate(rec.sig_name):\nlead_id = i\nann = wfdb.rdann(pt_src_path, extension=f\"pu{lead_id}\")\nseg_start = seg_stop = sym_id = None\ndata[:, lead_id] = rec.p_signal[:, i]\nfor j, symbol in enumerate(ann.symbol):\n# Start of segment\nif symbol == \"(\":\nseg_start = ann.sample[j]\nseg_stop = None\n# Fiducial / segment type\nelif symbol in QtdbSymbolMap:\nsym_id = QtdbSymbolMap.get(symbol)\nif seg_start is None:\nseg_start = ann.sample[j]\nfids.append([lead_id, sym_id, ann.sample[j]])\nelif symbol == \")\" and seg_start and sym_id:\nseg_stop = ann.sample[j]\nsegs.append([lead_id, sym_id, seg_start, seg_stop])\nelse:\nseg_start = seg_stop = None\nsym_id = None\n# END FOR\n# END FOR\nfids = np.array(fids)\nsegs = np.array(segs)\nif dst_path:\nos.makedirs(dst_path, exist_ok=True)\npt_dst_path = os.path.join(dst_path, f\"{patient}.h5\")\nwith h5py.File(pt_dst_path, \"w\") as h5:\nh5.create_dataset(\"data\", data=data, compression=\"gzip\")\nh5.create_dataset(\"segmentations\", data=segs, compression=\"gzip\")\nh5.create_dataset(\"fiducials\", data=fids, compression=\"gzip\")\n# END WITH\n# END IF\nreturn data, segs, fids\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.download","title":"<code>download(num_workers=None, force=False)</code>","text":"<p>Download QT dataset</p> <p>Parameters:</p> <ul> <li> num_workers             (<code>int | None</code>)         \u2013 </li> <li> force             (<code>bool</code>)         \u2013 <p>Force redownload. Defaults to False.</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download QT dataset\n    Args:\n        num_workers (int | None, optional): # parallel workers. Defaults to None.\n        force (bool, optional): Force redownload. Defaults to False.\n    \"\"\"\nlogger.info(\"Downloading QTDB dataset\")\nds_url = \"https://physionet.org/static/published-projects/qtdb/qt-database-1.0.0.zip\"\nds_zip_path = os.path.join(self.ds_path, \"qtdb.zip\")\nos.makedirs(self.ds_path, exist_ok=True)\nif os.path.exists(ds_zip_path) and not force:\nlogger.warning(\nf\"Zip file already exists. Please delete or set `force` flag to redownload. PATH={ds_zip_path}\"\n)\nelse:\ndownload_file(ds_url, ds_zip_path, progress=True)\n# 2. Extract and convert patient ECG data to H5 files\nlogger.info(\"Generating QT patient data\")\nself.convert_dataset_zip_to_hdf5(zip_path=ds_zip_path, force=force, num_workers=num_workers)\nlogger.info(\"Finished QTDB patient data\")\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.download--parallel-workers-defaults-to-none","title":"parallel workers. Defaults to None.","text":""},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.get_patient_data_segments","title":"<code>get_patient_data_segments(patient)</code>","text":"<p>Get patient's entire data and segments</p> <p>Parameters:</p> <ul> <li> patient             (<code>int</code>)         \u2013 <p>Patient ID (1-based)</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: (data, segment labels)</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def get_patient_data_segments(self, patient: int) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Get patient's entire data and segments\n    Args:\n        patient (int): Patient ID (1-based)\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: (data, segment labels)\n    \"\"\"\npt_key = f\"p{patient:05d}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as pt:\ndata: npt.NDArray = pt[\"data\"][:]\nsegs: npt.NDArray = pt[\"segmentations\"][:]\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):  # pylint: disable=no-member\nseg = segs[seg_idx]\nlabels[seg[2] : seg[3] + 0, seg[0]] = seg[1]\nreturn data, labels\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.get_test_patient_ids","title":"<code>get_test_patient_ids()</code>","text":"<p>Get dataset patient IDs reserved for testing only</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[82:]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.get_train_patient_ids","title":"<code>get_train_patient_ids()</code>","text":"<p>Get dataset training patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nreturn self.patient_ids[:82]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.segmentation_generator","title":"<code>segmentation_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and segment labels.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient Generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n    Args:\n        patient_generator (PatientGenerator): Patient Generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\nfor _, pt in patient_generator:\n# NOTE: [:] will load all data into RAM- ideal for small dataset\ndata = pt[\"data\"][:]\nsegs = pt[\"segmentations\"][:]\nfids = pt[\"fiducials\"][:]\nif self.sampling_rate != self.target_rate:\nratio = self.target_rate / self.sampling_rate\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\nsegs[:, (SEG_BEG_IDX, SEG_END_IDX)] = segs[:, (SEG_BEG_IDX, SEG_END_IDX)] * ratio\nfids[:, FID_LOC_IDX] = fids[:, FID_LOC_IDX] * ratio\n# END IF\n# Create segmentation mask\nlabels = np.zeros_like(data)\nfor seg_idx in range(segs.shape[0]):\nseg = segs[seg_idx]\nlabels[seg[SEG_BEG_IDX] : seg[SEG_END_IDX], seg[SEG_LEAD_IDX]] = seg[SEG_LBL_IDX]\n# END FOR\nstart_offset = max(0, segs[0][SEG_BEG_IDX] - 100)\nstop_offset = max(0, data.shape[0] - segs[-1][SEG_END_IDX] + 100)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead\nlead_idx = np.random.randint(data.shape[1])\n# Randomly select frame within the segment\nframe_start = np.random.randint(start_offset, data.shape[0] - self.frame_size - stop_offset)\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\ny = labels[frame_start:frame_end, lead_idx].astype(np.int32)\nyield x, y\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.segmentation_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.signal_generator","title":"<code>signal_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames using patient generator. from the segments in patient data by placing a frame in a random location within one of the segments.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Generator that yields a tuple of patient id and patient data.     Patient data may contain only signals, since labels are not used.</p> </li> <li> samples_per_patient             (<code>int</code>)         \u2013 <p>Samples per patient.</p> </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Generator of input data of shape (frame_size, 1)</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n    Generate frames using patient generator.\n    from the segments in patient data by placing a frame in a random location within one of the segments.\n    Args:\n        patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                Patient data may contain only signals, since labels are not used.\n        samples_per_patient (int): Samples per patient.\n    Returns:\n        SampleGenerator: Generator of input data of shape (frame_size, 1)\n    \"\"\"\nfor _, pt in patient_generator:\ndata = pt[\"data\"][:]\nif self.sampling_rate != self.target_rate:\ndata = resample_signal(data, self.sampling_rate, self.target_rate)\n# END IF\nfor _ in range(samples_per_patient):\nlead_idx = np.random.randint(data.shape[1])\nif data.shape[0] &gt; self.frame_size:\nframe_start = np.random.randint(data.shape[0] - self.frame_size)\nelse:\nframe_start = 0\nframe_end = frame_start + self.frame_size\nx = data[frame_start:frame_end, lead_idx].astype(np.float32).reshape((self.frame_size,))\nyield x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.task_data_generator","title":"<code>task_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Task-level data generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient data generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample data generator</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n    Args:\n        patient_generator (PatientGenerator): Patient data generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample data generator\n    \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.task_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.qtdb.QtdbDataset.uniform_patient_generator","title":"<code>uniform_patient_generator(patient_ids, repeat=True, shuffle=True)</code>","text":"<p>Yield data for each patient in the array.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>pt.ArrayLike</code>)         \u2013 <p>Array of patient ids</p> </li> <li> repeat             (<code>bool</code>)         \u2013 <p>Whether to repeat generator. Defaults to True.</p> </li> <li> shuffle             (<code>bool</code>)         \u2013 <p>Whether to shuffle patient ids.. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> PatientGenerator(            <code>PatientGenerator</code> )        \u2013 <p>Patient generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>PatientGenerator</code>         \u2013 <p>Iterator[PatientGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/qtdb.py</code> <pre><code>def uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n    Args:\n        patient_ids (pt.ArrayLike): Array of patient ids\n        repeat (bool, optional): Whether to repeat generator. Defaults to True.\n        shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n    Returns:\n        PatientGenerator: Patient generator\n    Yields:\n        Iterator[PatientGenerator]\n    \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\npt_key = f\"{patient_id}\"\nwith h5py.File(os.path.join(self.ds_path, f\"{pt_key}.h5\"), mode=\"r\") as h5:\nyield patient_id, h5\n# END FOR\nif not repeat:\nbreak\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic","title":"<code>heartkit.datasets.synthetic</code>","text":""},{"location":"api/datasets/#heartkit.datasets.synthetic.SyntheticDataset","title":"<code>SyntheticDataset</code>","text":"<p>         Bases: <code>HeartKitDataset</code></p> <p>Synthetic dataset</p> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>class SyntheticDataset(HeartKitDataset):\n\"\"\"Synthetic dataset\"\"\"\ndef __init__(\nself,\nds_path: str,\ntask: HeartTask = HeartTask.arrhythmia,\nframe_size: int = 1250,\ntarget_rate: int = 250,\nnum_pts: int = 250,\n) -&gt; None:\nsuper().__init__(os.path.join(ds_path, \"synthetic\"), task, frame_size, target_rate)\nself._num_pts = num_pts\n@property\ndef cachable(self) -&gt; bool:\n\"\"\"If dataset supports file caching.\"\"\"\nreturn False\n@property\ndef sampling_rate(self) -&gt; int:\n\"\"\"Sampling rate in Hz\"\"\"\nreturn self.target_rate\n@property\ndef mean(self) -&gt; float:\n\"\"\"Dataset mean\"\"\"\nreturn 0\n@property\ndef std(self) -&gt; float:\n\"\"\"Dataset st dev\"\"\"\nreturn 1\n@property\ndef patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nreturn np.arange(0, self._num_pts)\ndef get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nnumel = int(0.80 * self._num_pts)\nreturn self.patient_ids[:numel]\ndef get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n        Returns:\n            npt.NDArray: patient IDs\n        \"\"\"\nnumel = int(0.80 * self._num_pts)\nreturn self.patient_ids[numel:]\ndef task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n        Args:\n            patient_generator (PatientGenerator): Patient data generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample data generator\n        \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\ndef signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n        Generate frames using patient generator.\n        Args:\n            patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                    Patient data may contain only signals, since labels are not used.\n            samples_per_patient (int): Samples per patient.\n        Returns:\n            SampleGenerator: Generator of input data of shape (frame_size, 1)\n        \"\"\"\nstart_offset = 0\nnum_leads = 12  # Use all 12 leads\npresets = (\nEcgPresets.SR,\nEcgPresets.LAHB,\nEcgPresets.LPHB,\nEcgPresets.LBBB,\nEcgPresets.ant_STEMI,\nEcgPresets.random_morphology,\nEcgPresets.high_take_off,\n)\npreset_weights = (14, 1, 1, 1, 1, 1, 1)\nfor _ in patient_generator:\n_, syn_ecg, _, _, _ = generate_nsr(\nleads=num_leads,\nsignal_frequency=self.sampling_rate,\nrate=np.random.uniform(40, 120),\npreset=random.choices(presets, preset_weights, k=1)[0].value,\nnoise_multiplier=np.random.uniform(0.2, 0.8),\nimpedance=np.random.uniform(0.60, 1.1),\np_multiplier=np.random.uniform(0.60, 1.1),\nt_multiplier=np.random.uniform(0.60, 1.1),\nduration=max(\n10,\n(self.frame_size / self.sampling_rate) * (samples_per_patient),\n),\nvoltage_factor=np.random.uniform(275, 325),\n)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead and frame\nlead_idx = np.random.randint(syn_ecg.shape[0])\nframe_start = np.random.randint(start_offset, syn_ecg.shape[1] - self.frame_size)\nframe_end = frame_start + self.frame_size\nx = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32).reshape((self.frame_size,))\nyield x\n# END FOR\n# END FOR\ndef segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n        Args:\n            patient_generator (PatientGenerator): Patient Generator\n            samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n        Returns:\n            SampleGenerator: Sample generator\n        Yields:\n            Iterator[SampleGenerator]\n        \"\"\"\nstart_offset = 0\nnum_leads = 12  # Use all 12 leads\npresets = (\nEcgPresets.SR,\nEcgPresets.LAHB,\nEcgPresets.LPHB,\nEcgPresets.LBBB,\nEcgPresets.ant_STEMI,\nEcgPresets.random_morphology,\nEcgPresets.high_take_off,\n)\npreset_weights = (14, 1, 1, 1, 1, 1, 1)\nfor _ in patient_generator:\n_, syn_ecg, syn_segs_t, _, _ = generate_nsr(\nleads=num_leads,\nsignal_frequency=self.sampling_rate,\nrate=np.random.uniform(40, 120),\npreset=random.choices(presets, preset_weights, k=1)[0].value,\nnoise_multiplier=np.random.uniform(0.2, 0.8),\nimpedance=np.random.uniform(0.60, 1.1),\np_multiplier=np.random.uniform(0.60, 1.1),\nt_multiplier=np.random.uniform(0.60, 1.1),\nduration=max(\n10,\n(self.frame_size / self.sampling_rate) * (samples_per_patient / num_leads / 2),\n),\nvoltage_factor=np.random.uniform(275, 325),\n)\nsyn_segs = np.zeros_like(syn_segs_t)\nfor i in range(syn_segs_t.shape[0]):\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.tp_overlap))[0]] = HeartSegment.pwave\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.p_wave))[0]] = HeartSegment.pwave\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.qrs_complex))[0]] = HeartSegment.qrs\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.t_wave))[0]] = HeartSegment.twave\n# END FOR\n# # BELOW NEW\n# start_offset = int(0.55 * self.frame_size)\n# stop_offset = int(syn_ecg.shape[1] - 0.55 * self.frame_size)\n# rfids = np.vstack(\n#     np.where((syn_fids_t == SyntheticFiducials.r_peak) | (syn_fids_t == SyntheticFiducials.rpr_peak))\n# ).T\n# rfids = rfids[(rfids[:, 1] &gt; start_offset) &amp; (rfids[:, 1] &lt; stop_offset)]\n# if rfids.shape[0] &lt;= 2:\n#     continue\n# np.random.shuffle(rfids)\n# for i in range(min(samples_per_patient, rfids.shape[0])):\n#     lead_idx = rfids[i, 0]\n#     frame_start = rfids[i, 1] - int(random.uniform(0.45, 0.55) * self.frame_size)\n#     frame_end = frame_start + self.frame_size\n#     if frame_end - frame_start &lt; self.frame_size:\n#         print(\"x\")\n#         continue\n#     x = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32).reshape((self.frame_size,))\n#     y = syn_segs[lead_idx, frame_start:frame_end].astype(np.int32)\n#     yield x, y\n# # END FOR\n# # ABOVE NEW\nfor i in range(samples_per_patient):\n# Randomly pick an ECG lead and frame\nlead_idx = np.random.randint(syn_ecg.shape[0])\nframe_start = np.random.randint(start_offset, syn_ecg.shape[1] - self.frame_size)\nframe_end = frame_start + self.frame_size\nx = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32)\ny = syn_segs[lead_idx, frame_start:frame_end].astype(np.int32)\nyield x, y\n# END FOR\n# END FOR\ndef uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n        Args:\n            patient_ids (pt.ArrayLike): Array of patient ids\n            repeat (bool, optional): Whether to repeat generator. Defaults to True.\n            shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n        Returns:\n            PatientGenerator: Patient generator\n        Yields:\n            Iterator[PatientGenerator]\n        \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\nyield patient_id, None\n# END FOR\nif not repeat:\nbreak\n# END WHILE\ndef download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download dataset\n        Args:\n            num_workers (int | None, optional): # parallel workers. Defaults to None.\n            force (bool, optional): Force redownload. Defaults to False.\n        \"\"\"\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.cachable","title":"<code>cachable: bool</code>  <code>property</code>","text":"<p>If dataset supports file caching.</p>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.mean","title":"<code>mean: float</code>  <code>property</code>","text":"<p>Dataset mean</p>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.patient_ids","title":"<code>patient_ids: npt.NDArray</code>  <code>property</code>","text":"<p>Get dataset patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.sampling_rate","title":"<code>sampling_rate: int</code>  <code>property</code>","text":"<p>Sampling rate in Hz</p>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.std","title":"<code>std: float</code>  <code>property</code>","text":"<p>Dataset st dev</p>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.download","title":"<code>download(num_workers=None, force=False)</code>","text":"<p>Download dataset</p> <p>Parameters:</p> <ul> <li> num_workers             (<code>int | None</code>)         \u2013 </li> <li> force             (<code>bool</code>)         \u2013 <p>Force redownload. Defaults to False.</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def download(self, num_workers: int | None = None, force: bool = False):\n\"\"\"Download dataset\n    Args:\n        num_workers (int | None, optional): # parallel workers. Defaults to None.\n        force (bool, optional): Force redownload. Defaults to False.\n    \"\"\"\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.download--parallel-workers-defaults-to-none","title":"parallel workers. Defaults to None.","text":""},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.get_test_patient_ids","title":"<code>get_test_patient_ids()</code>","text":"<p>Get dataset patient IDs reserved for testing only</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def get_test_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset patient IDs reserved for testing only\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nnumel = int(0.80 * self._num_pts)\nreturn self.patient_ids[numel:]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.get_train_patient_ids","title":"<code>get_train_patient_ids()</code>","text":"<p>Get dataset training patient IDs</p> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: patient IDs</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def get_train_patient_ids(self) -&gt; npt.NDArray:\n\"\"\"Get dataset training patient IDs\n    Returns:\n        npt.NDArray: patient IDs\n    \"\"\"\nnumel = int(0.80 * self._num_pts)\nreturn self.patient_ids[:numel]\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.segmentation_generator","title":"<code>segmentation_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames and segment labels.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient Generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>SampleGenerator</code>         \u2013 <p>Iterator[SampleGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def segmentation_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Generate frames and segment labels.\n    Args:\n        patient_generator (PatientGenerator): Patient Generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample generator\n    Yields:\n        Iterator[SampleGenerator]\n    \"\"\"\nstart_offset = 0\nnum_leads = 12  # Use all 12 leads\npresets = (\nEcgPresets.SR,\nEcgPresets.LAHB,\nEcgPresets.LPHB,\nEcgPresets.LBBB,\nEcgPresets.ant_STEMI,\nEcgPresets.random_morphology,\nEcgPresets.high_take_off,\n)\npreset_weights = (14, 1, 1, 1, 1, 1, 1)\nfor _ in patient_generator:\n_, syn_ecg, syn_segs_t, _, _ = generate_nsr(\nleads=num_leads,\nsignal_frequency=self.sampling_rate,\nrate=np.random.uniform(40, 120),\npreset=random.choices(presets, preset_weights, k=1)[0].value,\nnoise_multiplier=np.random.uniform(0.2, 0.8),\nimpedance=np.random.uniform(0.60, 1.1),\np_multiplier=np.random.uniform(0.60, 1.1),\nt_multiplier=np.random.uniform(0.60, 1.1),\nduration=max(\n10,\n(self.frame_size / self.sampling_rate) * (samples_per_patient / num_leads / 2),\n),\nvoltage_factor=np.random.uniform(275, 325),\n)\nsyn_segs = np.zeros_like(syn_segs_t)\nfor i in range(syn_segs_t.shape[0]):\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.tp_overlap))[0]] = HeartSegment.pwave\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.p_wave))[0]] = HeartSegment.pwave\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.qrs_complex))[0]] = HeartSegment.qrs\nsyn_segs[i, np.where((syn_segs_t[i] == SyntheticSegments.t_wave))[0]] = HeartSegment.twave\n# END FOR\n# # BELOW NEW\n# start_offset = int(0.55 * self.frame_size)\n# stop_offset = int(syn_ecg.shape[1] - 0.55 * self.frame_size)\n# rfids = np.vstack(\n#     np.where((syn_fids_t == SyntheticFiducials.r_peak) | (syn_fids_t == SyntheticFiducials.rpr_peak))\n# ).T\n# rfids = rfids[(rfids[:, 1] &gt; start_offset) &amp; (rfids[:, 1] &lt; stop_offset)]\n# if rfids.shape[0] &lt;= 2:\n#     continue\n# np.random.shuffle(rfids)\n# for i in range(min(samples_per_patient, rfids.shape[0])):\n#     lead_idx = rfids[i, 0]\n#     frame_start = rfids[i, 1] - int(random.uniform(0.45, 0.55) * self.frame_size)\n#     frame_end = frame_start + self.frame_size\n#     if frame_end - frame_start &lt; self.frame_size:\n#         print(\"x\")\n#         continue\n#     x = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32).reshape((self.frame_size,))\n#     y = syn_segs[lead_idx, frame_start:frame_end].astype(np.int32)\n#     yield x, y\n# # END FOR\n# # ABOVE NEW\nfor i in range(samples_per_patient):\n# Randomly pick an ECG lead and frame\nlead_idx = np.random.randint(syn_ecg.shape[0])\nframe_start = np.random.randint(start_offset, syn_ecg.shape[1] - self.frame_size)\nframe_end = frame_start + self.frame_size\nx = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32)\ny = syn_segs[lead_idx, frame_start:frame_end].astype(np.int32)\nyield x, y\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.segmentation_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.signal_generator","title":"<code>signal_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Generate frames using patient generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Generator that yields a tuple of patient id and patient data.     Patient data may contain only signals, since labels are not used.</p> </li> <li> samples_per_patient             (<code>int</code>)         \u2013 <p>Samples per patient.</p> </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Generator of input data of shape (frame_size, 1)</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def signal_generator(self, patient_generator: PatientGenerator, samples_per_patient: int = 1) -&gt; SampleGenerator:\n\"\"\"\n    Generate frames using patient generator.\n    Args:\n        patient_generator (PatientGenerator): Generator that yields a tuple of patient id and patient data.\n                Patient data may contain only signals, since labels are not used.\n        samples_per_patient (int): Samples per patient.\n    Returns:\n        SampleGenerator: Generator of input data of shape (frame_size, 1)\n    \"\"\"\nstart_offset = 0\nnum_leads = 12  # Use all 12 leads\npresets = (\nEcgPresets.SR,\nEcgPresets.LAHB,\nEcgPresets.LPHB,\nEcgPresets.LBBB,\nEcgPresets.ant_STEMI,\nEcgPresets.random_morphology,\nEcgPresets.high_take_off,\n)\npreset_weights = (14, 1, 1, 1, 1, 1, 1)\nfor _ in patient_generator:\n_, syn_ecg, _, _, _ = generate_nsr(\nleads=num_leads,\nsignal_frequency=self.sampling_rate,\nrate=np.random.uniform(40, 120),\npreset=random.choices(presets, preset_weights, k=1)[0].value,\nnoise_multiplier=np.random.uniform(0.2, 0.8),\nimpedance=np.random.uniform(0.60, 1.1),\np_multiplier=np.random.uniform(0.60, 1.1),\nt_multiplier=np.random.uniform(0.60, 1.1),\nduration=max(\n10,\n(self.frame_size / self.sampling_rate) * (samples_per_patient),\n),\nvoltage_factor=np.random.uniform(275, 325),\n)\nfor _ in range(samples_per_patient):\n# Randomly pick an ECG lead and frame\nlead_idx = np.random.randint(syn_ecg.shape[0])\nframe_start = np.random.randint(start_offset, syn_ecg.shape[1] - self.frame_size)\nframe_end = frame_start + self.frame_size\nx = syn_ecg[lead_idx, frame_start:frame_end].astype(np.float32).reshape((self.frame_size,))\nyield x\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.task_data_generator","title":"<code>task_data_generator(patient_generator, samples_per_patient=1)</code>","text":"<p>Task-level data generator.</p> <p>Parameters:</p> <ul> <li> patient_generator             (<code>PatientGenerator</code>)         \u2013 <p>Patient data generator</p> </li> <li> samples_per_patient             (<code>int | list[int]</code>)         \u2013 </li> </ul> <p>Returns:</p> <ul> <li> SampleGenerator(            <code>SampleGenerator</code> )        \u2013 <p>Sample data generator</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def task_data_generator(\nself,\npatient_generator: PatientGenerator,\nsamples_per_patient: int | list[int] = 1,\n) -&gt; SampleGenerator:\n\"\"\"Task-level data generator.\n    Args:\n        patient_generator (PatientGenerator): Patient data generator\n        samples_per_patient (int | list[int], optional): # samples per patient. Defaults to 1.\n    Returns:\n        SampleGenerator: Sample data generator\n    \"\"\"\nif self.task == HeartTask.segmentation:\nreturn self.segmentation_generator(\npatient_generator=patient_generator,\nsamples_per_patient=samples_per_patient,\n)\nraise NotImplementedError()\n</code></pre>"},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.task_data_generator--samples-per-patient-defaults-to-1","title":"samples per patient. Defaults to 1.","text":""},{"location":"api/datasets/#heartkit.datasets.synthetic.synthetic_dataset.SyntheticDataset.uniform_patient_generator","title":"<code>uniform_patient_generator(patient_ids, repeat=True, shuffle=True)</code>","text":"<p>Yield data for each patient in the array.</p> <p>Parameters:</p> <ul> <li> patient_ids             (<code>pt.ArrayLike</code>)         \u2013 <p>Array of patient ids</p> </li> <li> repeat             (<code>bool</code>)         \u2013 <p>Whether to repeat generator. Defaults to True.</p> </li> <li> shuffle             (<code>bool</code>)         \u2013 <p>Whether to shuffle patient ids.. Defaults to True.</p> </li> </ul> <p>Returns:</p> <ul> <li> PatientGenerator(            <code>PatientGenerator</code> )        \u2013 <p>Patient generator</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>PatientGenerator</code>         \u2013 <p>Iterator[PatientGenerator]</p> </li> </ul> Source code in <code>heartkit/datasets/synthetic/synthetic_dataset.py</code> <pre><code>def uniform_patient_generator(\nself,\npatient_ids: npt.NDArray,\nrepeat: bool = True,\nshuffle: bool = True,\n) -&gt; PatientGenerator:\n\"\"\"Yield data for each patient in the array.\n    Args:\n        patient_ids (pt.ArrayLike): Array of patient ids\n        repeat (bool, optional): Whether to repeat generator. Defaults to True.\n        shuffle (bool, optional): Whether to shuffle patient ids.. Defaults to True.\n    Returns:\n        PatientGenerator: Patient generator\n    Yields:\n        Iterator[PatientGenerator]\n    \"\"\"\npatient_ids = np.copy(patient_ids)\nwhile True:\nif shuffle:\nnp.random.shuffle(patient_ids)\nfor patient_id in patient_ids:\nyield patient_id, None\n# END FOR\nif not repeat:\nbreak\n</code></pre>"},{"location":"api/models/","title":"Models","text":"<p>See Architecture for information about overall network architecture and models used.</p>"},{"location":"api/models/#heartkit.segmentation","title":"<code>heartkit.segmentation</code>","text":""},{"location":"api/models/#heartkit.segmentation.evaluate_model","title":"<code>evaluate_model(params)</code>","text":"<p>Test segmentation model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams</code>)         \u2013 <p>Testing/evaluation parameters</p> </li> </ul> Source code in <code>heartkit/segmentation.py</code> <pre><code>def evaluate_model(params: HeartTestParams):\n\"\"\"Test segmentation model.\n    Args:\n        params (HeartTestParams): Testing/evaluation parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\ntest_x, test_y = load_test_datasets(params)\nwith tfmot.quantization.keras.quantize_scope():\nlogger.info(\"Loading model\")\nmodel = load_model(str(params.model_file))\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nlogger.info(\"Performing inference\")\ny_true = np.argmax(test_y, axis=2)\ny_prob = tf.nn.softmax(model.predict(test_x)).numpy()\ny_pred = np.argmax(y_prob, axis=2)\n# END WITH\n# Summarize results\nlogger.info(\"Testing Results\")\ntest_acc = np.sum(y_pred == y_true) / y_true.size\ntest_iou = compute_iou(y_true, y_pred, average=\"weighted\")\nlogger.info(f\"[TEST SET] ACC={test_acc:.2%}, IoU={test_iou:.2%}\")\ncm_path = str(params.job_dir / \"confusion_matrix_test.png\")\nclass_names = get_class_names(HeartTask.segmentation)\nconfusion_matrix_plot(\ny_true.flatten(),\ny_pred.flatten(),\nlabels=class_names,\nsave_path=cm_path,\nnormalize=\"true\",\n)\n</code></pre>"},{"location":"api/models/#heartkit.segmentation.export_model","title":"<code>export_model(params)</code>","text":"<p>Export segmentation model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartDemoParams</code>)         \u2013 <p>Deployment parameters</p> </li> </ul> Source code in <code>heartkit/segmentation.py</code> <pre><code>def export_model(params: HeartExportParams):\n\"\"\"Export segmentation model.\n    Args:\n        params (HeartDemoParams): Deployment parameters\n    \"\"\"\ntfl_model_path = str(params.job_dir / \"model.tflite\")\ntflm_model_path = str(params.job_dir / \"model_buffer.h\")\n# Load model and set fixed batch size of 1\nlogger.info(\"Loading trained model\")\nwith tfmot.quantization.keras.quantize_scope():\nmodel = load_model(str(params.model_file))\nin_shape, _ = get_task_shape(HeartTask.segmentation, params.frame_size)\ninputs = tf.keras.layers.Input(in_shape, dtype=tf.float32, batch_size=1)\noutputs = model(inputs)\nif not params.use_logits and not isinstance(model.layers[-1], tf.keras.layers.Softmax):\noutputs = tf.keras.layers.Softmax()(outputs)\nmodel = tf.keras.Model(inputs, outputs, name=model.name)\noutputs = model(inputs)\n# END IF\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\ntest_x, test_y = load_test_datasets(params)\nlogger.info(\"Converting model to TFLite\")\ntflite_model = convert_tflite(\nmodel=model,\nquantize=params.quantization,\ntest_x=test_x,\ninput_type=tf.int8 if params.quantization else None,\noutput_type=tf.int8 if params.quantization else None,\n)\n# Save TFLite model\nlogger.info(f\"Saving TFLite model to {tfl_model_path}\")\nwith open(tfl_model_path, \"wb\") as fp:\nfp.write(tflite_model)\n# Save TFLM model\nlogger.info(f\"Saving TFL micro model to {tflm_model_path}\")\nxxd_c_dump(\nsrc_path=tfl_model_path,\ndst_path=tflm_model_path,\nvar_name=params.tflm_var_name,\nchunk_len=20,\nis_header=True,\n)\n# Verify TFLite results match TF results on example data\nlogger.info(\"Validating model results\")\ny_true = np.argmax(test_y, axis=2)\ny_pred_tf = np.argmax(model.predict(test_x), axis=2)\ny_pred_tfl = np.argmax(predict_tflite(model_content=tflite_model, test_x=test_x), axis=2)\ntf_acc = np.sum(y_true == y_pred_tf) / y_true.size\ntf_iou = compute_iou(y_true, y_pred_tf, average=\"weighted\")\nlogger.info(f\"[TF SET] ACC={tf_acc:.2%}, IoU={tf_iou:.2%}\")\ntfl_acc = np.sum(y_true == y_pred_tfl) / y_true.size\ntfl_iou = compute_iou(y_true, y_pred_tfl, average=\"weighted\")\nlogger.info(f\"[TFL SET] ACC={tfl_acc:.2%}, IoU={tfl_iou:.2%}\")\n# Check accuracy hit\ntfl_acc_drop = max(0, tf_acc - tfl_acc)\nif params.val_acc_threshold is not None and (1 - tfl_acc_drop) &lt; params.val_acc_threshold:\nlogger.warning(f\"TFLite accuracy dropped by {tfl_acc_drop:0.2%}\")\nelif params.val_acc_threshold:\nlogger.info(f\"Validation passed ({tfl_acc_drop:0.2%})\")\nif params.tflm_file and tflm_model_path != params.tflm_file:\nlogger.info(f\"Copying TFLM header to {params.tflm_file}\")\nshutil.copyfile(tflm_model_path, params.tflm_file)\n</code></pre>"},{"location":"api/models/#heartkit.segmentation.load_test_datasets","title":"<code>load_test_datasets(params)</code>","text":"<p>Load segmentation test dataset.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams | HeartExportParams</code>)         \u2013 <p>Test params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: Test data and labels</p> </li> </ul> Source code in <code>heartkit/segmentation.py</code> <pre><code>def load_test_datasets(\nparams: HeartTestParams | HeartExportParams,\n) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Load segmentation test dataset.\n    Args:\n        params (HeartTestParams|HeartExportParams): Test params\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: Test data and labels\n    \"\"\"\ndataset_names: list[str] = getattr(params, \"datasets\", [\"ludb\"])\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy().squeeze()\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\nwith console.status(\"[bold green] Loading test dataset...\"):\ndatasets: list[HeartKitDataset] = []\nif \"synthetic\" in dataset_names:\ndatasets.append(\nSyntheticDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\nnum_pts=getattr(params, \"num_pts\", 200),\n)\n)\nif \"ludb\" in dataset_names:\ndatasets.append(\nLudbDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\n)\nif \"qtdb\" in dataset_names:\ndatasets.append(\nQtdbDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\n)\ntest_datasets = [\nds.load_test_dataset(\ntest_pt_samples=params.samples_per_patient,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\nfor ds in datasets\n]\nds_weights = np.array([len(ds.get_test_patient_ids()) for ds in datasets])\nds_weights = ds_weights / ds_weights.sum()\ntest_ds = tf.data.Dataset.sample_from_datasets(test_datasets, weights=ds_weights)\ntest_x, test_y = next(test_ds.batch(params.test_size).as_numpy_iterator())\n# END WITH\nreturn test_x, test_y\n</code></pre>"},{"location":"api/models/#heartkit.segmentation.load_train_datasets","title":"<code>load_train_datasets(params)</code>","text":"<p>Load segmentation train datasets.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Train params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[tf.data.Dataset, tf.data.Dataset]</code>         \u2013 <p>tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets</p> </li> </ul> Source code in <code>heartkit/segmentation.py</code> <pre><code>def load_train_datasets(\nparams: HeartTrainParams,\n) -&gt; tuple[tf.data.Dataset, tf.data.Dataset]:\n\"\"\"Load segmentation train datasets.\n    Args:\n        params (HeartTrainParams): Train params\n    Returns:\n        tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets\n    \"\"\"\ndataset_names: list[str] = getattr(params, \"datasets\", [\"ludb\"])\nnum_pts = getattr(params, \"num_pts\", 1000)\ndatasets: list[HeartKitDataset] = []\nif \"synthetic\" in dataset_names:\ndatasets.append(\nSyntheticDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\nnum_pts=num_pts,\n)\n)\nif \"ludb\" in dataset_names:\ndatasets.append(\nLudbDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\n)\nif \"qtdb\" in dataset_names:\ndatasets.append(\nQtdbDataset(\nstr(params.ds_path),\ntask=HeartTask.segmentation,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\n)\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy().squeeze()\nif params.augmentations:\nxx = augment_pipeline(xx, augmentations=params.augmentations, sample_rate=params.sampling_rate)\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\ntrain_datasets = []\nval_datasets = []\nfor ds in datasets:\n# Create TF datasets\ntrain_ds, val_ds = ds.load_train_datasets(\ntrain_patients=params.train_patients,\nval_patients=params.val_patients,\ntrain_pt_samples=params.samples_per_patient,\nval_pt_samples=params.val_samples_per_patient,\nval_file=params.val_file,\nval_size=params.val_size,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\ntrain_datasets.append(train_ds)\nval_datasets.append(val_ds)\n# END FOR\nds_weights = np.array([len(ds.get_train_patient_ids()) for ds in datasets])\nds_weights = ds_weights / ds_weights.sum()\ntrain_ds = tf.data.Dataset.sample_from_datasets(train_datasets, weights=ds_weights)\nval_ds = tf.data.Dataset.sample_from_datasets(val_datasets, weights=ds_weights)\n# Shuffle and batch datasets for training\ntrain_ds = (\ntrain_ds.shuffle(\nbuffer_size=params.buffer_size,\nreshuffle_each_iteration=True,\n)\n.batch(\nbatch_size=params.batch_size,\ndrop_remainder=False,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\n.prefetch(buffer_size=tf.data.AUTOTUNE)\n)\nval_ds = val_ds.batch(\nbatch_size=params.batch_size,\ndrop_remainder=True,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\nreturn train_ds, val_ds\n</code></pre>"},{"location":"api/models/#heartkit.segmentation.prepare","title":"<code>prepare(x, sample_rate)</code>","text":"<p>Prepare dataset.</p> Source code in <code>heartkit/segmentation.py</code> <pre><code>def prepare(x: npt.NDArray, sample_rate: float) -&gt; npt.NDArray:\n\"\"\"Prepare dataset.\"\"\"\nx = signal.filter_signal(\nx,\nlowcut=0.5,\nhighcut=30,\norder=3,\nsample_rate=sample_rate,\naxis=0,\nforward_backward=True,\n)\nx = signal.normalize_signal(x, eps=0.1, axis=None)\nreturn x\n</code></pre>"},{"location":"api/models/#heartkit.segmentation.train_model","title":"<code>train_model(params)</code>","text":"<p>Train segmentation model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Training parameters</p> </li> </ul> Source code in <code>heartkit/segmentation.py</code> <pre><code>def train_model(params: HeartTrainParams):\n\"\"\"Train segmentation model.\n    Args:\n        params (HeartTrainParams): Training parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\nos.makedirs(str(params.job_dir), exist_ok=True)\nlogger.info(f\"Creating working directory in {params.job_dir}\")\nwith open(str(params.job_dir / \"train_config.json\"), \"w\", encoding=\"utf-8\") as fp:\nfp.write(params.json(indent=2))\nif env_flag(\"WANDB\"):\nwandb.init(\nproject=f\"heartkit-{HeartTask.segmentation}\",\nentity=\"ambiq\",\ndir=params.job_dir,\n)\nwandb.config.update(params.dict())\ntrain_ds, val_ds = load_train_datasets(params)\nstrategy = get_strategy()\nwith strategy.scope():\nlogger.info(\"Building model\")\nin_shape, _ = get_task_shape(HeartTask.segmentation, params.frame_size)\ninputs = tf.keras.Input(shape=in_shape, batch_size=None, dtype=tf.float32)\nif params.model_file:\nmodel = load_model(str(params.model_file))\nelse:\nmodel = create_task_model(\ninputs,\nHeartTask.segmentation,\nname=params.model,\nparams=params.model_params,\n)\n# If fine-tune, freeze model encoder weights\nif bool(getattr(params, \"finetune\", False)):\nfor layer in model.layers:\nif layer.name.startswith(\"ENC\"):\nlogger.info(f\"Freezing {layer.name}\")\nlayer.trainable = False\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\n# Grab optional LR parameters\nlr_rate: float = getattr(params, \"lr_rate\", 1e-4)\nlr_cycles: int = getattr(params, \"lr_cycles\", 3)\nsteps_per_epoch = params.steps_per_epoch or 1000\nif lr_cycles &gt; 1:\nscheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\ninitial_learning_rate=lr_rate,\nfirst_decay_steps=int(0.1 * steps_per_epoch * params.epochs),\nt_mul=1.65 / (0.1 * lr_cycles * (lr_cycles - 1)),\nm_mul=0.4,\n)\nelse:\nscheduler = tf.keras.optimizers.schedules.CosineDecay(\ninitial_learning_rate=lr_rate / 5, decay_steps=steps_per_epoch * params.epochs\n)\noptimizer = tf.keras.optimizers.Adam(scheduler, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\nloss = tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True)\nmetrics = [\ntf.keras.metrics.CategoricalAccuracy(name=\"acc\"),\ntf.keras.metrics.OneHotIoU(\nnum_classes=get_num_classes(HeartTask.segmentation),\ntarget_class_ids=tuple(s.value for s in HeartSegment),\nname=\"iou\",\n),\n]\nif params.weights_file:\nlogger.info(f\"Loading weights from file {params.weights_file}\")\nmodel.load_weights(str(params.weights_file))\nparams.weights_file = str(params.job_dir / \"model.weights\")\n# Perform QAT if requested (typically used for fine-tuning)\nif params.quantization:\nlogger.info(\"Performing QAT...\")\nmodel = tfmot.quantization.keras.quantize_model(model)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel(inputs)\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nmodel_callbacks = [\ntf.keras.callbacks.EarlyStopping(\nmonitor=f\"val_{params.val_metric}\",\npatience=max(int(0.25 * params.epochs), 1),\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nrestore_best_weights=True,\n),\ntf.keras.callbacks.ModelCheckpoint(\nfilepath=params.weights_file,\nmonitor=f\"val_{params.val_metric}\",\nsave_best_only=True,\nsave_weights_only=True,\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nverbose=1,\n),\ntf.keras.callbacks.CSVLogger(str(params.job_dir / \"history.csv\")),\ntf.keras.callbacks.TensorBoard(log_dir=str(params.job_dir), write_steps_per_second=True),\n]\nif env_flag(\"WANDB\"):\nmodel_callbacks.append(WandbCallback())\ntry:\nmodel.fit(\ntrain_ds,\nsteps_per_epoch=steps_per_epoch,\nverbose=2,\nepochs=params.epochs,\nvalidation_data=val_ds,\ncallbacks=model_callbacks,\n)\nexcept KeyboardInterrupt:\nlogger.warning(\"Stopping training due to keyboard interrupt\")\n# Restore best weights from checkpoint\nmodel.load_weights(params.weights_file)\n# Save full model\ntf_model_path = str(params.job_dir / \"model.tf\")\nlogger.info(f\"Model saved to {tf_model_path}\")\nmodel.save(tf_model_path)\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia","title":"<code>heartkit.arrhythmia</code>","text":""},{"location":"api/models/#heartkit.arrhythmia.evaluate_model","title":"<code>evaluate_model(params)</code>","text":"<p>Test arrhythmia model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams</code>)         \u2013 <p>Testing/evaluation parameters</p> </li> </ul> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def evaluate_model(params: HeartTestParams):\n\"\"\"Test arrhythmia model.\n    Args:\n        params (HeartTestParams): Testing/evaluation parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\ntest_x, test_y = load_test_dataset(params)\nstrategy = get_strategy()\nwith strategy.scope():\nlogger.info(\"Loading model\")\nmodel = load_model(str(params.model_file))\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nlogger.info(\"Performing inference\")\ny_true = np.argmax(test_y, axis=1)\ny_prob = tf.nn.softmax(model.predict(test_x)).numpy()\ny_pred = np.argmax(y_prob, axis=1)\n# Summarize results\nlogger.info(\"Testing Results\")\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[TEST SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\nclass_names = get_class_names(HeartTask.arrhythmia)\nif len(class_names) == 2:\nroc_path = str(params.job_dir / \"roc_auc_test.png\")\nroc_auc_plot(y_true, y_prob[:, 1], labels=class_names, save_path=roc_path)\n# END IF\n# If threshold given, only count predictions above threshold\nif params.threshold:\nnumel = len(y_true)\ny_prob, y_pred, y_true = threshold_predictions(y_prob, y_pred, y_true, params.threshold)\ndrop_perc = 1 - len(y_true) / numel\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[TEST SET] THRESH={params.threshold:0.2%}, DROP={drop_perc:.2%}\")\nlogger.info(f\"[TEST SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\n# END IF\ncm_path = str(params.job_dir / \"confusion_matrix_test.png\")\nconfusion_matrix_plot(y_true, y_pred, labels=class_names, save_path=cm_path, normalize=\"true\")\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia.export_model","title":"<code>export_model(params)</code>","text":"<p>Export arrhythmia model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartExportParams</code>)         \u2013 <p>Deployment parameters</p> </li> </ul> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def export_model(params: HeartExportParams):\n\"\"\"Export arrhythmia model.\n    Args:\n        params (HeartExportParams): Deployment parameters\n    \"\"\"\ntfl_model_path = str(params.job_dir / \"model.tflite\")\ntflm_model_path = str(params.job_dir / \"model_buffer.h\")\n# Load model and set fixed batch size of 1\nlogger.info(\"Loading trained model\")\nmodel = load_model(str(params.model_file))\nin_shape, _ = get_task_shape(HeartTask.arrhythmia, params.frame_size)\ninputs = tf.keras.layers.Input(in_shape, dtype=tf.float32, batch_size=1)\noutputs = model(inputs)\nif not params.use_logits and not isinstance(model.layers[-1], tf.keras.layers.Softmax):\noutputs = tf.keras.layers.Softmax()(outputs)\nmodel = tf.keras.Model(inputs, outputs, name=model.name)\noutputs = model(inputs)\n# END IF\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\ntest_x, test_y = load_test_dataset(params)\nlogger.info(\"Converting model to TFLite\")\ntflite_model = convert_tflite(\nmodel=model,\nquantize=params.quantization,\ntest_x=test_x,\ninput_type=tf.int8 if params.quantization else None,\noutput_type=tf.int8 if params.quantization else None,\n)\n# Save TFLite model\nlogger.info(f\"Saving TFLite model to {tfl_model_path}\")\nwith open(tfl_model_path, \"wb\") as fp:\nfp.write(tflite_model)\n# Save TFLM model\nlogger.info(f\"Saving TFL micro model to {tflm_model_path}\")\nxxd_c_dump(\nsrc_path=tfl_model_path,\ndst_path=tflm_model_path,\nvar_name=params.tflm_var_name,\nchunk_len=20,\nis_header=True,\n)\n# Verify TFLite results match TF results on example data\nlogger.info(\"Validating model results\")\ny_true = np.argmax(test_y, axis=1)\ny_pred_tf = np.argmax(model.predict(test_x), axis=1)\ny_pred_tfl = np.argmax(predict_tflite(model_content=tflite_model, test_x=test_x), axis=1)\ntf_acc = np.sum(y_true == y_pred_tf) / y_true.size\ntf_f1 = f1_score(y_true, y_pred_tf, average=\"weighted\")\nlogger.info(f\"[TF SET] ACC={tf_acc:.2%}, F1={tf_f1:.2%}\")\ntfl_acc = np.sum(y_true == y_pred_tfl) / y_true.size\ntfl_f1 = f1_score(y_true, y_pred_tfl, average=\"weighted\")\nlogger.info(f\"[TFL SET] ACC={tfl_acc:.2%}, F1={tfl_f1:.2%}\")\n# Check accuracy hit\ntfl_acc_drop = max(0, tf_acc - tfl_acc)\nif params.val_acc_threshold is not None and (1 - tfl_acc_drop) &lt; params.val_acc_threshold:\nlogger.warning(f\"TFLite accuracy dropped by {tfl_acc_drop:0.2%}\")\nelif params.val_acc_threshold:\nlogger.info(f\"Validation passed ({tfl_acc_drop:0.2%})\")\nif params.tflm_file and tflm_model_path != params.tflm_file:\nlogger.info(f\"Copying TFLM header to {params.tflm_file}\")\nshutil.copyfile(tflm_model_path, params.tflm_file)\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia.load_test_dataset","title":"<code>load_test_dataset(params)</code>","text":"<p>Load arrhythmia test dataset.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams | HeartExportParams</code>)         \u2013 <p>Test params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: Test data and labels</p> </li> </ul> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def load_test_dataset(\nparams: HeartTestParams | HeartExportParams,\n) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Load arrhythmia test dataset.\n    Args:\n        params (HeartTestParams|HeartExportParams): Test params\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: Test data and labels\n    \"\"\"\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy().squeeze()\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\nwith console.status(\"[bold green] Loading test dataset...\"):\nds = IcentiaDataset(\nds_path=str(params.ds_path),\ntask=HeartTask.arrhythmia,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\ntest_ds = ds.load_test_dataset(\ntest_patients=params.test_patients,\ntest_pt_samples=params.samples_per_patient,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\ntest_x, test_y = next(test_ds.batch(params.test_size).as_numpy_iterator())\n# END WITH\nreturn test_x, test_y\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia.load_train_datasets","title":"<code>load_train_datasets(params)</code>","text":"<p>Load arrhythmia train datasets.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Train params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[tf.data.Dataset, tf.data.Dataset]</code>         \u2013 <p>tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets</p> </li> </ul> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def load_train_datasets(\nparams: HeartTrainParams,\n) -&gt; tuple[tf.data.Dataset, tf.data.Dataset]:\n\"\"\"Load arrhythmia train datasets.\n    Args:\n        params (HeartTrainParams): Train params\n    Returns:\n        tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets\n    \"\"\"\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy().squeeze()\nif params.augmentations:\nxx = augment_pipeline(xx, augmentations=params.augmentations, sample_rate=params.sampling_rate)\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\n# Create TF datasets\nds = IcentiaDataset(\nds_path=str(params.ds_path),\ntask=HeartTask.arrhythmia,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\ntrain_ds, val_ds = ds.load_train_datasets(\ntrain_patients=params.train_patients,\nval_patients=params.val_patients,\ntrain_pt_samples=params.samples_per_patient,\nval_pt_samples=params.val_samples_per_patient,\nval_file=params.val_file,\nval_size=params.val_size,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\n# Shuffle and batch datasets for training\ntrain_ds = (\ntrain_ds.shuffle(\nbuffer_size=params.buffer_size,\nreshuffle_each_iteration=True,\n)\n.batch(\nbatch_size=params.batch_size,\ndrop_remainder=True,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\n.prefetch(buffer_size=tf.data.AUTOTUNE)\n)\nval_ds = val_ds.batch(\nbatch_size=params.batch_size,\ndrop_remainder=True,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\nreturn train_ds, val_ds\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia.prepare","title":"<code>prepare(x, sample_rate)</code>","text":"<p>Prepare dataset.</p> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def prepare(x: npt.NDArray, sample_rate: float) -&gt; npt.NDArray:\n\"\"\"Prepare dataset.\"\"\"\nx = signal.filter_signal(\nx,\nlowcut=0.5,\nhighcut=30,\norder=3,\nsample_rate=sample_rate,\naxis=0,\nforward_backward=True,\n)\nx = signal.normalize_signal(x, eps=0.1, axis=None)\nreturn x\n</code></pre>"},{"location":"api/models/#heartkit.arrhythmia.train_model","title":"<code>train_model(params)</code>","text":"<p>Train rhythm-level arrhythmia model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Training parameters</p> </li> </ul> Source code in <code>heartkit/arrhythmia.py</code> <pre><code>def train_model(params: HeartTrainParams):\n\"\"\"Train rhythm-level arrhythmia model.\n    Args:\n        params (HeartTrainParams): Training parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\nos.makedirs(str(params.job_dir), exist_ok=True)\nlogger.info(f\"Creating working directory in {params.job_dir}\")\nwith open(str(params.job_dir / \"train_config.json\"), \"w\", encoding=\"utf-8\") as fp:\nfp.write(params.json(indent=2))\nif env_flag(\"WANDB\"):\nwandb.init(\nproject=f\"heartkit-{HeartTask.arrhythmia}\",\nentity=\"ambiq\",\ndir=params.job_dir,\n)\nwandb.config.update(params.dict())\ntrain_ds, val_ds = load_train_datasets(params)\nstrategy = get_strategy()\nwith strategy.scope():\nlogger.info(\"Building model\")\nin_shape, _ = get_task_shape(HeartTask.arrhythmia, params.frame_size)\ninputs = tf.keras.Input(in_shape, batch_size=None, dtype=tf.float32)\nmodel = create_task_model(inputs, HeartTask.arrhythmia, name=params.model, params=params.model_params)\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\n# Grab optional LR parameters\nsteps_per_epoch = params.steps_per_epoch or 1000\nlr_rate: float = getattr(params, \"lr_rate\", 1e-3)\nlr_cycles: int = getattr(params, \"lr_cycles\", 3)\nscheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\ninitial_learning_rate=lr_rate,\nfirst_decay_steps=int(0.1 * steps_per_epoch * params.epochs),\nt_mul=1.65 / (0.1 * lr_cycles * (lr_cycles - 1)),\nm_mul=0.4,\n)\noptimizer = tf.keras.optimizers.Adam(scheduler)\nloss = tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True)\nmetrics = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel(inputs)\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nif params.weights_file:\nlogger.info(f\"Loading weights from file {params.weights_file}\")\nmodel.load_weights(str(params.weights_file))\nparams.weights_file = str(params.job_dir / \"model.weights\")\nmodel_callbacks = [\ntf.keras.callbacks.EarlyStopping(\nmonitor=f\"val_{params.val_metric}\",\npatience=max(int(0.25 * params.epochs), 1),\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nrestore_best_weights=True,\n),\ntf.keras.callbacks.ModelCheckpoint(\nfilepath=params.weights_file,\nmonitor=f\"val_{params.val_metric}\",\nsave_best_only=True,\nsave_weights_only=True,\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nverbose=1,\n),\ntf.keras.callbacks.CSVLogger(str(params.job_dir / \"history.csv\")),\ntf.keras.callbacks.TensorBoard(log_dir=str(params.job_dir), write_steps_per_second=True),\n]\nif env_flag(\"WANDB\"):\nmodel_callbacks.append(WandbCallback())\ntry:\nmodel.fit(\ntrain_ds,\nsteps_per_epoch=steps_per_epoch,\nverbose=2,\nepochs=params.epochs,\nvalidation_data=val_ds,\ncallbacks=model_callbacks,\n)\nexcept KeyboardInterrupt:\nlogger.warning(\"Stopping training due to keyboard interrupt\")\n# Restore best weights from checkpoint\nmodel.load_weights(params.weights_file)\n# Save full model\ntf_model_path = str(params.job_dir / \"model.tf\")\nlogger.info(f\"Model saved to {tf_model_path}\")\nmodel.save(tf_model_path)\n# Get full validation results\nlogger.info(\"Performing full validation\")\ntest_labels = [label.numpy() for _, label in val_ds]\ny_true = np.argmax(np.concatenate(test_labels), axis=1)\ny_pred = np.argmax(model.predict(val_ds), axis=1)\n# Summarize results\nclass_names = get_class_names(task=HeartTask.arrhythmia)\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[VAL SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\ncm_path = str(params.job_dir / \"confusion_matrix.png\")\nconfusion_matrix_plot(y_true, y_pred, labels=class_names, save_path=cm_path, normalize=\"true\")\nif env_flag(\"WANDB\"):\nconf_mat = wandb.plot.confusion_matrix(preds=y_pred, y_true=y_true, class_names=class_names)\nwandb.log({\"conf_mat\": conf_mat})\n</code></pre>"},{"location":"api/models/#heartkit.beat","title":"<code>heartkit.beat</code>","text":""},{"location":"api/models/#heartkit.beat.evaluate_model","title":"<code>evaluate_model(params)</code>","text":"<p>Test beat-level model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams</code>)         \u2013 <p>Testing/evaluation parameters</p> </li> </ul> Source code in <code>heartkit/beat.py</code> <pre><code>def evaluate_model(params: HeartTestParams):\n\"\"\"Test beat-level model.\n    Args:\n        params (HeartTestParams): Testing/evaluation parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\ntest_x, test_y = load_test_dataset(params)\nstrategy = get_strategy()\nwith strategy.scope():\nlogger.info(\"Loading model\")\nmodel = load_model(str(params.model_file))\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nlogger.info(\"Performing inference\")\ny_true = np.argmax(test_y, axis=1)\ny_prob = tf.nn.softmax(model.predict(test_x)).numpy()\ny_pred = np.argmax(y_prob, axis=1)\n# Summarize results\nlogger.info(\"Testing Results\")\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[TEST SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\n# If threshold given, only count predictions above threshold\nif params.threshold:\nnumel = len(y_true)\ny_prob, y_pred, y_true = threshold_predictions(y_prob, y_pred, y_true, params.threshold)\ndrop_perc = 1 - len(y_true) / numel\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[TEST SET] THRESH={params.threshold:0.2%}, DROP={drop_perc:.2%}\")\nlogger.info(f\"[TEST SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\n# END IF\nclass_names = get_class_names(HeartTask.beat)\ncm_path = str(params.job_dir / \"confusion_matrix_test.png\")\nconfusion_matrix_plot(y_true, y_pred, labels=class_names, save_path=cm_path, normalize=\"true\")\nif len(class_names) == 2:\nroc_path = str(params.job_dir / \"roc_auc_test.png\")\nroc_auc_plot(y_true, y_prob[:, 1], labels=class_names, save_path=roc_path)\n</code></pre>"},{"location":"api/models/#heartkit.beat.export_model","title":"<code>export_model(params)</code>","text":"<p>Export beat-level model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartExportParams</code>)         \u2013 <p>Deployment parameters</p> </li> </ul> Source code in <code>heartkit/beat.py</code> <pre><code>def export_model(params: HeartExportParams):\n\"\"\"Export beat-level model.\n    Args:\n        params (HeartExportParams): Deployment parameters\n    \"\"\"\ntfl_model_path = str(params.job_dir / \"model.tflite\")\ntflm_model_path = str(params.job_dir / \"model_buffer.h\")\n# Load model and set fixed batch size of 1\nlogger.info(\"Loading trained model\")\nmodel = load_model(str(params.model_file))\nin_shape, _ = get_task_shape(HeartTask.beat, params.frame_size)\ninputs = tf.keras.layers.Input(in_shape, dtype=tf.float32, batch_size=1)\noutputs = model(inputs)\nif not params.use_logits and not isinstance(model.layers[-1], tf.keras.layers.Softmax):\noutputs = tf.keras.layers.Softmax()(outputs)\nmodel = tf.keras.Model(inputs, outputs, name=model.name)\noutputs = model(inputs)\n# END IF\ntest_x, test_y = load_test_dataset(params)\nlogger.info(\"Converting model to TFLite\")\ntflite_model = convert_tflite(\nmodel=model,\nquantize=params.quantization,\ntest_x=test_x,\ninput_type=tf.int8 if params.quantization else None,\noutput_type=tf.int8 if params.quantization else None,\n)\n# Save TFLite model\nlogger.info(f\"Saving TFLite model to {tfl_model_path}\")\nwith open(tfl_model_path, \"wb\") as fp:\nfp.write(tflite_model)\n# Save TFLM model\nlogger.info(f\"Saving TFL micro model to {tflm_model_path}\")\nxxd_c_dump(\nsrc_path=tfl_model_path,\ndst_path=tflm_model_path,\nvar_name=params.tflm_var_name,\nchunk_len=20,\nis_header=True,\n)\n# Verify TFLite results match TF results on example data\nlogger.info(\"Validating model results\")\ny_true = np.argmax(test_y, axis=1)\ny_pred_tf = np.argmax(model.predict(test_x), axis=1)\ny_pred_tfl = np.argmax(predict_tflite(model_content=tflite_model, test_x=test_x), axis=1)\ntf_acc = np.sum(y_true == y_pred_tf) / y_true.size\ntf_f1 = f1_score(y_true, y_pred_tf, average=\"weighted\")\nlogger.info(f\"[TF SET] ACC={tf_acc:.2%}, F1={tf_f1:.2%}\")\ntfl_acc = np.sum(y_true == y_pred_tfl) / y_true.size\ntfl_f1 = f1_score(y_true, y_pred_tfl, average=\"weighted\")\nlogger.info(f\"[TFL SET] ACC={tfl_acc:.2%}, F1={tfl_f1:.2%}\")\n# Check accuracy hit\ntfl_acc_drop = max(0, tf_acc - tfl_acc)\nif params.val_acc_threshold is not None and (1 - tfl_acc_drop) &lt; params.val_acc_threshold:\nlogger.warning(f\"TFLite accuracy dropped by {tfl_acc_drop:0.2%}\")\nelif params.val_acc_threshold:\nlogger.info(f\"Validation passed ({tfl_acc_drop:0.2%})\")\nif params.tflm_file and tflm_model_path != params.tflm_file:\nlogger.info(f\"Copying TFLM header to {params.tflm_file}\")\nshutil.copyfile(tflm_model_path, params.tflm_file)\n</code></pre>"},{"location":"api/models/#heartkit.beat.load_test_dataset","title":"<code>load_test_dataset(params)</code>","text":"<p>Load beat test dataset.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams | HeartExportParams</code>)         \u2013 <p>Test params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[npt.NDArray, npt.NDArray]: Test data and labels</p> </li> </ul> Source code in <code>heartkit/beat.py</code> <pre><code>def load_test_dataset(\nparams: HeartTestParams | HeartExportParams,\n) -&gt; tuple[npt.NDArray, npt.NDArray]:\n\"\"\"Load beat test dataset.\n    Args:\n        params (HeartTestParams|HeartExportParams): Test params\n    Returns:\n        tuple[npt.NDArray, npt.NDArray]: Test data and labels\n    \"\"\"\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy()\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\nwith console.status(\"[bold green] Loading test dataset...\"):\nds = IcentiaDataset(\nds_path=str(params.ds_path),\ntask=HeartTask.beat,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\ntest_ds = ds.load_test_dataset(\ntest_patients=params.test_patients,\ntest_pt_samples=params.samples_per_patient,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\ntest_x, test_y = next(test_ds.batch(params.test_size).as_numpy_iterator())\n# END WITH\nreturn test_x, test_y\n</code></pre>"},{"location":"api/models/#heartkit.beat.load_train_datasets","title":"<code>load_train_datasets(params)</code>","text":"<p>Load beat train datasets.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Train params</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[tf.data.Dataset, tf.data.Dataset]</code>         \u2013 <p>tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets</p> </li> </ul> Source code in <code>heartkit/beat.py</code> <pre><code>def load_train_datasets(\nparams: HeartTrainParams,\n) -&gt; tuple[tf.data.Dataset, tf.data.Dataset]:\n\"\"\"Load beat train datasets.\n    Args:\n        params (HeartTrainParams): Train params\n    Returns:\n        tuple[tf.data.Dataset, tf.data.Dataset]: Train and validation datasets\n    \"\"\"\ndef preprocess(x: npt.NDArray) -&gt; npt.NDArray:\nxx = x.copy().squeeze()\n# Augment each channel\nif params.augmentations:\nfor i in range(x.shape[-1]):\nxx[:, i] = augment_pipeline(\nxx[:, i],\naugmentations=params.augmentations,\nsample_rate=params.sampling_rate,\n)\n# END FOR\nxx = prepare(xx, sample_rate=params.sampling_rate)\nreturn xx\n# Create TF datasets\nds = IcentiaDataset(\nds_path=str(params.ds_path),\ntask=HeartTask.beat,\nframe_size=params.frame_size,\ntarget_rate=params.sampling_rate,\n)\ntrain_ds, val_ds = ds.load_train_datasets(\ntrain_patients=params.train_patients,\nval_patients=params.val_patients,\ntrain_pt_samples=params.samples_per_patient,\nval_pt_samples=params.val_samples_per_patient,\nval_file=params.val_file,\nval_size=params.val_size,\npreprocess=preprocess,\nnum_workers=params.data_parallelism,\n)\n# Shuffle and batch datasets for training\ntrain_ds = (\ntrain_ds.shuffle(\nbuffer_size=params.buffer_size,\nreshuffle_each_iteration=True,\n)\n.batch(\nbatch_size=params.batch_size,\ndrop_remainder=True,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\n.prefetch(buffer_size=tf.data.AUTOTUNE)\n)\nval_ds = val_ds.batch(\nbatch_size=params.batch_size,\ndrop_remainder=True,\nnum_parallel_calls=tf.data.AUTOTUNE,\n)\nreturn train_ds, val_ds\n</code></pre>"},{"location":"api/models/#heartkit.beat.prepare","title":"<code>prepare(x, sample_rate)</code>","text":"<p>Prepare dataset.</p> Source code in <code>heartkit/beat.py</code> <pre><code>def prepare(x: npt.NDArray, sample_rate: float) -&gt; npt.NDArray:\n\"\"\"Prepare dataset.\"\"\"\nx = signal.filter_signal(\nx,\nlowcut=0.5,\nhighcut=30,\norder=3,\nsample_rate=sample_rate,\naxis=0,\nforward_backward=True,\n)\nx = signal.normalize_signal(x, eps=0.1, axis=None)\nreturn x\n</code></pre>"},{"location":"api/models/#heartkit.beat.train_model","title":"<code>train_model(params)</code>","text":"<p>Train beat-level model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Training parameters</p> </li> </ul> Source code in <code>heartkit/beat.py</code> <pre><code>def train_model(params: HeartTrainParams):\n\"\"\"Train beat-level model.\n    Args:\n        params (HeartTrainParams): Training parameters\n    \"\"\"\nparams.seed = set_random_seed(params.seed)\nlogger.info(f\"Random seed {params.seed}\")\nos.makedirs(str(params.job_dir), exist_ok=True)\nlogger.info(f\"Creating working directory in {params.job_dir}\")\nwith open(str(params.job_dir / \"train_config.json\"), \"w\", encoding=\"utf-8\") as fp:\nfp.write(params.json(indent=2))\nif env_flag(\"WANDB\"):\nwandb.init(\nproject=f\"heartkit-{HeartTask.beat}\",\nentity=\"ambiq\",\ndir=str(params.job_dir),\n)\nwandb.config.update(params.dict())\ntrain_ds, val_ds = load_train_datasets(params)\nstrategy = get_strategy()\nwith strategy.scope():\nlogger.info(\"Building model\")\nin_shape, _ = get_task_shape(HeartTask.beat, params.frame_size)\ninputs = tf.keras.Input(in_shape, batch_size=None, dtype=tf.float32)\nmodel = create_task_model(inputs, HeartTask.beat, name=params.model, params=params.model_params)\nflops = get_flops(model, batch_size=1, fpath=str(params.job_dir / \"model_flops.log\"))\n# Grab optional LR parameters\nlr_rate: float = getattr(params, \"lr_rate\", 1e-3)\nlr_cycles: int = getattr(params, \"lr_cycles\", 3)\nsteps_per_epoch = params.steps_per_epoch or 1000\nscheduler = tf.keras.optimizers.schedules.CosineDecayRestarts(\ninitial_learning_rate=lr_rate,\nfirst_decay_steps=int(0.1 * steps_per_epoch * params.epochs),\nt_mul=1.65 / (0.1 * lr_cycles * (lr_cycles - 1)),\nm_mul=0.4,\n)\noptimizer = tf.keras.optimizers.Adam(scheduler)\nloss = tf.keras.losses.CategoricalFocalCrossentropy(from_logits=True)\nmetrics = [tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nmodel(inputs)\nmodel.summary(print_fn=logger.info)\nlogger.info(f\"Model requires {flops/1e6:0.2f} MFLOPS\")\nif params.weights_file:\nlogger.info(f\"Loading weights from file {params.weights_file}\")\nmodel.load_weights(str(params.weights_file))\nparams.weights_file = params.job_dir / \"model.weights\"\nmodel_callbacks = [\ntf.keras.callbacks.EarlyStopping(\nmonitor=f\"val_{params.val_metric}\",\npatience=max(int(0.25 * params.epochs), 1),\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nrestore_best_weights=True,\n),\ntf.keras.callbacks.ModelCheckpoint(\nfilepath=params.weights_file,\nmonitor=f\"val_{params.val_metric}\",\nsave_best_only=True,\nsave_weights_only=True,\nmode=\"max\" if params.val_metric == \"f1\" else \"auto\",\nverbose=1,\n),\ntf.keras.callbacks.CSVLogger(str(params.job_dir / \"history.csv\")),\ntf.keras.callbacks.TensorBoard(log_dir=str(params.job_dir), write_steps_per_second=True),\n]\nif env_flag(\"WANDB\"):\nmodel_callbacks.append(WandbCallback())\ntry:\nmodel.fit(\ntrain_ds,\nsteps_per_epoch=steps_per_epoch,\nverbose=2,\nepochs=params.epochs,\nvalidation_data=val_ds,\ncallbacks=model_callbacks,\n)\nexcept KeyboardInterrupt:\nlogger.warning(\"Stopping training due to keyboard interrupt\")\n# Restore best weights from checkpoint\nmodel.load_weights(params.weights_file)\n# Save full model\ntf_model_path = str(params.job_dir / \"model.tf\")\nlogger.info(f\"Model saved to {tf_model_path}\")\nmodel.save(tf_model_path)\n# Get full validation results\nlogger.info(\"Performing full validation\")\ntest_labels = [label.numpy() for _, label in val_ds]\ny_true = np.argmax(np.concatenate(test_labels), axis=1)\ny_pred = np.argmax(model.predict(val_ds), axis=1)\n# Summarize results\nclass_names = get_class_names(task=HeartTask.beat)\ntest_acc = np.sum(y_pred == y_true) / len(y_true)\ntest_f1 = f1_score(y_true, y_pred, average=\"macro\")\nlogger.info(f\"[VAL SET] ACC={test_acc:.2%}, F1={test_f1:.2%}\")\ncm_path = str(params.job_dir / \"confusion_matrix.png\")\nconfusion_matrix_plot(y_true, y_pred, labels=class_names, save_path=cm_path, normalize=\"true\")\nif env_flag(\"WANDB\"):\nconf_mat = wandb.plot.confusion_matrix(preds=y_pred, y_true=y_true, class_names=class_names)\nwandb.log({\"conf_mat\": conf_mat})\n</code></pre>"},{"location":"api/models/#heartkit.hrv","title":"<code>heartkit.hrv</code>","text":""},{"location":"api/models/#heartkit.hrv.compute_hrv","title":"<code>compute_hrv(data, qrs_mask, sampling_rate=1000)</code>","text":"<p>Compute HRV metrics</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>ECG Data</p> </li> <li> qrs_mask             (<code>npt.NDArray</code>)         \u2013 <p>QRS binary mask</p> </li> <li> sampling_rate             (<code>int</code>)         \u2013 <p>Sampling rate in Hz. Defaults to 1000.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[float, npt.NDArray, npt.NDArray]</code>         \u2013 <p>tuple[float, npt.NDArray, npt.NDArray]: HR in bpm, RR lengths, R peak indices</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def compute_hrv(\ndata: npt.NDArray, qrs_mask: npt.NDArray, sampling_rate: int = 1000\n) -&gt; tuple[float, npt.NDArray, npt.NDArray]:\n\"\"\"Compute HRV metrics\n    Args:\n        data (npt.NDArray): ECG Data\n        qrs_mask (npt.NDArray): QRS binary mask\n        sampling_rate (int, optional): Sampling rate in Hz. Defaults to 1000.\n    Returns:\n        tuple[float, npt.NDArray, npt.NDArray]: HR in bpm, RR lengths, R peak indices\n    \"\"\"\nrpeaks = find_peaks_from_segments(data=data, qrs_mask=qrs_mask, sampling_rate=sampling_rate)\nrr_lens = ecg_rate(peaks=rpeaks, sampling_rate=sampling_rate)\nhr_bpm = ecg_bpm(peaks=rpeaks, sampling_rate=sampling_rate)\nreturn hr_bpm, rr_lens, rpeaks\n</code></pre>"},{"location":"api/models/#heartkit.hrv.ecg_bpm","title":"<code>ecg_bpm(peaks, sampling_rate=1000, min_rate=None, max_rate=None)</code>","text":"<p>Compute average hearte rate (BPM) from R peaks</p> <p>Parameters:</p> <ul> <li> peaks             (<code>npt.NDArray</code>)         \u2013 <p>R peaks</p> </li> <li> sampling_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz. Defaults to 1000.</p> </li> <li> min_rate             (<code>float | None</code>)         \u2013 <p>Min rate in sec. Defaults to None.</p> </li> <li> max_rate             (<code>float | None</code>)         \u2013 <p>Max rate in sec. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> float(            <code>float</code> )        \u2013 <p>Heart rate in BPM</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def ecg_bpm(\npeaks: npt.NDArray,\nsampling_rate: float = 1000,\nmin_rate: float | None = None,\nmax_rate: float | None = None,\n) -&gt; float:\n\"\"\"Compute average hearte rate (BPM) from R peaks\n    Args:\n        peaks (npt.NDArray): R peaks\n        sampling_rate (float, optional): Sampling rate in Hz. Defaults to 1000.\n        min_rate (float | None, optional): Min rate in sec. Defaults to None.\n        max_rate (float | None, optional): Max rate in sec. Defaults to None.\n    Returns:\n        float: Heart rate in BPM\n    \"\"\"\nif np.size(peaks) &lt;= 1:\nreturn -1\nrr_intervals = ecg_rate(peaks=peaks, sampling_rate=sampling_rate, desired_length=None)\nif min_rate is not None:\nrr_intervals = np.where(rr_intervals &lt; min_rate, min_rate, rr_intervals)\nif max_rate is not None:\nrr_intervals = np.where(rr_intervals &gt; max_rate, max_rate, rr_intervals)\nif np.size(peaks) &lt;= 1:\nreturn -1\nreturn 60 / np.mean(rr_intervals)\n</code></pre>"},{"location":"api/models/#heartkit.hrv.ecg_rate","title":"<code>ecg_rate(peaks, sampling_rate=1000, desired_length=None)</code>","text":"<p>Compute ECG rate from R peak indices</p> <p>Parameters:</p> <ul> <li> peaks             (<code>npt.NDArray</code>)         \u2013 <p>R peak indices</p> </li> <li> sampling_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz. Defaults to 1000.</p> </li> <li> desired_length             (<code>int | None</code>)         \u2013 <p>Perform extrapolation to given length. Defaults to None.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: RR rates</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def ecg_rate(peaks: npt.NDArray, sampling_rate: float = 1000, desired_length: int | None = None) -&gt; npt.NDArray:\n\"\"\"Compute ECG rate from R peak indices\n    Args:\n        peaks (npt.NDArray): R peak indices\n        sampling_rate (float, optional): Sampling rate in Hz. Defaults to 1000.\n        desired_length (int | None, optional): Perform extrapolation to given length. Defaults to None.\n    Returns:\n        npt.NDArray: RR rates\n    \"\"\"\nif np.size(peaks) &lt;= 3:\nreturn np.zeros_like(peaks)\nrr_intervals = nk.signal_period(peaks=peaks, sampling_rate=sampling_rate, desired_length=desired_length)\n# NK used global average for first peak- Instead lets take neighboring peak\nrr_intervals[0] = rr_intervals[1]\nreturn rr_intervals\n</code></pre>"},{"location":"api/models/#heartkit.hrv.evaluate_model","title":"<code>evaluate_model(params)</code>","text":"<p>Test HRV model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTestParams</code>)         \u2013 <p>Testing/evaluation parameters</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def evaluate_model(params: HeartTestParams):\n\"\"\"Test HRV model.\n    Args:\n        params (HeartTestParams): Testing/evaluation parameters\n    \"\"\"\n</code></pre>"},{"location":"api/models/#heartkit.hrv.export_model","title":"<code>export_model(params)</code>","text":"<p>Export segmentation model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartExportParams</code>)         \u2013 <p>Deployment parameters</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def export_model(params: HeartExportParams):\n\"\"\"Export segmentation model.\n    Args:\n        params (HeartExportParams): Deployment parameters\n    \"\"\"\n</code></pre>"},{"location":"api/models/#heartkit.hrv.find_peaks_from_segments","title":"<code>find_peaks_from_segments(data, qrs_mask, sampling_rate=250)</code>","text":"<p>Find R peaks using QRS segment mask.</p> <p>Parameters:</p> <ul> <li> data             (<code>npt.NDArray</code>)         \u2013 <p>1-ch ECG data</p> </li> <li> qrs_mask             (<code>npt.NDArray</code>)         \u2013 <p>Segmentation mask</p> </li> <li> sampling_rate             (<code>float</code>)         \u2013 <p>Sampling rate in Hz</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>npt.NDArray</code>         \u2013 <p>npt.NDArray: R peak indices</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def find_peaks_from_segments(data: npt.NDArray, qrs_mask: npt.NDArray, sampling_rate: float = 250) -&gt; npt.NDArray:\n\"\"\"Find R peaks using QRS segment mask.\n    Args:\n        data (npt.NDArray): 1-ch ECG data\n        qrs_mask (npt.NDArray): Segmentation mask\n        sampling_rate (float): Sampling rate in Hz\n    Returns:\n        npt.NDArray: R peak indices\n    \"\"\"\n# Apply moving average and theshold above 66%\nr_segs = np.where(np.convolve(qrs_mask, np.ones(6), \"same\") &gt; 4, 1, 0)\n# Locate start and end of QRS segments\nqrs_starts = np.where(np.diff(r_segs) == 1)[0]\nqrs_ends = np.where(np.diff(r_segs) == -1)[0]\n# For now use middle of QRS as R peak. Later look for abs peak in data\npeaks = np.vstack((qrs_starts, qrs_ends)).mean(axis=0).astype(int)\nreturn peaks\n</code></pre>"},{"location":"api/models/#heartkit.hrv.train_model","title":"<code>train_model(params)</code>","text":"<p>Train HRV model.</p> <p>Parameters:</p> <ul> <li> params             (<code>HeartTrainParams</code>)         \u2013 <p>Training parameters</p> </li> </ul> Source code in <code>heartkit/hrv.py</code> <pre><code>def train_model(params: HeartTrainParams):\n\"\"\"Train HRV model.\n    Args:\n        params (HeartTrainParams): Training parameters\n    \"\"\"\n</code></pre>"},{"location":"tutorials/arrhythmia-demo/","title":"HeartKit Arrhythmia Tutorial","text":"<p>This tutorial shows running an end-to-end heart arrhythmia classifier on the Apollo 4 EVB. The basic flow chart is depicted below.</p> <pre><code>flowchart LR\n    S[1. Collect] --&gt;| | P[2. Preprocess] --&gt; M[3. Arrhythmia Model] --&gt; L[4. Display]</code></pre> <p>In the first stage, 4 seconds of sensor data is collected- either directly from the MAX86150 sensor or test data from the PC. In stage 2, the data is preprocessed by bandpass filtering and standardizing. The data is then fed into the CNN network to perform inference. Finally, in stage 4, the ECG data will be classified as normal (NSR), arrhythmia (AFIB/AFL) or inconclusive. Inconclusive is assigned when the prediction confidence is less than a pre-defined threshold (e.g. 90%).</p> <p>Note</p> <p>A reference arrhythmia model (<code>./evb/src/arrhythmia_model_buffer.h</code>) is included and can be used to quickly evaluate the hardware. The model is trained on Icentia11k dataset that has the associated non-commercial license. The model is intended for evaluation purposes only and cannot be used for commercial use without permission.</p>"},{"location":"tutorials/arrhythmia-demo/#demo-setup","title":"Demo Setup","text":"<p>Please follow EVB Setup Guide to prepare EVB and connect to PC. To use the pre-trained model, please skip to Run Demo Section.</p>"},{"location":"tutorials/arrhythmia-demo/#1-train-the-arrhythmia-model-w-given-configuration","title":"1. Train the arrhythmia model w/ given configuration","text":"<pre><code>heartkit \\\n--task arrhythmia \\\n--mode train \\\n--config ./configs/train-arrhythmia-model.json\n</code></pre> <p>Note</p> <p>Due to the large dataset and class imbalance, the batch size and buffer size are large for arrhythmia training to ensure properly shuffling of patients as well as classes. The first epoch will take much longer as it fills up this buffer. To train on dedicated GPUs, it's recommended to have at least 10 GB of VRAM.</p>"},{"location":"tutorials/arrhythmia-demo/#2-evaluate-the-arrhythmia-models-performance","title":"2. Evaluate the arrhythmia model's performance","text":"<pre><code>heartkit \\\n--task arrhythmia \\\n--mode evaluate \\\n--config ./configs/evaluate-arrhythmia-model.json\n</code></pre>"},{"location":"tutorials/arrhythmia-demo/#3-export-the-model-into-a-tflm-header-file","title":"3. Export the model into a TFLM header file","text":"<pre><code>heartkit \\\n--task arrhythmia \\\n--mode export \\\n--config ./configs/export-arrhythmia-model.json\n</code></pre> <p>The exported model will be placed into <code>./evb/src/arrhythmia_model_buffer.h</code>. Please review <code>./evb/src/constants.h</code> and ensure settings match configuration file.</p>"},{"location":"tutorials/arrhythmia-demo/#run-demo","title":"Run Demo","text":"<p>Please open three terminals to ease running the demo. We shall refer to these as EVB Terminal, REST Terminal and PC Terminal.</p>"},{"location":"tutorials/arrhythmia-demo/#1-run-client-on-evb","title":"1. Run client on EVB","text":"<p>Run the following commands in the EVB Terminal. This will compile the EVB binary and flash it to the EVB. The binary will be located in <code>./evb/build</code>.</p> <pre><code>make -C ./evb\nmake -C ./evb deploy\nmake -C ./evb view\n</code></pre> <p>Now press the reset button on the EVB. This will allow SWO output to be captured.</p>"},{"location":"tutorials/arrhythmia-demo/#2-run-rest-server-on-host-pc","title":"2. Run REST server on host PC","text":"<p>In REST Terminal, start the REST server on the PC.</p> <pre><code>python -m heartkit.demo.server\n</code></pre>"},{"location":"tutorials/arrhythmia-demo/#3-run-client-and-ui-on-host-pc","title":"3. Run client and UI on host PC:","text":"<p>In PC Terminal, start the PC client (console UI).</p> <pre><code>heartkit \\\n--task arrhythmia \\\n--mode demo \\\n--config ./configs/arrhythmia-demo.json\n</code></pre> <p>Upon start, the client will scan and connect to the EVB serial port. If no port is detected after 30 seconds, the client will exit. If successful, the client should discover the USB port and start updating UI.</p>"},{"location":"tutorials/arrhythmia-demo/#4-trigger-start","title":"4. Trigger start","text":"<p>Now that the three tasks are running, press either Button 1 (BTN1) or Button 2 (BTN2) on the EVB to start the demo. Pressing Button 1 will use live sensor data whereas Button 2 will use test dataset supplied by the PC. In EVB Terminal, the EVB should be printing the stage it's in (e.g <code>INFERENCE STAGE</code>) and any results. In PC Terminal, the PC should be plotting the data along with classification results. Once finished, Button 1 or Button 2 can be pressed to stop capturing.</p> <p></p> <p>Tip</p> <p>Please use a monospaced font in the terminal for proper alignment of the plot.</p> <p>To shutdown the PC client, a keyboard interrupt can be used (e.g <code>[CTRL]+C</code>) in PC Terminal. Likewise, a keyboard interrupt can be used (e.g <code>[CTRL]+C</code>) to stop the PC REST server in REST Terminal.</p>"},{"location":"tutorials/evb-setup/","title":"EVB Setup &amp; Compiling","text":""},{"location":"tutorials/evb-setup/#requirements","title":"Requirements","text":""},{"location":"tutorials/evb-setup/#software","title":"Software","text":"<p>In order to compile the EVB binary, both Arm GNU Toolchain and Segger J-Link must be installed on the host PC. After installing, ensure they are both available from the terminal. (e.g <code>which arm-none-eabi-gcc</code>)</p> <ul> <li>Arm GNU Toolchain 11.3</li> <li>Segger J-Link v7.56+</li> </ul>"},{"location":"tutorials/evb-setup/#hardware","title":"Hardware","text":"<p>The following items are needed to flash firmware and run demos.</p> <ul> <li>Apollo4 EVB</li> <li>MAX86150 Eval Board</li> <li>2x USB-C cables</li> <li>Jumper wires -OR- Qwiic cable</li> </ul>"},{"location":"tutorials/evb-setup/#hardware-setup","title":"Hardware Setup","text":"<p>In order to connect the MAX86150 breakout board to the Apollo 4 EVB, we must first solder the 5-pin header on to the MAX86150 board.</p> <p></p> <p>Once soldered, connect the breakout board to the EVB using 5 jumper wires as follows:</p> Breakout Apollo 4 EVB VCC J17 pin 2 (5V) SCL J11 pin 3 (GPIO8) SDA J11 pin 1 (GPIO9) INT Not used GND J17 pin 4 (GND) <p></p> <p>Note</p> <p>Alternatively, the Qwiic connector on the breakout board can be used. This will require a Qwiic breakout cable. For 3V3, J3 pin 5 (3.3V) can be leveraged on the EVB.</p>"},{"location":"tutorials/evb-setup/#firmware-setup","title":"Firmware Setup","text":"<p>Connect the EVB to your laptop using both USB-C ports on the EVB.</p> <p>Note</p> <p>Quality of ECG from the onboard pads depends on contact quality and will produce more artifacts. For a better quality ECG, it is recommended that the accompanying ECG cable be used with electrodes. Place blue electrode on left wrist/finger/arm and red electrode on right wrist/finger/arm. Optionally, the black ground electrode can be placed on the body to further improve SNR.</p>"},{"location":"tutorials/evb-setup/#firmware-commands","title":"Firmware Commands","text":"Name Command Description Clean <code>make -C ./evb clean</code> Clean build artifacts Build <code>make -C ./evb</code> Compile binary Flash <code>make -C ./evb deploy</code> Flash binary to EVB View <code>make -C ./evb view</code> View SWO Output"},{"location":"tutorials/heartkit-demo/","title":"HeartKit Tutorial","text":"<p>This tutorial shows running the full HeartKit demonstrator on the Apollo 4 EVB. The basic flow chart is depicted below.</p> <pre><code>flowchart LR\n    COLL[1. Collect] --&gt; PRE[2. Preprocess]\n    PRE[2. Preprocess] --&gt; SEG[3. Segmentation]\n    subgraph Models\n    SEG[3. Segmentation] --&gt; HRV[4. HRV]\n    SEG[3. Segmentation] --&gt; BEAT[4. BEAT]\n    SEG[3. Segmentation] --&gt; ARR[4. Arrhythmia]\n    end\n    HRV[4. HRV] --&gt; DISP[5. Display]\n    BEAT[4. BEAT] --&gt; DISP\n    ARR[4. Arrhythmia] --&gt; DISP</code></pre> <p>In the first stage, 10 seconds of sensor data is collected- either directly from the MAX86150 sensor or test data from the PC. In stage 2, the data is preprocessed by bandpass filtering and standardizing. The data is then fed into the HeartKit models to perform inference. Finally, in stage 4, the ECG data and classification results will be displayed in the front-end UI.</p>"},{"location":"tutorials/heartkit-demo/#demo-setup","title":"Demo Setup","text":"<p>Please follow EVB Setup Guide to prepare EVB and connect to PC. To use the pre-trained models, please skip to Run Demo Section.</p>"},{"location":"tutorials/heartkit-demo/#1-train-all-the-models","title":"1. Train all the models","text":"<p>1.1 Train and fine-tune the segmentation model:</p> <pre><code>heartkit \\\n--task segmentation \\\n--mode train \\\n--config ./configs/pretrain-segmentation-model.json\n</code></pre> <pre><code>heartkit \\\n--task segmentation \\\n--mode train \\\n--config ./configs/train-segmentation-model.json\n</code></pre> <p>Note</p> <p>The second train command uses quantization-aware training to reduce accuracy drop when exporting to 8-bit.</p> <p>1.2 Train the arrhythmia model:</p> <pre><code>heartkit \\\n--task arrhythmia \\\n--mode train \\\n--config ./configs/train-arrhythmia-model.json\n</code></pre> <p>1.3 Train the beat model:</p> <pre><code>heartkit \\\n--task beat \\\n--mode train \\\n--config ./configs/train-beat-model.json\n</code></pre>"},{"location":"tutorials/heartkit-demo/#2-evaluate-all-the-models","title":"2. Evaluate all the models","text":"<p>2.1 Evaluate the segmentation model performance:</p> <pre><code>heartkit \\\n--task segmentation \\\n--mode evaluate \\\n--config ./configs/evaluate-segmentation-model.json\n</code></pre> <p>2.2 Evaluate the arrhythmia model performance:</p> <pre><code>heartkit \\\n--task arrhythmia \\\n--mode evaluate \\\n--config ./configs/evaluate-arrhythmia-model.json\n</code></pre> <p>2.3 Evaluate the beat model performance:</p> <pre><code>heartkit \\\n--task beat \\\n--mode evaluate \\\n--config ./configs/evaluate-beat-model.json\n</code></pre>"},{"location":"tutorials/heartkit-demo/#3-export-all-the-models","title":"3. Export all the models","text":"<p>3.1 Export the segmentation model to <code>./evb/src/segmentation_model_buffer.h</code></p> <pre><code>heartkit \\\n--task segmentation \\\n--mode export \\\n--config ./configs/export-segmentation-model.json\n</code></pre> <p>3.2 Export the arrhythmia model to <code>./evb/src/arrhythmia_model_buffer.h</code></p> <pre><code>heartkit \\\n--task arrhythmia \\\n--mode export \\\n--config ./configs/export-arrhythmia-model.json\n</code></pre> <p>3.3 Export the beat model to <code>./evb/src/beat_model_buffer.h</code></p> <pre><code>heartkit \\\n--task beat \\\n--mode export \\\n--config ./configs/export-beat-model.json\n</code></pre> <p>Note</p> <p>Review <code>./evb/src/constants.h</code> and ensure settings match configuration file.</p>"},{"location":"tutorials/heartkit-demo/#run-demo","title":"Run Demo","text":"<p>Please open three terminals to ease running the demo. We shall refer to these as EVB Terminal, REST Terminal and PC Terminal.</p>"},{"location":"tutorials/heartkit-demo/#1-run-client-on-evb","title":"1. Run client on EVB","text":"<p>Run the following commands in the EVB Terminal. This will compile the EVB binary and flash it to the EVB. The binary will be located in <code>./evb/build</code>.</p> <pre><code>make -C ./evb\nmake -C ./evb deploy\nmake -C ./evb view\n</code></pre> <p>Now press the reset button on the EVB. This will allow SWO output to be captured.</p>"},{"location":"tutorials/heartkit-demo/#2-run-rest-server-on-host-pc","title":"2. Run REST server on host PC","text":"<p>In REST Terminal, start the REST server on the PC.</p> <pre><code>uvicorn heartkit.demo.server:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"tutorials/heartkit-demo/#3-run-client-and-ui-on-host-pc","title":"3. Run client and UI on host PC","text":"<p>In PC Terminal, start the PC client (console UI).</p> <pre><code>heartkit --mode demo --config ./configs/heartkit-demo.json\n</code></pre> <p>Upon start, the client will scan and connect to the EVB serial port. If no port is detected after 30 seconds, the client will exit. If successful, the client should discover the USB port and start updating UI.</p>"},{"location":"tutorials/heartkit-demo/#4-trigger-start","title":"4. Trigger start","text":"<p>Now that the EVB client, PC client, and PC REST server are running, press either Button 1 (BTN1) or Button 2 (BTN2) on the EVB to start the demo. Pressing Button 1 will use live sensor data whereas Button 2 will use test dataset supplied by the PC. In EVB Terminal, the EVB should be printing the stage it's in (e.g <code>INFERENCE STAGE</code>) and any results. In PC Terminal, the PC should be plotting the data along with classification results. Once finished, Button 1 or Button 2 can be pressed to stop capturing.</p> <p></p> <p>To shutdown the PC client, a keyboard interrupt can be used (e.g <code>[CTRL]+C</code>) in PC Terminal. Likewise, a keyboard interrupt can be used (e.g <code>[CTRL]+C</code>) to stop the PC REST server in REST Terminal.</p>"}]}